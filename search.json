[{"categories":null,"content":"Sink is a ’loading’ part of ETL(Extract, Transformation, Loading) inside Flink. It is last process of data pipeline, to store data inside datalake after it has been extract from source, and transformed into specific format.\nThis is example of how you can sink from Flink DataStream:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // ... private static DataStream\u003cString\u003e createSourceFromStaticConfig(StreamExecutionEnvironment env) { Properties inputProperties = new Properties(); inputProperties.setProperty(ConsumerConfigConstants.AWS_REGION, region); inputProperties.setProperty(ConsumerConfigConstants.STREAM_INITIAL_POSITION, \"LATEST\"); return env.addSource(new FlinkKinesisConsumer\u003c\u003e(inputStreamName, new SimpleStringSchema(), inputProperties)); } public static void main(String[] args) throws Exception { // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u003cString\u003e input = createSourceFromStaticConfig(env); input.addSink(mySinkFunction(\"s3://data-storage/\")); // \u003c- this is for `SinkFunction\u003cT\u003e` input.sinkTo(mySinkFunction(\"s3://data-storage/\")); // \u003c- this is for `FileSink` } FileSink, and StreamingFileSink If you see the documents here, you can find out there are StreamingFileSink and FileSink.\nInternally, StreamingFileSink is a predecessor of FileSink. And in the document it has written that FileSink supports BATCH and STREAMING both, while StreamingFileSink is only for streaming.\nAnd finally from Flink 1.17, StreamingFileSink has been deprecated, so it would be good to go on with FileSink from now.\naddSink function requires SinkFunction\u003cT\u003e parameter, and this is for the case when you’re trying to use StreamingFileSink. Or instead for FileSink, call sinkTo to add on sink logic.\nIn this post, I’ll just talk about raw file/compressed file sink, which I’ve worked recently. For big data format file, such as parquet, orc …, refer the guide of connectors in documents .\nSink out raw text data First, you need additional dependencies for FileSink:\n1 2 3 4 5 \u003cdependency\u003e \u003cgroupId\u003eorg.apache.flink\u003c/groupId\u003e \u003cartifactId\u003eflink-connector-files\u003c/artifactId\u003e \u003cversion\u003e${flink.version}\u003c/version\u003e \u003c/dependency\u003e and here is some configurations needs for file sink(some are optional).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public static void main(String[] args) throws Exception { // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u003cString\u003e input = createSourceFromStaticConfig(env); input.sinkTo(mySinkFunction(\"s3://data-storage/\")); } private static FileSink\u003cString\u003e mySinkFunction(String bucket) throws IOException { OutputFileConfig outputConfig = OutputFileConfig .builder() .withPartPrefix(\"my-data\") .withPartSuffix(\".txt\") .build(); return FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder\u003cString\u003e(\"UTF-8\")) .withBucketAssigner(new DateTimeBucketAssigner(\"yyyy-MM-dd_HH-mm\")) .withRollingPolicy( DefaultRollingPolicy.builder() .withRolloverInterval(TimeUnit.MINUTES.toMillis(15)) .withInactivityInterval(TimeUnit.MINUTES.toMillis(5)) .withMaxPartSize(1024 * 1024 * 1024) .build() ) .withOutputFileConfig(outputConfig) .build(); } In forRowFormat, you should define output path, and Encoder to serialize individual row data, to output stream. If your data is just raw text data, it will be just fine to use default SimpleStringEncoder with UTF-8 encoded.\nwithBucketAssigner is to define the directory name data to be stored, in specific rule. DateTimeBucketAssigner is to generate directory in date-time format. In above, it will put each data inside directory like /2022-09-10_18-46\nwithRollingPolicy is to decide the rule, how/when the stream data will be roll-out as output file. In the rule above, single .txt file will append the data in stream in following status\nwhen data has been collected at least 15 minutes there are no new elements for 5 minutes file size has been reached to 1GB And with output configuration, file name will be defined as my-data-????.txt by defining prefix/suffix.\nSo finally, output will be located as s3://data-storage/2022-09-10_18-46/my-data-????.txt\nSome other case, there can be case if user needs to make all elements placed in separate files. File release are related with rolling policy. So in my case, I’ve make custom policy which extends default CheckpointRollingPolicy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 private static FileSink\u003cString\u003e mySinkFunction(String bucket) throws IOException { OutputFileConfig outputConfig = OutputFileConfig .builder() .withPartPrefix(\"my-data\") .withPartSuffix(\".txt\") .build(); return FileSink .forRowFormat(new Path(outputPath), new SimpleStringEncoder\u003cString\u003e(\"UTF-8\")) .withBucketAssigner(new DateTimeBucketAssigner(\"yyyy-MM-dd_HH-mm\")) .withRollingPolicy(new CustomCheckpointRollingPolicy()) .withOutputFileConfig(outputConfig) .build(); } ... static final class CustomCheckpointRollingPolicy\u003cIN, BucketID\u003e extends CheckpointRollingPolicy\u003cIN, BucketID\u003e { private static final long serialVersionUID = 1L; CustomCheckpointRollingPolicy() {} public boolean shouldRollOnEvent(PartFileInfo\u003cBucketID\u003e partFileState, IN element) { return true; } public boolean shouldRollOnProcessingTime(PartFileInfo\u003cBucketID\u003e partFileState, long currentTime) { return false; } } It has been defined shouldRollOnEvent to always return true, so it will roll out for every elemnts. In this case, every ‘string’ data from DataStream will be generated as separated text file.\nCompress(’.gzip’) output sink file Now let’s think about when you want to sink data as compressed file(like gzip). In most of case you would want in this way, to reduce file size, network traffic. As you know, these are all related with cost.\nFor compressed format, you need to use bulk format, with BulkWriter which can be defined with base BulkWriter.Factory. This is not just for compressed file such as zip, gzip, but also for big data formats like parquet, avro, orc.\nBulk encoded formats\nYou can find several built-in writer here. And for gzip, it needs to use CompressWriterFactory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 private static FileSink\u003cString\u003e mySinkFunction(String bucket) throws IOException { OutputFileConfig outputConfig = OutputFileConfig .builder() .withPartPrefix(\"my-data\") .withPartSuffix(\".gzip\") .build(); return FileSink .forBulkFormat( new Path(outputPath), CompressWriters.forExtractor(new DefaultExtractor()).withHadoopCompression(\"GzipCodec\") ) .withBucketAssigner(new DateTimeBucketAssigner(\"yyyy-MM-dd_HH-mm\")) .withRollingPolicy(new CustomCheckpointRollingPolicy()) .withOutputFileConfig(outputConfig) .build(); } CompressWriters are builder for creating CompressWriterFactory instance, and DefaultExtractor is to turn record into byte array for writing data. This transformed byte array data can be compressed with following hadoop compression codec, by withHadoopCompression.\nDEFLATE: org.apache.hadoop.io.compress.DefaultCodec gzip: org.apache.hadoop.io.compress.GzipCodec bzip2: org.apache.hadoop.io.compress.BZip2Codec LZO: com.hadoop.compression.lzo.LzopCodec Okay, now it’s done in code. But it could cause exception as following,\njava.util.concurrent.CompletionException: java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration\\n\\tat java.base java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314) java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319) java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702) java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base/java.lang.Thread.run(Thread.java:829) Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration org.apache.flink.formats.compress.CompressWriterFactory.withHadoopCompression(CompressWriterFactory.java:76) and this means it cannot find class definition for hadoop file system. In this case you should add following package:\n1 2 3 4 5 6 7 \u003cgroupId\u003eorg.apache.flink\u003c/groupId\u003e \u003cartifactId\u003eflink-compress\u003c/artifactId\u003e \u003cversion\u003e${flink.version}\u003c/version\u003e ... \u003cgroupId\u003eorg.apache.flink\u003c/groupId\u003e \u003cartifactId\u003eflink-s3-fs-hadoop\u003c/artifactId\u003e \u003cversion\u003e${flink.version}\u003c/version\u003e Reference https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/overview/ https://stackoverflow.com/questions/ https://docs.aws.amazon.com/kinesisanalytics/latest/java/how-sinks.html https://www.oreilly.com/library/view/hadoop-the-definitive/9780596521974/ch04.html ","description":"","tags":["bigdata","flink","sink","raw","gzip"],"title":"Export file(raw, compressed) from Flink application","uri":"/posts/2022-09-11-export-file-from-flink-app/"},{"categories":null,"content":"Data stream is a flow of data, which are coming from multiple sources continuously. It can be event log or database log captured by CDC internally, or sensor data from IoT devices.\nThese kind of streams are usually very high-frequent in production level, and in this case we will think of connecting sources into stream processing framework to handle it, such as Spark or Flink.\nBecause I’ve started using Flink recently, so try to look on how this is being treated.\nWhat is Flink Flink is distributed data processing framework for stateful computations over unbounded and bounded data streams. In the point of ‘distribute data processing framework’ it looks similar with Apache Spark. Actually both has lots in common, and are being widely used in data engineering field. If you’re looking on job description for data engineering position, you could see at least one of these(or both) in requirements.\nOne of big difference is that Flink is implemented as to be true streaming engine, while Spark is handling streaming data with ‘micro-batch’ engine. Though it looks like streaming, there are difference under the hood.\nTo treat bounded/unbounded streaming(which means whether the end of streaming is defined or not), there are lots of logics implemented in Flink, to let developers handles streaming data safe and stable.\nHow Flink handles stream data - basics After receiving stream, we probably want to transform the data to specific format, and send outside to the target. In Flink, it offers very expressive window semantics, with window function.\n‘Window’ are core of processing streams. It defines how to splits infinite stream into finite size, so it can apply computation.\nCheck the general structure below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... DataStream\u003cT\u003e inputStream = ...; // 1. Keyed inputStream .keyBy(...) // keyed versus non-keyed windows .window(...) // required: \"assigner\" // ... and more operator // 2. Non-Keyed inputStream .window(...) // required: \"assigner\" // ... and more operator Generated stream variable inputStream, is stream of T instances. And by window assigner, stream will be splited into defined size.\nAfter splitted, it can be computed with variable functions offered by Flink. Also there are several more operators(trigger, evictor, …) to control data flow which is optional. But here, I’ve just noted about how data inside stream are being assigned into separated window.\nKeyed / Non-Keyed On the code, you can see difference between 2 structure, that one of them has keyBy function.\n‘Keyed’ stream, will allow stream computation to be performed in parallel, and each logical keyed data will be processed separately, with data which have different key. More simply, you can think key as some kind of ‘partition’ of data processing.\n1 2 3 4 5 6 7 8 9 10 11 class User { String name; int id; int groupId; } DataStream\u003cUser\u003e userStream = ...; userStream .keyBy(User::groupId) .window(...); So in this code, stream will be internally partitioned by groupId.\nWindow Assigners ‘Window Assigners’ are the features which goes inside window(...) operator. It is to define how elements in stream will be ‘assigned’ in windows.\nTumbling windows ‘Tumbling windows’ assigners is to assign input elements to window of specified window size. In this, size means time range(event time, or processing time).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DataStream\u003cT\u003e inputStream = ...; // define window size as 5 second range of event time inputStream .keyBy(...) .window(TumblingEventTimeWindows.of(Time.seconds(5))); // define window size as 1 minute range of processing time inputStream .keyBy(...) .window(TumblingProcessingTimeWindows.of(Time.minutes(1))); // can input offset parameter as option. // 1 day range(event time) of window, with -9 hour inputStream .keyBy(...) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-9))); Sliding windows ‘Sliding windows’ also assign inputs to range-defined window, but it needs to define how frequently start new window. So as you can see in diagram above, there can be overlapped part between windows(if frequency is smaller than window size). So single input can be assigned to multiple windows.\n1 2 3 4 5 6 7 8 9 10 11 DataStream\u003cT\u003e inputStream = ...; // window size is 10 second of event time, and create new one every 5 second inputStream .keyBy(...) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))); // window size is 1 minute of processing time, and create new one every 50 second inputStream .keyBy(...) .window(SlidingProcessingTimeWindows.of(Time.minutes(1), Time.seconds(50))); Session windows ‘Session windows’ groups inputs by sessions of activity. It opens window when input has been received, and close if there’s no input in defined time range. So there are no fixed start/end time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 DataStream\u003cT\u003e inputStream = ...; // window is kept opened until there are no inputs on 10 minutes inputStream .keyBy(...) .window(EventTimeSessionWindows.withGap(Time.minutes(10))); // event-time session windows with dynamic gap inputStream .keyBy(...) .window(EventTimeSessionWindows.withDynamicGap((element) -\u003e { // determine and return session gap })); Global windows ‘Global windows’ assigns all elements with the same key to the same single global window. This is only useful when specifiying custom trigger. Otherwise, no computation will be performed.\n1 2 3 4 5 DataStream\u003cT\u003e inputStream = ...; inputStream .keyBy(...) .window(GlobalWindows.create()); Reference https://nightlies.apache.org/flink/flink-docs-master/ ","description":"","tags":["bigdata","flink","window","stream","java"],"title":"How streams are being handled in Flink, with window","uri":"/posts/2022-06-05-flink-handle-stream-window/"},{"categories":null,"content":"About year ago, I’ve wrote some post about BFF architecture. One of the architecture’s component was ‘reverse proxy’. This is for more description about it.\nProxy server Definition of proxy server in networking, means server application works as relay server between client and target server. By using this, client can request file or data through proxy server, which evaluates the requests to make required network transaction.\nForward \u0026 Reverse ‘Forward proxy’ is server that sits in edge of network, which regulates outbound traffic by defined policies in network cluster.\nPosition of ‘reverse proxy’ also locates in front of the client(same as forward). Clients send requests to the origin server of a website, those requests are intercepted by the reverse proxy server. The reverse proxy server will then send requests to and receive responses from the origin server.\nThe difference between these 2 are pretty delicate. An easy approach to summarize it is saying that forward proxy sit before a customer and guarantee that no beginning worker at any point discusses straightforwardly with that particular customer. And reverse proxy, intermediary sits before a beginning worker and guarantees that no customer at any point discusses straightforwardly with that beginning worker.\nSimple implementation of Reverse proxy Assume that there are web server running localhost:9000, which returns following at /proxy\n{\"hello\":\"world\"} Now I’ll make simple reverse proxy server running in localhost:4000, which will catch request and send it again to specific target to get response if API is /proxy.\nThis is simple implementation based on axum web framework for this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 use axum::{ extract::Extension, http::{uri::Uri, Request, Response}, routing::get, AddExtensionLayer, Router, }; use hyper::{client::HttpConnector, Body}; use std::{convert::TryFrom, net::SocketAddr}; type Client = hyper::client::Client\u003cHttpConnector, Body\u003e; #[tokio::main] async fn main() { let client = Client::new(); let app = Router::new() .route(\"/proxy\", get(proxy_handler)) .layer(AddExtensionLayer::new(client)); let addr = SocketAddr::from(([127, 0, 0, 1], 4000)); println!(\"server running on {}\", addr); axum::Server::bind(\u0026addr) .serve(app.into_make_service()) .await .unwrap(); } async fn proxy_handler(Extension(client): Extension\u003cClient\u003e, mut req: Request\u003cBody\u003e) -\u003e Response\u003cBody\u003e { let path_query = req .uri() .path_and_query() .map(|v| v.as_str()) .unwrap_or(req.uri().path()); let uri = format!(\"http://127.0.0.1:9000{}\", path_query); *req.uri_mut() = Uri::try_from(uri).unwrap(); client.request(req).await.unwrap() } Make this run, and you can receive the data {\"hello\":\"world\"} through reverse proxy server by calling localhost:4000/proxy instead of calling localhost:9000/proxy.\nWhat is it for? Performance World’s top web services, such as Google or Facebook, should get billions of requests every day, and it is mission impossible to handle all incoming traffic with single origin server. In this case, reverse proxy can be work as load balancer solution and distribute traffics to divided origin servers.\nNot only this, it can store data to make it run as caching system. So instead of making requests to origin server for every case, it can return the result to client with cached data directly. It can reduce response time of request dramatically.\nEfficient security Reverse proxy are placed in front of network cluster, so web service don’t need to reveal IP address of origin servers. Hackers can only face to reverse proxy server, and maintainer also only need to focus on security of proxy server.\nAlso, because there are unified endpoint, it doesn’t need to encrypt and unscramble SSL for every system.\nReference https://en.wikipedia.org/wiki/Proxy_server https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://www.wallarm.com/what/what-is-the-reverse-proxy https://github.com/tokio-rs/axum ","description":"","tags":["proxy","reverseproxy","rust","network"],"title":"What is reverse proxy for","uri":"/posts/2021-11-29-what-reverse-proxy-for/"},{"categories":null,"content":"This is just for reminding my knowledge, about tree data structure, which most of developers will know:\nyes, something looks like this.\nMost of the cases we don’t need to think about the implementation deeply, cause most of the programming languages have library packages to make tree structure. But if you need to do your own, how would we start?\nStart to make some tree Tree is one of favorite question in coding test, and I think because it’s pretty good to know interviewee’s knowledge bit deeply, with single question.\nIf test goes on coding tool, usually it does not need to make things from scratch. For example if they ask you to find out whether specific value exists in tree, you just need to implement the code inside of given function, with given information of Node(branch of tree) object.\n1 2 3 public boolean isExists(Node t, int target) { // do something } But there are cases you should make code in whiteboard or pure text editor, and they can start asking you too ‘create some tree class’.\nWell, it’ll be simple, but it’s good to be friendly in this situation.\n1 2 3 4 5 6 7 8 9 class Node { Node left; Node right; int value; public Node (int value) { this.value = value; } } DFS \u0026 BFS With this tree, if we need to implement isExists function above, we can think of doing it in recursive way.\n1 2 3 4 5 public boolean isExists(Node t, int target) { if (t == null) return false; return isExists(t.left) || isExists(t.right); } Using recursion is one common way to go through tree structure by calling same function until it finds leaf.\nBut this is the way using DFS, cause it will go through left child first, until it faces null. If you need to make this in BFS way, it needs some additional features.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public boolean isExists(Node t, int target) { Queue\u003cNode\u003e q = new LinkedList\u003cNode\u003e(); q.add(t); while (!q.isEmpty()) { Node tree = q.poll(); if (tree.val == target) return true; if (tree.left != null) { q.add(tree.left); } if (tree.right != null) { q.add(tree.right); } } return false; } Cause it should read all data in same level before going on to next level, recursive way is not adaptable.\nUsing queue is not mandatory, but the logic required collection to push/poll value in FIFO order, so it is most optimized way.\nBinary tree Now you can maybe face on additional request, to make binary tree using Node above. It requires one more condition that every node has a value that is…\ngreater than or equal to the node values in the left sub-tree less than or equal to the node values in the right sub-tree` In the previous tree it should define the location of every node manually, like:\n1 2 3 Node t = new Node(3); t.left = new Node(4); t.right = new Node(5); But for binary tree, it only needs to add, and location should be defined by itself.\n1 2 3 4 5 6 7 8 9 10 11 12 BinaryTree t = new BinaryTree(3); t.add(4); t.add(5); /* should be performed like this... 3 \\ 4 \\ 5 */ Using Node class above, we could start with:\n1 2 3 4 5 6 7 class BinaryTree { Node root; public BinaryTree(int val) { root = new Node(val); } } and for add, it needs logic to find the location to be placed.\nDoing recursively will be good, to track the tree by depth:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ... public Node findRecursively(Node node, int val) { if (node == null) { return new Node(val); } if (val \u003c node.val) { // value is lower than value in node, so go left node.left = findRecursively(node.left, val); } else if (val \u003e node.val) { // value is bigger than value in node, so go right node.right = findRecursively(node.right, val); } return current; } and with this method, make add:\n1 2 3 4 public void add(int val) { // make a tree with new value, and setup at root node root = findRecursively(root, val); } Reference https://www.baeldung.com/ https://www.geeksforgeeks.org/ ","description":"","tags":["programming","java","datastructure","dfs","bfs"],"title":"Reminding tree structure","uri":"/posts/2021-11-15-back-to-basic-of-tree-structure/"},{"categories":null,"content":"‘Data Skewness’ is a one of issue you can face oftenly while treating spark which is caused based on parallel computing.\nFollowing image simply described the situation of ‘skewed’.\nWhy it happens Spark is distributed computing system, and it distributes the data separately inside cluster, to make data processing. So if data has been spread unevenly, processing will be concentrated in few specific machine which receive heavy data load.\nIt makes cluster unefficiently, and causes bad performance of system.\nOne main reason of this cause, is by calling join, groupBy, which makes data transformation to change data partitioning.\nIf you see the diagram,\nthis is caused by running logic below:\n1 2 3 4 t1.join(t2, Seq(\"make\", \"model\")) .filter(abs(t2(\"engine_size\") - t1(\"engine_size\")) \u003c= BigDecimal(\"0.1\")) .groupBy(\"registration\") .agg(avg(\"sale_price\").as(\"average_price\") On join process, data will be repartitioned with join criteria make/model. But because value of the criteria is skewed, distribution will be held up unequaly, like the diagram.\nWay to solve Add key to improve join If we can expect the status of data, we can select the way to add column for join, which has equity can make distribution more equity.\nBased on this table, we can including engine_size in join condition to achieve the desired result -\u003e every original record from t1 will be joined to every record from t2 with the same make, model, and engine size +/- 0.1. To acheive this we can make modified column, using explode.\nSpark function explode, is to split the array value by separate rows. For example,\n1 2 3 4 5 6 7 8 scala\u003e df.show(false) +-------+---------------------+ |name |knownLanguages | +-------+---------------------+ |James |[\"Java\",\"Scala\"] | |Michael|[\"Spark\",\"Java\",null]| |Robert |[\"CSharp\",\"\"] | +-------+---------------------+ assume there are dataframe like following. And by explode, it becomes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 scala\u003e df.select($\"name\",explode($\"knownLanguages\")) .show(false) +-------+------+ |name |col | +-------+------+ |James |Java | |James |Scala | |Michael|Spark | |Michael|Java | |Michael|null | |Robert |CSharp| |Robert | | +-------+------+ So, by making new column engine_size, and split the row by filter above:\n1 2 3 4 t1.withColumn(\"engine_size\", explode(array($\"engine_size\" - BigDecimal(\"0.1\"), $\"engine_size\", $\"engine_size\" + BigDecimal(\"0.1\")))) .join(t2, Seq(\"make\", \"model\", \"engine_size\")) .groupBy(\"registration\") .agg(avg(\"sale_price\").as(\"average_price\")) repartitioning will be held equally. Salting Or if it is difficult to indicate the equity status of table data, you can make some random data to table, and add in condition to make partitioning evenly. Shortly, it means to add noise data.\nAssume there are some logic like this:\n1 df.groupBy(\"address\", \"type\") make random value, and setup in groupBy.\n1 2 3 4 val salt = random(0, 100) - 1) df.withColumn(\"salt\", lit(salt)) .groupBy(\"address\", \"type\", \"salt\") .drop(\"salt\") To make more efficiently, random value should be in range 0 ~ partition count - 1.\nBroadcase join In other way, you can use function broadcast to spread join event to be done distributedly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 scala\u003e orgTable.show(false) +-------+ |id | +-------+ |1 | |2 | |3 | |... | |... | |9999 | |10000 | +-------+ scala\u003e newTable.show(false) +-----+-------+ |id |medal | +-----+-------+ |1 |gold | |2 |silver | |3 |bronze | +-----+-------+ With 2 table above, you can join these with id.\n1 val joined = orgTable.join(newTable, \"id\") Actually there are no problem on result, but there are in performance.\n1 2 3 4 5 6 7 8 9 10 11 12 joined.explain() == Physical Plan == *(5) Project [id#259L, medal#264] +- *(5) SortMergeJoin [id#259L], [cast(id#263 as bigint)], Inner :- *(2) Sort [id#259L ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(id#259L, 200) : +- *(1) Range (1, 10000, step=1, splits=6) +- *(4) Sort [cast(id#263 as bigint) ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(cast(id#263 as bigint), 200) +- *(3) Filter isnotnull(id#263) +- Scan ExistingRDD[id#263,order#264] In physical plan, you can find ‘SortMergeJoin’ on id, which includes the logic following:\nSorting orgTable Suffle orgTable and ‘Suffling’ is very expensive operation for performance. Simply, just wrap small table with broadcast:\n1 val newJoined = orgTable.join(broadcast(newTable), \"id\") you can find improved plan.\n1 2 3 4 5 6 7 8 9 newJoined.explain() == Physical Plan == *(2) Project [id#294L, order#299] +- *(2) BroadcastHashJoin [id#294L], [cast(id#298 as bigint)], Inner, BuildRight :- *(2) Range (1, 10000, step=1, splits=6) +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint))) +- *(1) Filter isnotnull(id#298) +- Scan ExistingRDD[id#298,order#299] There are no shuffling in orgTable, and BroadcastExchange is held for newTable, and it will only take small cost hence this is small table.\nReference http://spark.apache.org/ https://coxautomotivedatasolutions.github.io/datadriven/spark/data%20skew/joins/data_skew/ https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8 https://blog.rockthejvm.com/spark-broadcast-joins/ https://sparkbyexamples.com/ ","description":"","tags":["scala","spark","parallel","distribution","skew"],"title":"Optimizing spark pipeline from data skew","uri":"/posts/2021-09-03-face-on-skew-issue-in-spark/"},{"categories":null,"content":"Usually, data pipeline requires complex workflow. Generated data should be sent to various endpoints, and needs to be manufactured by status while moving on.\nFor example, if there’s a log file stored in S3, the pipeline may need to\nSend this to ELK, for monitoring purpose every 10 minute Format to filter useless columns, and send to BigQuery for researching insights, every hour. Or more… It is pretty complex to design workflow of jobs which need to handle each processes. So lot of great engineers had created project to make this done in simple way.\nAirflow Airflow is a platform to create/schedule/monitor workflows. This workflow are consist of 1 or more task, which is an implementation of an Operator. There are PythonOperator to execute Python code, BashOperator to run bash commands, and much more to run spark, flink, or else.\nThis is simple way to create workflow, consist with bash task and python task.\nOrder of these task can be design easily by using \u003e\u003e\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.python import PythonOperator def print_context(ds, **kwargs): \"\"\"Print the Airflow context and ds variable from the context.\"\"\" pprint(kwargs) print(ds) return 'Whatever you return gets printed in the logs' # Initiate DAG dag = DAG( 'tutorial', default_args=default_args, schedule_interval=timedelta(days=1) ) # define bash task with BashOperator bash_task = BashOperator( task_id='run_after_loop', bash_command='echo 1', dag=dag ) # define python task with PythonOperator py_task = PythonOperator( task_id='print_the_context', python_callable=print_context, dag=dag ) # define order of workflow. Run bash task first, and python task after it ends. bash_task \u003e\u003e py_task DAG object is to nest the tasks. This defines the detail configuration of the workflow, including schedule to run, or else.\nIf you’re working with Scala/Java and need to run spark jobs, you can use airflow-livy-operators module to call it simple.\n1 2 3 4 5 6 7 8 9 10 11 12 from airflow_livy import LivyBatchOperator // ... spark_op = LivyBatchOperator( file='spark-job.jar', class_name='com.example.Main' arguments=[ '-input=s3://input-bucket/input-path', '-output=s3://output-bucket/output-path', ], ... ) If you’re AWS user, you can think of using MWAA -Managed Workflows for Apache Airflow, which is managed orchestration service for Apache Airflow inside AWS. It makes easier for AWS user to set up and operate end-to-end data pipelines in the cloud at scale.\nDeployment with GitHub Action Copying python workflow scripts to cloud storage manually is pretty inconvenient. Moreover for security reason, uploading/downloading files through console is not a safe way to work, and that is one of the reason to use CI tool for uploading.\nBy using GitHub Action, above the pros of CI tool, you can easily control triggering by manually,or by github events(pull request, push…) with defined parameter.\nNow let’s describe the script to trigger the DAG.\nBy defining workflow_dispatch, you can make workflow which will run only by user’s action through Actions Tab in project page.\n1 2 3 4 5 6 on: workflow_dispatch: inputs: dag_name: description: 'DAG name' required: true I’ve setup parameter dag_name, to get DAG name which needs to be triggered inside mwaa. Now you can use this parameter as github.event.inputs.dag_name inside job.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 jobs: trigger: steps: - uses: actions/checkout@v2 - name: Trigger workflow env: # setup AWS environments value run: | sudo apt-get install --yes python3-pip cl-base64 pip3 install awscli==1.19.34 # Get token and hostname, for mwaa triggering CLI_JSON=$(aws mwaa create-cli-token --name {YOUR_HOST_NAME}) \\ \u0026\u0026 CLI_TOKEN=$(echo $CLI_JSON | jq -r '.CliToken') \\ \u0026\u0026 WEB_SERVER_HOSTNAME=$(echo $CLI_JSON | jq -r '.WebServerHostname') # Trigger DAG by API CLI_RESULTS=$(curl --request POST \"https://$WEB_SERVER_HOSTNAME/aws_mwaa/cli\" \\ --header \"Authorization: Bearer $CLI_TOKEN\" \\ --header \"Content-Type: text/plain\" \\ --data-raw \"trigger_dag ${{ github.event.inputs.dag_name }}\" ) CLI_STDOUT=$(echo $CLI_RESULTS | jq -r '.stdout' | base64 --decode) CLI_STDERR=$(echo $CLI_RESULTS | jq -r '.stderr' | base64 --decode) echo \"Output: $CLI_STDOUT\" echo \"Errors: $CLI_STDERR\" if [ -z \"$CLI_STDERR\" ] then echo \"DAG has been triggered successfully\" exit 0 else echo \"DAG trigger has been failed\" exit 1 fi MWAA is newely added feature(at point of 2021), so cli command aws mwaa will not work if your awscli tool version is old one. Make sure to upgrade it:\n1 2 $ sudo apt-get install --yes python3-pip cl-base64 $ pip3 install awscli==1.19.34 And run the following command to call API https://$WEB_SERVER_HOSTNAME/aws_mwaa/cli with token and hostname. It will be done correctly if AWS environment has been setup well.\n1 2 3 4 5 6 7 8 9 10 # Get token and hostname, for mwaa triggering CLI_JSON=$(aws mwaa create-cli-token --name {YOUR_HOST_NAME}) \\ \u0026\u0026 CLI_TOKEN=$(echo $CLI_JSON | jq -r '.CliToken') \\ \u0026\u0026 WEB_SERVER_HOSTNAME=$(echo $CLI_JSON | jq -r '.WebServerHostname') # Trigger DAG by API CLI_RESULTS=$(curl --request POST \"https://$WEB_SERVER_HOSTNAME/aws_mwaa/cli\" \\ --header \"Authorization: Bearer $CLI_TOKEN\" \\ --header \"Content-Type: text/plain\" \\ --data-raw \"trigger_dag ${{ github.event.inputs.dag_name }}\" ) The value of CLI_RESULTS looks like:\n1 2 3 4 { \"stderr\":\"\u003cSTDERR of the CLI execution (if any), base64 encoded\u003e\", \"stdout\":\"\u003cSTDOUT of the CLI execution, base64 encoded\u003e\" } so parse the value like following. If the result is OK, there will be no value inside CLI_STDERR.\n1 2 CLI_STDOUT=$(echo $CLI_RESULTS | jq -r '.stdout' | base64 --decode) CLI_STDERR=$(echo $CLI_RESULTS | jq -r '.stderr' | base64 --decode) Optional restriction There can be cases that if you want this work only be done in main branch, to avoid unofficial version to be triggered. You can make action to check the current branch, and decide whether to be triggered or not:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 jobs: trigger: steps: - uses: actions/checkout@v2 - name: Trigger workflow env: # setup AWS environments value run: | # If branch is not `main`, exit the process if [[ \"$GITHUB_REF\" != \"refs/heads/main\" ]] then echo \"Trigger only allowed in `main` branch\" exit 1; fi Or, maybe you could want this triggering job only be allowed to specific users. In this case, you can filter user by GitHub ID. Make file team_member.txt and write down the user’s ID who can run the job, and:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 jobs: trigger: steps: - uses: actions/checkout@v2 - name: Trigger workflow env: # setup AWS environments value run: | # If user is not in the list, exit the process while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"${{ github.actor }} is authorized user\" ALLOWED_USER=${{ github.actor }} break fi done \u003c \"team_member.txt\" if [ -z \"$ALLOWED_USER\" ]; then echo \"Job triggered by unauthorized user\" exit 1 fi There are many more variations to manage complex workflows, or you can make script for customized workflow yourself.\nReference https://airflow.apache.org https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html https://docs.github.com/en/actions ","description":"","tags":["airflow","aws","github","github_action","python","shell_script"],"title":"Deploying DAG to Managed Airflow(AWS), with GitHub Action","uri":"/posts/2021-02-15-deploy-airflow-with-github-action/"},{"categories":null,"content":"Amazon Athena is a service to analyze data stored in S3 with interactive SQL query. You don’t need additional setup, and only need to pay for the queries you’ve run.\nIt is usually being used by data scientists, or business developer who needs to get insights from big data(probably stored inside S3, or else).\nAlso for data engineers, it can be needed to get specific data to construct data process logic. And in this case, they may need to create application to automate querying job. Of course, Amazon is offering SDK for the developers.\nThis is a note of process to receive/treat data executed by Athena query, inside data pipeline.\nUsing official SDK(2.x) Let’s see SDK for Athena first. Newest version for AWS is 2.x.\n1 2 3 4 libraryDependencies ++= Seq( ... \"software.amazon.awssdk\" % \"athena\" % \"2.15.79\" ) First it requires to build client:\n1 2 3 4 5 6 7 8 9 import software.amazon.awssdk.auth.credentials.InstanceProfileCredentialsProvider import software.amazon.awssdk.regions.Region import software.amazon.awssdk.services.athena.AthenaClient val client: AthenaClient = AthenaClient .builder .region(Region.US_WEST_1) .credentialsProvider(InstanceProfileCredentialsProvider.create()) .build Now query can be submitted with client, but there are several configuration to be done:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import software.amazon.awssdk.services.athena.model.{ QueryExecutionContext, ResultConfiguration, StartQueryExecutionRequest } // ... val QUERY_STRING = \"SELECT * FROM user ORDER BY created_at DESC LIMIT 10\" val OUTPUT_PATH = \"s3://output/athena\" val queryExecutionContext = QueryExecutionContext.builder.database(\"athena_project\").build val resultConfiguration = ResultConfiguration.builder.outputLocation(OUTPUT_PATH).build val startQueryExecutionRequest = StartQueryExecutionRequest .builder .queryString(QUERY_STRING) .queryExecutionContext(queryExecutionContext) .resultConfiguration(resultConfiguration) .build This is to submit ’newest 10 row inside user dataset of database athena_project.\nAthena SDK requires user to define ‘output location’ to store result data in S3. This can be useless if you just want to check the result, but it is a requirements, and it will return error when not defined with ResultConfiguration.builder.outputLocation.\nQuery execution can take time depend on database size or complication of query, so need to add wait loop logic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // get id from 'startQueryExecutionRequest' above val queryExecutionId = startQueryExecutionResponse.queryExecutionId() val getQueryExecutionRequest = GetQueryExecutionRequest.builder.queryExecutionId(queryExecutionId).build var getQueryExecutionResponse = null var isQueryStillRunning = true while (isQueryStillRunning) { getQueryExecutionResponse = athenaClient.getQueryExecution(getQueryExecutionRequest) val queryState = getQueryExecutionResponse.queryExecution.status.state queryState match { case QueryExecutionState.FAILED =\u003e throw new RuntimeException(\"The Amazon Athena query failed to run with error message: \" + getQueryExecutionResponse.queryExecution.status.stateChangeReason) case QueryExecutionState.CANCELLED =\u003e throw new RuntimeException(\"The Amazon Athena query was cancelled.\") case QueryExecutionState.SUCCEEDED =\u003e isQueryStillRunning = false case _ =\u003e Thread.sleep(1000) } logger.info(\"The current status is: \" + queryState) } It will execute query execution state every 1 second until state is success or failed(cancel), and move on to next step if query execution has done successfully.\nIf process is going well until here, you can find the result by GetQueryResultsRequest, using query execution ID.\n1 2 val getQueryResultsRequest = GetQueryResultsRequest.builder.queryExecutionId(queryExecutionId).build val getQueryResultsResults = athenaClient.getQueryResultsPaginator(getQueryResultsRequest) Now you can see the result through this.\nBut if this process is inside data pipeline with big data processing engine(such as Spark), you might need to load the result inside spark dataframe. In this case, you can read the result using output CSV file(which defined above).\n1 spark.read.option(\"header\", \"true\").csv(OUTPUT_PATH) But this writing to file + read into dataframe seems causing unnecessary process, because actually it don’t need to write file if it can be read directly to dataframe.\nIn this case, you can use JDBC driver for this approach.\nWith JDBC driver There are several drivers for this, and I’ll go on to with the one introduced officially in AWS. You should download JDBC driver from Athena official website =\u003e https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html\nOne bad thing is, driver has not been updated in maven repository for around 3 years, so several features will not work. You should download the driver library, and put into /lib directory inside project.\n(For example Catalog option will not work with old version)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val driver = \"com.simba.athena.jdbc.Driver\" Class.forName(driver) val connector: String = Seq( s\"jdbc:awsathena://MetadataRetrievalMethod=ProxyAPI\", s\"AwsRegion=us-west-1\", s\"S3OutputLocation=$OUTPUT_PATH\", s\"AwsCredentialsProviderClass=${credential-provider}\", s\"Catalog=${your-catalog}\", s\"Schema=${your-schema}\" ).mkString(\";\") spark.read .option(\"driver\", driver) .option(\"url\", connector) .option(\"query\", QUERY_STRING) .format(\"jdbc\") .load() Detail for connector url can be found here\nIf you need to setup credential-provider, you could refer this.\nReference https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/examples-athena.html https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html https://www.simba.com/products/Athena/doc/JDBC_InstallGuide/content/jdbc/using/intro.htm ","description":"","tags":["scala","spark","aws","athena","sdk","dataframe","jdbc"],"title":"Athena with Scala and Spark","uri":"/posts/2020-12-02-athena-with-sdk-spark-jdbc/"},{"categories":null,"content":"Few days ago, I’ve planned to design some side project, which is based on mobile application. This is the record about how I’ve implement authentication logic through application side to back-end side.\nI’ve once wrote about authentication with golang, but in this time I’ve decided to use nodejs. Still yet golang was not handy to me, and because I’ve changed my position on work, it requires much time to get used on new work(and studying new programming language).\nMoreover, I’ve decided use Flutter, which I’ve never used before, so couldn’t have much time to assign a time for get used to golang.\nWhy Flutter? If the performance is not a high priority for application, it’s inconvenient to develop apps for iOS and Android separately. Luckly there are several ways to develop application for iOS/Android in same code base.\nThere are other frameworks(Xamarin, React-Native) to make this done, but lots of performance benchmark(it’s pretty close to native!), growth of community, and support from Google make me to choose Flutter. Also, there were similarities with ‘component’ based development at front-end.\nOne sad thing(maybe good thing) is there are no other choice than using Dart for development, but thought it worths to go on with this.\nWhy Nestjs NestJS is one of web framework based on NodeJS, like Express, Koa. It has rapidly grow-up in few years and become one of most trendy project, by offering user to build modulized, looseley coupled architecture with less effort.\nCheck the small piece of code:\n1 2 3 4 5 6 7 8 9 import { Controller, Get } from '@nestjs/common'; @Controller('cats') export class CatsController { @Get() findAll(): string { return 'This action returns all cats'; } } As you can see, it actively supports Decorator for clean code. Maybe you can remind of Spring framework in Java, or Angular framework. It also supports TypeScript by default, for type-safety application.\nI was using Express for years but had chance to use this for few month, and impressed in development efficiency. Now I’m not with NodeJS, but wanted to get more deeply with this framework, by apply in this personal project.\nAuthentication logic for Flutter These are the logic for basic authentication.\nOpen application Check state of authentication with stored JWT If JWT is available, show main page If not, move to login page In 2-2, login with ID/PW, and store JWT to local storage You can just call API for each process and check state separately, but let’s try to use ‘provider pattern’ to keep state in unified logic.\nDefine API First let’s define APIs to get/post login informations from server.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 import 'dart:async'; import 'dart:convert'; import 'dart:io'; // flutter import 'package:http/http.dart' as http; import 'package:logging/logging.dart'; import 'package:flutter_secure_storage/flutter_secure_storage.dart'; // local import 'package:app/services/api_response.dart'; import 'package:app/services/api_error.dart'; import 'package:app/models/user.dart'; String _baseUrl = \"base-url-to-call-api\"; Future\u003cApiResponse\u003e authenticateUser(String username, String password) async { ApiResponse _apiResponse = new ApiResponse(); final _storage = new FlutterSecureStorage(); try { final resp = await http.post('${_baseUrl}api/auth/login', body: { 'username': username, 'password': password }); if (resp.statusCode == 200 || resp.statusCode == 201) { Map mapped= json.decode(resp.body); String token = mapped[\"access_token\"]; await _storage.write(key: \"accessToken\", value: token); String tok = await _storage.read(key: \"accessToken\"); return await getProfile(); } else { _apiResponse.ApiError = ApiError.fromJson(json.decode(resp.body)); } } catch (e) { _apiResponse.ApiError = ApiError(error: \"Unknown server error. Please retry\"); } return _apiResponse; } Future\u003cApiResponse\u003e getProfile() async { final _storage = new FlutterSecureStorage(); final token = await _storage.read(key: 'accessToken'); ApiResponse _apiResponse = new ApiResponse(); try { final profile = await http.get( '${_baseUrl}api/users/profile', headers: {HttpHeaders.authorizationHeader: 'Bearer ${token}'} ); if (profile.statusCode == 200 || profile.statusCode == 201) { _apiResponse.Data = Users.fromJson(json.decode(profile.body)); } else { _apiResponse.ApiError = ApiError.fromJson(json.decode(profile.body)); } } on Exception { _apiResponse.ApiError = ApiError(error: \"Unknown server error. Please retry\"); } return _apiResponse; } I’ve used wrapper class for response handler(ApiResponse, ApiError). About this, you can find more in here. It is one kind of method to keep common rule for response handling. This post will only focus on managing login state.\nThere are 2 APIs here:\n${_baseUrl}api/auth/login -\u003e login with ID/PW ${_baseUrl}api/users/profile -\u003e get profile with token stored in device. It is to check user is logged in or not. with provider pattern Provider pattern is one of design pattern, to manage application state. In this pattern, states are being managed in separate module, and views which requires these information will check states from this module. If you’re familiar with react-redux or vuex, you can understand more quickly.\nFor implementation, you need to add provider module.\n1 2 3 dependencies: ... provider: ^4.3.2 First, let’s define ‘state’ of login. Basically, you can think of ’loading’, ‘success’, ‘fail’,　but you can divide into more segments if needed.\n1 enum LoginState { Loading, Success, Fail } Now will make provider class. For this, you need to extend ChangeNotifier. This is a simple class included in the Flutter SDK which provides change notification to its listeners. By extending this, user can subscribe to its change.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class AuthProviders with ChangeNotifier { LoginState _state = LoginState.Fail; LoginState get state =\u003e _state; AuthProviders() { this._state = LoginState.Loading; getProfile().then((resp) { ApiResponse response = resp; if (response != null \u0026\u0026 response.Data != null) { _log.info('success auth api call'); this._state = LoginState.Success; } else { _log.info('fail auth api call'); this._state = LoginState.Fail; } notifyListeners(); }).catchError((onError) { this._state = LoginState.Fail; notifyListeners(); }); } Future\u003cbool\u003e login(String id, String password) async { this._state = LoginState.Loading; notifyListeners(); try { await authenticateUser(id, password); } catch (e) { this._state = LoginState.Fail; } notifyListeners(); } } First, you can see line LoginState _state = LoginState.Fail, for initiating state. Init value is ‘fail’ because it didn’t started(or you can make state ‘Init’ for this).\nWhen user try to call API to login or check state, it will change state as Loading until it gets the result(Success/Fail).\nJWT inside Nestjs Nestjs offers various features for web server development as module, and so as authentication. I’m able to check whether user is authenticated or not by checking value inside ‘Bearer Token’ is valid one. It can be checked by parsing header with code, but Nestjs offers smarter way with passport.js module.\nIn Nestjs, it offers Guards class annotation. As you can expect by the name, it makes whether a given request will be handled by the route handler or not, depending on certain conditions (like permissions, roles, ACLs, etc.) present at run-time. This is pretty useful function for authentication logic.\nAlso, it makes user to integrate Strategy which is offered from passport module.\nPassport has a rich ecosystem of strategies that implement various authentication mechanisms. While simple in concept, the set of Passport strategies you can choose from is large and presents a lot of variety\nFirst setup scaffold with Nest CLI, install additional modules for this:\n$ npm install --save passport passport-jwt @nestjs/passport @nestjs/jwt $ npm install --save-dev @types/passport-jwt @nestjs/passport is extention for Nestjs, which will offer extendable module for authentication.\nBefore making authentication guards, make API controllers for API defined in flutter app.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import { Controller, Get, Post, Logger, Request, UseGuards, } from '@nestjs/common' import { AuthService } from './auth/auth.service' @Controller() export class AppController { constructor(private readonly authService: AuthService) {} // POST: login user @Post('api/auth/login') async postLogin(@Request() req) { return this.authService.login(req.user) } // GET : return user profile @Get('api/user/profile') async profile(@Request() req) { return this.authService.getProfile(req.user) } } I’ll make api/user/profile to be blocked if token is not included or invalid, and api/auth/login to return token for this.\nFirst, for login:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import { Injectable, Logger } from '@nestjs/common' import { JwtService } from '@nestjs/jwt' @Injectable() export class AuthService { private readonly logger = new Logger(AuthService.name) constructor( private readonly jwtService: JwtService, ) {} async login(user: any) { const payload = { username: user.username, sub: user.password } return { access_token: this.jwtService.sign(payload), } } } If login has been succeed, it will return access_token which will be used for authentication. Now make Guards for this…\n[jwt-auth.guard.ts]\n1 2 3 4 5 import { Injectable } from '@nestjs/common' import { AuthGuard } from '@nestjs/passport' @Injectable() export class JwtAuthGuard extends AuthGuard('jwt') {} and define Strategy for JWT:\n[jwt-strategy.ts]\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import { ExtractJwt, Strategy } from 'passport-jwt' import { PassportStrategy } from '@nestjs/passport' import { Injectable } from '@nestjs/common' const jwtConstants = { secret: 'secretKey', } @Injectable() export class JwtStrategy extends PassportStrategy(Strategy) { constructor() { super({ jwtFromRequest: ExtractJwt.fromAuthHeaderAsBearerToken(), ignoreExpiration: false, secretOrKey: jwtConstants.secret, }) } async validate(payload: any) { console.log(payload) return { userId: payload.sub, username: payload.username } } } and call with UseGuards\n1 2 3 4 5 6 7 8 9 10 11 12 13 import { Controller, Get, Request, UseGuards, } from '@nestjs/common' import { JwtAuthGuard } from '../jwt-auth.guard' // ... @UseGuards(JwtAuthGuard) @Get('api/user/profile') async profile(@Request() req) { return this.authService.getProfile(req.user) } This will make api/user/profile protected from unauthorized call…\n$ curl http://localhost:3000/api/user/profile $ # result -\u003e {\"statusCode\":401,\"error\":\"Unauthorized\"} $ curl http://localhost:3000/api/user/profile -H \"Authorization: Bearer \u003creceived-auth-token\u003e\" $ # result -\u003e {\"userId\": \"user-id\", \"username\": \"user-name\"} You can find full example here.\nReference https://flutter.dev/docs https://mundanecode.com/posts/flutter-restapi-login/ https://docs.nestjs.com/ https://docs.nestjs.com/security/authentication ","description":"","tags":["web","flutter","dart","nestjs","nodejs","typescript"],"title":"Making authentication logic for Flutter + Nestjs","uri":"/posts/2020-10-20-authentication-with-flutter-nestjs/"},{"categories":null,"content":"I’ve had a change to join on hackathon event few weeks ago, and participate as developing backend with infra setup. In my mind hackathon event has more meaning on testing experimental things instead of making stable result, so I’ve decided to try on ‘Cloud Run’.\nCloud Run on Google Cloud Platform(GCP) Year ago, google has launched new solution Cloud Run, for running serverless application from docker image. It is fully managed compute platform for deploying and scaling containerized applications quickly and securely. It makes user can manage and deploy website without any of the infrastructure overhead you experience with VM or pure kubernetes based deployments.\nFrankly, I’ve worked on developing front-end side to back-end side, but didn’t involved deeply at infrastructure. Just touching the tool or scripts which platform members prepared…so it seems good selection to me.\nProcedure here is not talking about the optimized way. It just helps you how to come up to the scratch of using this, and let you know it is pretty easy to start on if you know about understanding of docker.\nBefore going on…\nyou need to install gcloud create project in GCP Dockerize scaffolds for develop environment In my hackathon team, there was great members who can make front-end UI based on React, and machine learning logic for recommendation system. So I could focus on design basic directory structure and simple API logic. Because recommendation logic was based on Python, I’ve choose Django for framework.\nFirst thing I’ve done is to make docker environment, for quick development + deployment. Because Cloud Run is running based on k8s, application should be deployed as docker container.\nI’ve started with scaffolding client/server part with common scripts(create-react-app ..., django-admin startproject ...), and add Dockerfile for each one:\n[client/Dockerfile]\nFROM node RUN mkdir -p /app/frontend WORKDIR /app/frontend COPY package.json package-lock.json /app/frontend/ RUN npm install COPY . /app/frontend/ EXPOSE 3000 CMD [\"npm\", \"start\"] [server/Dockerfile]\nFROM python:3.7 RUN apt-get update \\ \u0026\u0026 apt-get install -y --no-install-recommends \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* WORKDIR /app/server COPY requirements.txt /app/server RUN pip install -r requirements.txt EXPOSE 8000 CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] [docker-compose.yml]\nversion: \"3\" services: server: build: ./server command: [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] stdin_open: true volumes: - ./server:/app/server ports: - \"8000:8000\" frontend: build: ./frontend command: [\"npm\", \"start\"] volumes: - ./frontend:/app/frontend - node-modules:/app/frontend/node_modules ports: - \"3000:3000\" stdin_open: true depends_on: - server volumes: node-modules: Now with docker-compose, it will make common develop container by docker. I like making docker setup, because it makes all team developers work on same base environment, and don’t need to make detail guide.\nConfiguration inside application Because we will run projects inside one container, you should change proxy address on client, to target correct server address. If you generate client project with create-react-app, you can fix on client/src/setupProxy.js\n1 2 3 4 5 const { createProxyMiddleware } = require('http-proxy-middleware') module.exports = function (app) { app.use(createProxyMiddleware('/server', { target: 'http://server:8000/' })) } and setup some configuration for server in settings.py\n1 2 3 4 5 6 7 8 ... ALLOWED_HOSTS = [os.environ.get('CURRENT_HOST', 'localhost'), '127.0.0.1'] STATIC_URL = \"/static/\" STATICFILES_DIRS = ( # update the STATICFILES_DIRS os.path.join(BASE_DIR, \"build\", \"static\"), ) Now it’s done for local development.\nPre-config to build image I’ll make one more Dockerfile in root, to generate application docker image which will be uploaded to Cloud Run.\n[Dockerfile]\nFROM python:3.7 RUN apt-get update \\ \u0026\u0026 apt-get install -y --no-install-recommends \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* WORKDIR /app/server COPY ./server/requirements.txt /app/server/ RUN pip install -r requirements.txt COPY . /app/ WORKDIR /app EXPOSE $PORT CMD python3 server/manage.py runserver 0.0.0.0:$PORT Okay, after you’ve finished development, build projects to prepare image build. Of course, this can be different by your settings.\n1 2 3 4 5 6 // build front-end bundle file $ cd frontend \u0026\u0026 npm install \u0026\u0026 npm run build $ cd .. // copy bundle to server $ cp -R frontend/build server/ Run image on Cloud Run Now you should have built image file in local. You should have gcloud here, as commented above. Let’s build and upload image from local project, to your GCP project.\nRun this command where Dockerfile exists, to build image. Do it with gcloud as:\n$ gcloud builds submit --tag gcr.io/${project-ID}/${image-name}:${image-version} gcloud builds submit --tag gcr.io/${project-ID}/${image-name}:${image-version} . ✹ ✭ Creating temporary tarball archive of 36186 file(s) totalling 336.8 MiB before compression. Some files were not included in the source upload. Check the gcloud log [/Users/yourname/.config/gcloud/logs/2020.07.01/20.52.43.963015.log] to see which files and the contents of the default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn more). ... ... or, you can just use docker command:\n$ docker build -t gcr.io/${project-ID}/${image-name}:${image-version} . ... $ docker push gcr.io/${project-ID}/${image-name}:${image-version} Now press Cloud Run on option inside dashboard:\nCreate new service here\nMake service setting for container.\nIf image has been uploaded correctly, you can find your image when press SELECT. Create when done\nWait for a while for service upload. You could find auto-generated url to access your service when done.\nReference https://cloud.google.com/run https://github.com/18F/docker-compose-django-react ","description":"","tags":["web","django","react","gcp","cloudrun","deploy"],"title":"Running Django + React service by Cloud Run","uri":"/posts/2020-06-30-cloud-run-django-react/"},{"categories":null,"content":"If you are starting to make some kind of web(or app) service, it is mandatory of thinking about authentication.\nEvery service has information to show, and it should divide public/private information to be displayed. If you are thinking of financial service, certification and security becomes even more important.\nAnd now I’m starting to make some personal web project, and this is how I setup basic authentication logic.\nReact, and Golang Every people who starts to implement web/app service, will think first of\nwhich language/framework to use\nYou could just make it with something they’re most used to, if you are tending to finish project as soon as possible.\nOr maybe you can challenge on using new stuffs which you’ve never used at work, because make something can be the quickest way to learn.\nWell, this can cause delay on finishing project, so you can decide by which point you are focusing on.\nWhy I’ve select React for front-end is, it has been a year since I haven’t look into this(used Vue in current work), so I needed a motivation to refresh my knowledge on React. It is still most popular FE framework, and don’t wanted to slip my mind.\nGolang…actually I don’t have any relationship with this. Didn’t have chance to use on work, and honestly, don’t like simplicity concept cause it seems this makes project implemented with Golang more complex.\n(My knowledge on Golang is very poor, so this point of view can be changed…)\nThough regardless how I think of this, it’s usage is growing exponentially globally, and simplicity of Golang helps user to make service with great performance fast, with short code.\nSo though there’s other handy programming languages, to try on golang, I’ve decided to bump up to this(it can be changed…).\nBasic process of authentication with JWT Basic rule of authentication we can think easily, is to setup token data in header when requesting through API. And usually JWT is being widely used for generating this token.\nJWT token is…\nJSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA…\nFrom : https://jwt.io/introduction/\nAs you can see it includes various information, and it makes service developer can control expiration time, access scope of request with this.\nJWT is composed with 3 feature, Header, Payload, and Signature.\n‘Header’ ‘Header’ contains informations which needs to verify then token. There are algorithm and type of token.\n1 2 3 4 { \"alg\": \"HS256\", \"typ\": \"JWT\" } This will be encoded in Base64Url, and become first part of JWT.\n‘Payload’ ‘Payload’ contains statements about an entity and additional data(called ‘claims’ in document). It basically have information about\niss (issuer) exp (expiration time) sub (subject) iat (issued at) and several more public data like name, email…\n1 2 3 4 5 { \"sub\": \"1234567890\", \"iat\": 1516239022, \"name\": \"John Doe\" } ‘Signature’ ‘Signature’ part is generated by signing encoded header, encoded payload, a secret with the algorithm specified in the header. For example:\nHMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), your-256-bit-secret ) Result Now, this is dummy token which has been generated with 3 part above:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c and as you can see, there are dot(.) character which works as separator of informations.\n{HEADER}.{PAYLOAD}.{SIGNATURE}\nSetup JWT middleware with golang + echo There are several suggestions that ‘modern web framework’ does not need for web development with Golang. They said it is enough with using standard library, and several utilities(such as Gorilla).\nBut using framework still gives several convenience, by offering defined function which is widely used, and good documantation and reference.\nAnd Echo,\nwhich I will use for web development, is one of popular web framework containing these advantages, with minimal size(I don’t prefer heavy full-framework).\nOf course, there are defined middleware for JWT.\nJWT middleware You can setup JWT config by:\n1 2 3 4 5 6 7 8 9 10 11 12 type jwtCustomClaims struct { Name string `json:\"name\"` Admin bool `json:\"admin\"` jwt.StandardClaims } // ... config := middleware.JWTConfig{ Claims: \u0026jwtCustomClaims{}, SigningKey: []byte(\"secret\"), } e.Use(middleware.JWTWithConfig(config)) In this example, it uses custom claim to extend information to be included in JWT. StandardClaims includes:\n1 2 3 4 5 6 7 8 9 type StandardClaims struct { Audience string `json:\"aud,omitempty\"` ExpiresAt int64 `json:\"exp,omitempty\"` Id string `json:\"jti,omitempty\"` IssuedAt int64 `json:\"iat,omitempty\"` Issuer string `json:\"iss,omitempty\"` NotBefore int64 `json:\"nbf,omitempty\"` Subject string `json:\"sub,omitempty\"` } so you can put personal data which is not here, on extended type.\nThis middleware will automatically send response as:\nFor valid token, it sets the user in context and calls next handler. For invalid token, it sends ‘401 - Unauthorized’ response. For missing or invalid Authorization header, it sends ‘400 - Bad Request’. Now let’s see how to setup this on API router.\nSetup authentication logic with Golang web server Let’s assume that we are developing online shopping mall like amazon.\nDon’t mind of detail, and think of showing\nMy information Items in main page.\nProcess rule should be:\nItems should be displayed whether user logged in or not User information should be only displayed when user logged in User could log in only when user are not logged in and in short, responses should be:\nImplementation Echo also offers easy way to group APIs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type jwtCustomClaims struct { Name string `json:\"name\"` jwt.StandardClaims } func main() { e := echo.New() // define apis to 'apiGroup' which doesn't require token apiGroup := e.Group(\"/api\") apiGroup.POST(\"/login\", Login) apiGroup.GET(\"/items\", ...) // define apis to 'apiAuthGroup' which requires token apiAuthGroup := e.Group(\"/api/user\") config := middleware.JWTConfig{ Claims: \u0026jwtCustomClaims{}, SigningKey: []byte(\"secret\"), } apiAuthGroup.Use(middleware.JWTWithConfig(config)) apiAuthGroup.GET(\"/user-profile\", ...) // ... e.Logger.Fatal(e.Start(\":1323\")) } APIs which is defined in apiGroup(apis which starts with /api) will not be effected by JWT in header, but apiAuthGroup(apis which starts with /api/user) will only send ‘200 OK’ when request includes appropriate token in header.\nNow it needs to return token data in /api/login, to let sender use after login succeeded.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 type LoginParam struct { UserId string `json:\"userId\"` Password string `json:\"password\"` } func Login(c echo.Context) error { lp := new(LoginParam) if err := c.Bind(lp); err != nil { return echo.ErrUnauthorized } id := lp.Username pw := lp.Password if id != \"user-id\" || pw != \"1234\" { return echo.ErrUnauthorized } claims := \u0026jwtCustomClaims{ id, jwt.StandardClaims{ ExpiresAt: time.Now().Add(time.Hour * 72).Unix(), }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) t, err := token.SignedString([]byte(\"secret\")) if err != nil { return err } return c.JSON(http.StatusOK, echo.Map{ \"token\": t, }) } Reference https://echo.labstack.com https://jwt.io/introduction/ ","description":"","tags":["web","server","golang","authentication","jwt","echo"],"title":"Authentication process on golang web server","uri":"/posts/2020-05-22-authentication-process-golang-web-part-1/"},{"categories":null,"content":"CI/CD in software engineering is a process to apply developer’s work to project. It includes maintaining code, compiling process, testings(unit, e2e), and all other things for software updates. This phrase has been used more than 20 years, and various tools for integration/deployment has been created during 2 decades.\nJenkins is open-source automation server for continuous integration/delivery of project. It is one of most popular tools for CI/CD, and I’ve also worked with this for several years.\nContainerizing has been commonly used in software development(powered by docker, k8s) from several years ago, and jenkins also applied features for treating containerized project.\nCurrent service I’m working on, is managed on Kubernetes(k8s) cluster, with various docker images.\nMotivation By jenkins main page, you can monitor current build pipelines, and create new one if you needed. But if project becomes bigger, jobs and pipelines will be pile up, and finding your target job from GUI dashboard will become inconvenient. Well, actually this is pretty simple process comparing with old-fashion logics, but anyway I wanted to make it more simple.\nThankfully, jenkins offers APIs to control functions and monitor job/pipeline status through HTTP request/response. So I’ve planned to make build-deploy process through command line interface(CLI) so not need to access dashboard for software updates.\nI’ve implemented and using it well as in-house private project. It saved lot of time accessing dashboard, going inside target pipeline, and monitoring the build status.\nLife is short, you need Python Cannot say Python is a best programming language, but at least it is best option to me, for developing quick scratch. It helps developer to create logics with short lines comparing with other languages, and that’s why people said Life is short, you need Python. Moreover, jenkins are offering wrapped-up API for python, so you don’t need to use http call directly.\nAlso there are great packages for CLI developments, so it helped me to make this fast and neat. Let’s see the basic settings for making package. There are several packages for CLI, so I’ve used click(I’ve used 7.x) for this plan.\nLet’s name this project as mycli. I’ve setup ‘click’, and python-jenkins package to access jenkins.\nSetup virtual-env, add packages on requirements.txt, and create setup.py to define package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 with codecs.open(os.path.join(here, 'requirements.txt')) as f: install_requirements = [ line.split('#')[0].strip() for line in f.readlines() if not line.startswith('#') ] setup( name='mycli', version='0.1.0', description='bla bla bla', install_requires=install_requirements, py_modules=['mycli'], entry_points=''' [console_scripts] mycli=mycli:cli ''', ) Now make mycli.py, and let’s define command to call user information. You need to get API token from user config page.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import click import jenkins @click.group() def cli(): pass # put command name as parameter in 'cli.command' if you want to use other name for command @cli.command() def user(): \"\"\" Get user information \"\"\" host = 'https://jenkins-url' try: server = jenkins.Jenkins( host, username='user-email', password='user-api-token' ) user_info = server.get_whoami() click.echo('Jenkins user:' + str(user_info)) except Exception as e: raise Exception(e) Basic configuration are prepared. Install packages and try user command to check this works well.\n$ pip install -e . ... $ mycli user ... Jenkins user: .... Design process for build and git-ops deployment GitOps is a way of implementing deployments for cloud native applications, by controlling containers with separated git repository.\nThe core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment match the described state in the repository. If you want to deploy a new application or update an existing one, you only need to update the repository - the automated process handles everything else…\nSo I’ve designed flow as below:\nand planned to create command for build(center part of image), and deploy(right side of image).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @cli.command('build') def build_job(): \"\"\" Build jenkins job \"\"\" host = 'https://jenkins-url' try: server = jenkins.Jenkins( host, username='user-email', password='usser-api-token', timeout=30 ) queue_number = server.build_job('project-name') from time import sleep; sleep(10) build_info = server.get_build_info('project-name', next_build_number) job_info = server.get_job_info(queue_number) except Exception as e: raise Exception(e) By definition of your project, either you can need build info or job info after you triggered the build. Because build takes few seconds to boot-up after started, you need to define delay(usually 10 seconds are OK) before getting build informations.\nControl git-ops repository After build has been completed, you need to update container information through config files managed by git. So deploy command will be:\n‘git pull’ for get newest configuration Update configuration file ‘git push’ to push changed configuration As you expected, there are also packages for git operation GitPython. Install and try:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from git import Repo @cli.command('deploy') def deploy_container(): \"\"\" Deploy container through git \"\"\" repo_url = 'https://git-ops-repo.url' repo = Repo(repo) repo.git.checkout('master') repo.git.pull() # fix configuration file # ... repo.index.add(['list-of-changed-file']) repo.index.commit('commit-message') origin = repo.remote(name='origin') origin.push() This is how to control repository by commented order. It will pull the repository in master branch, (fix container image info in configuration file to new one), and push it to apply cluster. You need to put your logics to update newest build status in configuration files(usually defined as .yml or .json) at commented part.\nConclusion Now, let’s check the tool:\n$ mycli --help ⏎ Usage: mycli [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. Commands: build Build jenkins job deploy Deploy container through git user Get user information As you can expected, this is concise version of real tool I’m using on. But because it includes major features, you can refer it to make your own tool which can help your development life just as it did to me.\nReference https://www.jenkins.io/ https://www.gitops.tech/ https://click.palletsprojects.com ","description":"","tags":["cli","python","jenkins","gitops","docker"],"title":"Developing command line tool for build \u0026 deployment","uri":"/posts/2020-04-20-cli-tool-for-build-deploy/"},{"categories":null,"content":"\nDesigning Java API API is a gate, entrance of the service for the user. Developer who is willing to use the service will research API design carefully, and make implementation plan based on this. It means good API design will make user’s code more clear and simple, and let them keep use our service consistently.\nBut we couldn’t sure how the user will integrate this, so cannot satisfy everyone. And once you’ve release officially, it is far more difficult to remove it. That would be the reason every document about API design saids ‘make it simple’. In other words, it should be ’easy to understand, use, extend’.\nShould be intuitive enough Can be used without strict condition(if possible) Function of API can be extend easily And, if possible, we should think of performance too, to not harm user’s service.\nBased on these suggestions, let’s think about how to make Java API. As you know there are tons of guide about good API design, and most of them are defined more than 10 years. These are the points which I’m bit more focusing on when creating Java APIs.\nMake it easy About ways to make it easy, we can think of ’naming’ first. Name should be self-explanatory as possible. It should describe what is doing, and avoid abbreviations.\n1 2 3 4 5 6 7 8 9 10 11 Car myCar = new Car(); /** assume it needs method to return current mileage of car */ myCar.currentMileage(); // good naming myCar.currMil(); // bad myCar.abc(); // horrible No doubt that documentation is important, but most developers will get information directly from code before reading long document. Intuitive naming will help developer reduce time searching document for understanding the function.\nAlso, names in same package should follow consistent naming and convention. This seems easy, but hard to keep while your service is growing up. You can even find on several standard Java APIs.\n1 Stream.of(2, 3, 4, 5).skip(2) // 3, 4, 5 skip(long n) in Stream API will skip n item of the stream data, and\n1 Stream.of(1, 2, 3, 4, 5).dropWhile(n -\u003e n % 2 == 1) // 2, 3, 4, 5 dropWhile(Predicate\u003c? super T\u003e predicate) is returning remaining element after dropping a subset of elements that match the given predicate. Though both offers similar operation, it has different naming. Of course, from functional point, they have proper name. But in developer’s side, if they only knows skip, they will think name skipWhile first when they need to search this kind of API.\nMake things immutable Making return type as Immutable can give plus for this. As you know, making ‘mutable’ object will require memory alloc/free which can harm performance. Though making object immutable will take more memory, I think you don’t needs to worry in most of cases.\nIt’s a bit old story, but for example, common Java function Component.getSize() is returning mutable Dimension object. It means JVM needs to allocate memory for this every time when function called. Though user only need to know the size of component, it causes massive useless memory alloc/free process.\nJava has several APIs which makes things immutable.\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Cart { List\u003cString\u003e currentItems; // ... setItem(String item) { currentItems.add(item); } List\u003cString\u003e getCurrentItems() { return Collections.unmodifiableList(currentItems); // preferred return currentItems; // bad } } Static factory method This is to call static method inside class for getting class instance, instead of calling new. It can be find very easily, such as:\n1 2 Stream\u003cInteger\u003e stream = Stream.of(1, 2, 3, 4, 5); String value = String.valueOf(1); It is one of popular design pattern, and it offers several advantages:\nYou can give meaningful name, more than just ’new’ You can make returning type more flexible, same type that implements the method(s), a subtype, and also primitives Static factory methods can be controlled-instanced methods. You can make class as Singleton more easily. Null handling Returning null is a bad option. If user didn’t setup check logics, they will see friendly NullPointerException. Though they prepared for that case, code will not look good. Let’s think of case that MyClass.getMyHobbies will return the list of string.\n1 2 3 4 5 MyClass myClass = MyClass.create(...); List\u003cString\u003e hobbies = myClass.getMyHobbies(); if (hobbies != null) { } For safety, user should check null every time when calling this.\nOne way is to return empty collection for this case. Java offers several API for this:\n1 2 3 return Collections.emptyList(); return List.of(); // for Java 9(or above) user If you are targeting Java 8, use Optional can be another option. It is a container object which may or may not contain a non-null value. These kinds of object are being applied on many other modern programming language(such as Option in Rust, or Maybe in Haskell), for avoiding null exceptions.\nBrian Goetz, Java Language Architect at Oracle, explained about this:\n…Our intention was to provide a limited mechanism for library method return types where there needed to be a \u003e clear way to represent “no result”, and using null for such was overwhelmingly likely to cause errors.\nWe can return Optional instead of\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class MyClass { public static MyClass create(...) { return new MyClass(...); } Optional\u003cList\u003cString\u003e\u003e getMyHobbies() { // generate the list... if (emptyList) { return Optional.empty(); } return Optional.of(listObject); } } This will throw Optional container of list. User can receive this by:\n1 2 3 4 MyClass myClass = MyClass.create(...); List\u003cString\u003e hobbies = myClass.stream() .flatMap(Optional::stream) .collect(Collectors.toList()); Or instead of using empty and of by status, you can use ofNullable. It will check the status of list and return, whether it is null or not.\n1 Optional.ofNullable(listObject); Reference http://www.cs.bc.edu/~muller/teaching/cs102/s06/lib/pdf/api-design http://tutorials.jenkov.com/api-design/index.html https://www.artima.com/interfacedesign/contents.html https://medium.com/97-things/lets-make-a-contract-the-art-of-designing-a-java-api-854441cd42f5 https://www.codebyamir.com/blog/stop-returning-null-in-java https://www.slideshare.net/MiroCupak/the-good-the-bad-and-the-ugly-of-java-api-design-179537151 https://jonathangiles.net/presentations/java-api-design-best-practices/ ","description":"","tags":["java","api","design"],"title":"API design for Java module","uri":"/posts/2020-03-11-api-designing-java/"},{"categories":null,"content":"Making from scratch is a typical way to learn something. Because most of modern programming languages has its own package manager, it is simple to get implemented module with single command. But it does not mean I totally understand how that works, so I’ve plan to start review \u0026 mocking project by topic.\nThe motivation was GitHub project, build-your-own-x. You can find lots of posts about building something from scratch here.\nI’ll go deep with key-value store on this post.\nKey-Value store As you can expect on the name, key-value store(or storage, or database) is a data storing module which keeps data as key-value format. Most simply you can think of Object in javascript, or dict in python. These are keeping value as key-value set, and let user can access value by key(or opposite). Most of them are optimized of getting value from key by hash-table, or some other logics.\nIt is type of non-relational data, so have different working logic with RDB. It treat the data as a single opaque collection, which means it can have different fields for every record. It makes storage easy to scale, and give more flexibility.\nConcept of log-structured file system Though computing performance has been improved massively(and still going on), data access performance is still big issue for software engineering field. Log-structured file system(aka LFS) is one theory for this approach.\nFundamental of Log-structured file system(aka LFS) has been introduced in early 90’s. In that time, people investigated sequential IO will be getting faster than random IO by increasement of physical RAM. So first approach is to\nKeep written data on in-memory segment, and write to log when it is full Segment write is handled in sequential manner Let’s see how this buffer written in disk. This is the data structure which is being used on LFS:\nInodes: Physical block pointers to files. Inode Map: This table indicates the location of each inode on the disk. The inode map is written in the segment itself. Segment Summary: This maintains information about each block in the segment. Segment Usage Table: This tells us the amount of data on a block. Consider the following figure, showing a data block D written onto the disk at location A0. Along with the data block is the inode, which points to the data block D. Usually, data blocks are 4 KBs while inodes are about 128 bytes in size.\n‘D’ is the data block and it is being stored at A0. After data block, there are following inode which has the location of the ‘D’. It makes data stored sequentially and make data accessible via inode.\nI’ll not go deeper about LFS for now, because it is out of scope of what I’m trying to do. For more, you could refer here, or this.\nBasic implementation My implementation is following the part of guide in pingcap/talent-plan. It includes the fundamental of key-value store(following the design of bitcask project), and other great training programs. Prefer to try on if you’re available…\nI’ve started with including most basic function on key-value store, put, get, and open. In bitcask document, it saids:\nWrites are append-only, not overwriting existing file. It forces writing to be sequential, and do not need to ‘seek’ for writing. The key to value index exists in memory. To follow this rule, it keeps Map object in memory, and stores key with position information.\nFor get function, it will access via position information in this ‘Map’ object where key-value exists.\nWhen open event happens, it will create new log file instead of overwriting existing one, and put current log info to in-memory for writing purpose. In put, it will keep end position of buffer and let next put event to refer this, to keep key-value in sequential order.\nReal bitcask rule is way more complicate for disk space optimization, concurrency, and more. But for now, I’ll only think about this feature. Probably I could keep work on in the future.\nOpen Make to object readers and index_map. It is to get data buffer for log, and position information of key-value pairs. Then make logs_list to get list of data files in log directory.\n1 2 3 4 let mut readers = HashMap::new(); let mut index_map = BTreeMap::new(); let logs_list = sorted_logs_list(\u0026path).unwrap(); For every logs, setup index(position) information for each map in log.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // ... let mut reader = BufReaderWithPos::new(File::open(log_path(\u0026path, log))?)?; load_log_index(gen, \u0026mut reader, \u0026mut index_map)?; readers.insert(gen, reader); // ... fn load_log_index( gen: u64, reader: \u0026mut BufReaderWithPos\u003cFile\u003e, index: \u0026mut BTreeMap\u003cString, CommandPos\u003e, ) -\u003e Result\u003cu64\u003e { let mut position = reader.seek(SeekFrom::Start(0))?; let mut stream = Deserializer::from_reader(reader).into_iter::\u003cCommand\u003e(); let mut uncompacted = 0; while let Some(cmd) = stream.next() { let new_pos = stream.byte_offset() as u64; match cmd? { Command::Set { key, .. } =\u003e { if let Some(old_cmd) = index.insert(key, (gen, position..new_pos).into()) { uncompacted += old_cmd.len; } } } position = new_pos; } } After index has been generated, index map will be like:\n\"key-0\": CommandPos { gen: 3, pos: 0, len: 41 }, \"key-1\": CommandPos { gen: 3, pos: 41, len: 41 }, \"key-2\": CommandPos { gen: 3, pos: 82, len: 41 }, \"key-3\": CommandPos { gen: 3, pos: 123, len: 41 }, ... 1 let writer = new_log_file(\u0026path, current_gen, \u0026mut readers)?; Now create new log file, and keep writer object to in-memory buffer for write-only.\nAppend data On put event, get ‘position’ information from writer object, and setup new key-value pair on the end. After then, update index store.\n1 2 3 4 5 6 7 8 9 10 let cmd = Command::set(key, value); let pos = self.writer.pos; serde_json::to_writer(\u0026mut self.writer, \u0026cmd)?; self.writer.flush()?; self.index_map.insert( key, (self.current_gen, pos..self.writer.pos).into() ) // ... Read from key 1 2 3 4 5 6 7 8 9 10 // ... if let Some(cmd_pos) = self.index_map.get(\u0026key) { let reader = self .readers .get_mut(\u0026cmd_pos.gen) .expect(\"cannot find reader\"); reader.seek(SeekFrom::Start(cmd_pos.pos))?; let cmd_reader = reader.take(cmd_pos.len); if let Command::Set { value, .. } = serde_json::from_reader(cmd_reader)? { For get, if key exists in index_map, it will find position information and get data from position in reader object.\nFor now… I’ve update this work in my repository. I’ve named as bawikv, motivated from rocksdb(‘bawi’ means ‘rock’ in Korean). Current work is just a copycat of pingcap/talent-plan project, but I’m planning to add more features to make this affordable for external user.\nThere will be a chance to treat about this.\nReference https://github.com/pingcap/talent-plan https://github.com/basho/bitcask/blob/develop/doc/bitcask-intro.pdf https://github.com/andresilva/cask https://www.geeksforgeeks.org/log-structured-file-system-lfs/ https://www.eecs.harvard.edu/~cs161/notes/lfs.pdf ","description":"","tags":["build-your-own-x","rust","key-value-store","storage","fundamental"],"title":"What is key-value store? (build-your-own-x)","uri":"/posts/2020-01-26-build-your-own-kv-store/"},{"categories":null,"content":"Microservice, or Microservice architecture is not a special technology anymore. There are lots of project which will help build-up micro-service quickly, and major cloud service provider is offering options for this too. And engineers are starting to think the efficiet ways to offer/use micro-service based system.\nAPI Gateway and BFF(Back-end for Front-end) Basically, microservice is composed of multiple services. It means that all of them are included in different machines with different hosts.\nWhen calling API, you need to setup different host to get information from different domain. Moreover, you need to check authentication data(such as token) everytime before requesting.\nGateway is not just making single request point. It encapsulates the internal system, and make single interface which can work as ‘gate’ of total infrastructure. Service only needs to handle lot of parts related with authentication, load balancing, caching, and more inside this gate.\nOf course, there are some problems.\nBecause all API reqeust are coming through this gate, it causes bottle-neck issue. Different type of devices could require different data, but it cannot cope with it flexable. How they can do is to make a new(or fix current ones) API in gateway. So some smart engineers in SoundCloud make a new approach for this, and named as BFF(Back-end for Front-end) pattern.\nThis is probably the most famous diagram which explains BFF pattern. Instead of designing API in gateway to handle every cases(or devices), make separate ‘gateway’ for different type of clients(web, mobile).\nActually, these 2 kinds of pattern are logically similar. It is to make layer in the middle between client and server, to arrange the request and handle common functions in single point.\nIf you want to know more, please take a look.\nProxy for Express Framework Most simple design for BFF(or Gateway) is to use proxy. There are good open source module http-proxy-middleware for NodeJS user to implement it with ease.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 const proxy = require('http-proxy-middleware') // ... function onProxyReq(proxyReq, req, res) { // do something on request } function onProxyRes(proxyRes, req, res) { // do something on response } function onError(err, req, res) { res.writeHead(500, { 'Content-Type': 'text/plain' }) res.end('Something went wrong. And we are reporting a custom error message.') } app.use( '/api/v1/order', proxy({ target: 'https://order-service-host/', changeOrigin: true, onProxyReq, onProxyRes, onError }) ) app.use( '/api/v1/shipping', proxy({ target: 'https://shipping-service-host/', changeOrigin: true, onProxyReq, onProxyRes, onError }) ) In this case, every API requests which includes /api/v1/order will be redirect to microservice with domain https://order-service-host/. If we need to modify request or response data, we can do it by default middleware offered by this module.\nFor example, if you want to add ‘Bearer’ token for every request, it can be done as:\n1 2 3 4 5 function onProxyReq(proxyReq, req, res) { // add custom header to request const token = apiToGetBearerToken() proxyReq.setHeader('Authorization', `Bearer ${token}`) } or you could want to send error logs to ELK for alert system. Do it like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 let winston = require('winston') let Elasticsearch = require('winston-elasticsearch') let esOption = { level: 'error' } let logger = winston.createLogger({ transports: [ new Elasticsearch(esOption) ] }) // ... function onError(err, req, res) { logger.error(`ERROR: ${err.message} / CODE: ${err.code}`) res.writeHead(500, { 'Content-Type': 'text/plain' }) res.end('Something went wrong. And we are reporting a custom error message.') } Bit advanced logic for better performance There is some case to avoid using proxy for better performance. As in image, there can be a case which needs to get data from multiple domains, and response from ‘API-1’ and ‘API-2’ is useless for rendering in client. It means there are ‘waste of http request/response’ on client. This is usually very small, but it can be very big.\nMoreover, if your service are hosted by cloud computing service provider, network performance will be stable(and fast!) between microservices if they are in same cluster. But between BFF and client, network speed will be highly effected by where user are, and if client is in place with poor network signal, this ‘waste of http request/response’ will effect more to the service performance.\nThis is bit advanced flow to reduce request/response size and(or) number between client and BFF. But using proxy, we cannot change the flow by status.\nSo for example if we want to find shipping status of the order, and need to call API from ‘order’ and ‘shipping’ to domain(assume client will call /api/v1/bff/shipping-of-order for this…), we can go on as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 const axios = require('axios') app.use( '/api/v1/bff/shipping-of-order', bffAPIs.getShippingOfOrder ) // ... const bffAPIs = { getShippingOfOrder: async (req, res, next) =\u003e { try { const token = apiToGetBearerToken() const orderInfo = await axios({ baseUrl: 'https://order-service-host/', data: { // put data to get order... }, headers: { `Authorization': Bearer ${token}`, } }) if (orderInfo.shippingId) { const shippingInfo = await axios({ baseUrl: 'https://shipping-service-host/', data: { // put data to get shipping }, headers: { `Authorization': Bearer ${token}`, } }) res.status(200).send({ data. shippingInfo.data }) } else { res.status(404).send({ message: 'No shipping ID!' }) } } catch (e) { // send exception... res.status(e.response.status).send({ message: 'Exception!!' }) } } } As you can see, BFF side code has been bigger, and logic has more complexity because we need to check data, and return value by status in person. But if there are cases to check status from multiple domain and client needs only few data from final result, this could make user more happy.\nReference https://microservices.io/patterns/microservices.html https://philcalcado.com/2015/09/18/the_back_end_for_front_end_pattern_bff.html https://www.nginx.com/blog/building-microservices-using-an-api-gateway/ https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends https://tsh.io/blog/design-patterns-in-microservices-api-gateway-bff-and-more/ https://fullstackdeveloper.tips/seeing-the-bff-pattern-used-in-the-wild ","description":"What is BFF pattern, and how it work for service","tags":["web","microservice","backend","frontend","node","express"],"title":"BFF(Back-end for Front-end) inside, for microservice","uri":"/posts/2019-10-17-about-how-bff-works/"},{"categories":null,"content":"I’ve received a request for a simple demo few weeks ago, which needs to catch-up real time data, and show the result on simple webpage. I’ve thought it would be simple at first meeting. But because I was first on using micro-service and traffic size was way more bigger than I thought, I faced with lot of issue on work.\nThis is the summary of the development.\nRequest This is the basic flow to do.\nConsume message from specific Kafka topic Parse message to defined format, and save on DB Web page repeatedly requests data to API server API server responds the data in DB to web page Health-check, and Actuator This app will be setup as part of big micro-services . As you know, micro service is a cluster consist of many services separated by domain(or else). And because services are running separately, cluster needs to monitor each service instances are running well without exception.\nFor this each service usually make an endpoint(usually /health) which will return the status of the service. Returning value does not need to be complicated, just let cluster know that I’m okay. To make it more simple, Spring has a module for this named Actuator.\nYou don’t need to do something on your code. Just add a line on gradle build file: [build.gradle.kts]\n... dependencies { implementation(\"org.springframework.boot:spring-boot-starter-actuator\") ... } Run the server, and call /actuator\n$ curl localhost:8080/actuator -i HTTP/1.1 200 Content-Type: application/vnd.spring-boot.actuator.v3+json ... {\"_links\":{\"self\":{\"href\":\"http://localhost:8080/actuator\",\"templated\":false},\"health\":{\"href\":\"http://localhost:8080/actuator/health\",\"templated\":false},\"health-path\":{\"href\":\"http://localhost:8080/actuator/health/{*path}\",\"templated\":true},\"info\":{\"href\":\"http://localhost:8080/actuator/info\",\"templated\":false}}} This automatically creates the endpoint which offers service status. If you access to /actuator/health, it will show:\n$ curl localhost:8080/actuator/health -i HTTP/1.1 200 Content-Type: application/vnd.spring-boot.actuator.v3+json ... {\"status\":\"UP\"} Of course you can customize the response informations. See official actuator document for more.\nKafka consumer setup Different with tutorial, it needs to handle 10~100Mb/sec payloads from producer server. To handle it more efficiently, some of settings need to be changed.\nfetch.max.bytes / max.fetch.partition.bytes It is to define the maximum amount of data that it wants to receive from the broker when fetching records. fetch.max.bytes means the maximum size which consumer can receive, and max.fetch.partition.bytes is the maximum fetch size from each partition.\nDefault value are both 1048576, but because avg. size of fetched data was way more bigger than this so needed to update it. Because the topic I’ve consuming was with 10 parition, I’ve setup fetch.max.bytes as 100Mb, and max.fetch.partition.bytes to 10Mb.\nfetch.min.bytes Opposite with above, it define the minimum data size that it wants to receive from the broker on fetching. Keeping default value(1bytes) will makes consumer response to producer too oftenly, so update to 1Mb.\nfetch.max.wait.ms The maximum time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy the requirement given by fetch.min.bytes. It means that consumer will wait during setup time passes until message to fetch becomes bigger than fetch.min.bytes. Default is 500ms, but this setting could cause delay while message piles up in a flash, so reduced to 100ms.\nreceive.buffer.bytes The size of the TCP receive buffer to use when reading data. Default is 65535, but seems needed to update with bigger value for receiving massive data.\nBecause I’m not Kafka engineer, I was undergoing trial and error, and still not sure this is the best answer. But thankfully, it worked well than before(with default setting).\nChecking null inside object parsed by Gson Think of making logic to check ‘key’ in json object, and return false if value is invalid. If you are using Gson for parsing json format data, this logic for null-check could not work.\n1 2 3 4 5 6 7 // payload is string =\u003e \"{\\\"key\\\": null}\" val jp = JsonParser() val jse = jp.parse(payload) // this fails... val key = jse.asJsonObject.get(\"key\") ?: return false ... Parsing logic in Gson, seems making null value into JsonNull object defined by Gson, instead of keeping it as own format. So you need to replace last code to:\n... if (jse.asJsonObject.get(\"key\").isJsonNull()) { return false } ... Currently, javadoc in code is commented as returning null:\n1 2 3 4 5 6 7 8 9 /** * Returns the member with the specified name. * * @param memberName name of the member that is being requested. * @return the member matching the name. Null if no such member exists. */ public JsonElement get(String memberName) { return members.get(memberName); } but it was not working in my case. So, if you want to make it more safe, do it as:\n1 2 3 4 5 ... if (jse.asJsonObject.get(\"key\") == null || jse.asJsonObject.get(\"key\").isJsonNull()) { return false } ... double-checking.\nDB Indexing While developing, there were request to show additional data. To get this data, additory DB table was needed. So I’ve created new one, with dumped data(around 500,000 row).\nMessage format was something like(different with real one…):\n1 2 3 4 5 6 { \"score\": \"90\", \"name\": \"user-2\", \"grade\": \"junior\", ... } and new table row was like:\nid name email 1 user-1 email-1 2 user-2 email-2 3 user-3 email-3 … … … and need to find name and get email of every message data.\nSo process was:\nReceive message from Kafka Get name from message Find email from new table by name Merge data and save to record After process has been updated, performance has been much slower, and caused big lag on consumer. I’ve tried to fix by tuning Kafka consumer configuration on first time, but problem has been fixed very simple, by indexing name column.\nIndexing is a way to optimize the performance of a database by minimizing the number of disk accesses required when a query is processed. This requires additional space for index column, but it was not a big matter comparing with this issue. So I’ve create new flyway migrate file for update:\n[v0_2__add_index_name.sql]\nALTER TABLE user_info ADD INDEX `idx_name` (`name`); Consumer lag has banished, and more, CPU usage of DB has decreased.\nReferences https://spring.io/projects/spring-boot https://microservices.io/patterns/observability/health-check-api.html https://kafka.apache.org/documentation/ https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ https://www.geeksforgeeks.org/ ","description":"","tags":["spring","kotlin","api","microservice","kafka"],"title":"How I handle issues from Kafka consumer + Spring boot","uri":"/posts/2019-08-02-handle-feature-kafka-spring/"},{"categories":null,"content":"It has been ‘bout a month using Vue.js on production level. I’ve used AnguarJS 1.x in first web development and work with React last 1 year. And first impression of Vue.js is, it is very easy to start on. Not because it has similarities with frameworks I’ve used, it has great document for beginners, and it is not fragmentized like React. For example, state management is almost unified with Vuex. I’m not sure this is good or bad, but it make to develop clear for the starters like me.\nMaking component Component are re-usable Vue instance, something like ‘function’ or ‘module’ in programming. It can be customized button, input, table, or mixed stuff such as ‘button with input’, ’table with title’, etc.\nActually you can put all of these in single page without grouping several parts as a component. But good thing of separating these, are making this re-usable. Like making useful utilities inside code project, making re-usable component helps reducing code and easy to maintain.\nSo, here is the component planning to work on. This is not the exact result, but it is enough to refer. I’ll try to wrap up features inside this as single component. In this image, you can see feature as:\ninput part label on top icon on right Let’s go on.\nAnd one more, I’ll not talk about styles(css things…), and will only focus on logic here.\nBasic form Let’s name this component as custom-icon-input.\nWhat properties will be needed for this component? It will require item as icon image, label text, input placeholder for UI. And more, it needs to receive icon touch events, and model to connect. So it will be like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u003ctemplate\u003e \u003cdiv\u003e \u003c!-- label --\u003e \u003clabel\u003e{{ label }}\u003c/label\u003e \u003c!-- icon --\u003e \u003cdiv v-on:click=\"iconClick\" \u003e \u003cimg src=\"iconSrc\" /\u003e \u003c/div\u003e \u003c!-- input --\u003e \u003cinput :name=\"inputName\" :placeholder=\"inputPlaceholder\" v-model=\"inputModel\" /\u003e \u003c/div\u003e \u003c/template\u003e and class part:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import { Component, Prop } from 'vue-property-decorator' @Component export default class CustomIconInput extends Vue { @Prop() label: string @Prop() iconSrc: string @Prop() iconClick: Function @Prop() inputPlaceholder: string @Prop() inputModel: string // ... } This component will be used outside as:\n1 2 3 4 5 6 7 8 \u003ccustom-icon-input icon-type=\"(app-icon-type)\" :icon-click=\"(app-icon-click-event)\" input-label=\"(app-input-label)\" input-name=\"(app-input-name)\" input-placeholder=\"(app-input-placeholder)\" :input-model=\"(app-input-model)\" /\u003e There are still several things to do. But first let’s check how it will work.\n‘[Vue warn]: Avoid mutating a prop directly since the value will be overwritten…’ If we implement like above, this will be the first hurdle.\nThere are lot of documents treating about this, so you can find more from Google or StackOverflow.\nAnyway, main reason of this is, because you are trying to change parent object (app-input-model) in child component(custom-icon-input). In this component \u003cinput\u003e is feature of child component. It will change inputModel when inputting text, and this inputModel is equal to (app-input-model).\nSeems it was allowed before, but now considered as anti-pattern in new rendering mechanism, whenever the parent component re-renders, the child component’s local changes will be overwritten. It seems to avoid two-way bindings(like other modern web frameworks like React, Angular).\nHow to avoid it? To send changed property to parent component, we need to send ’event’ using $emit.\nThis is fixed component template:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u003ctemplate\u003e \u003cdiv\u003e \u003c!-- label --\u003e \u003clabel\u003e{{ label }}\u003c/label\u003e \u003c!-- icon --\u003e \u003cdiv v-on:click=\"iconClick\" \u003e \u003cimg src=\"iconSrc\" /\u003e \u003c/div\u003e \u003c!-- input --\u003e \u003cinput :name=\"inputName\" :placeholder=\"inputPlaceholder\" v-bind:value=\"inputModel\" v-oninput=\"$emit('update:input-model', $event.target.value)\" /\u003e \u003c/div\u003e \u003c/template\u003e Instead of using v-model, it uses v-bind and v-on. Because:\n1 \u003cinput v-model=\"inputModel\" /\u003e works same as\n1 2 3 \u003cinput v-bind:value=\"inputModel\" v-on:input=\"inputModel=$event.target.value\" /\u003e But in this case, it directly changes inputModel in child, and it causes this warning. So instead of changing property directly, we can use $emit('update:xxx', value) to send ‘update event’ to parent, to make variable change on parent side.\n1 2 3 \u003cinput v-bind:value=\"inputModel\" v-on:input=\"$emit('update:input-model', $event.target.value)\" /\u003e This means to send ‘update input-model’ event to parent, with value from ‘$event.target.value’.\nNow you could check warning has been disappeared.\n.sync This is the part to use component from parent.\n1 2 3 4 5 6 7 8 \u003ccustom-icon-input icon-type=\"(app-icon-type)\" :icon-click=\"(app-icon-click-event)\" input-label=\"(app-input-label)\" input-name=\"(app-input-name)\" input-placeholder=\"(app-input-placeholder)\" :input-model.sync=\"(app-input-model)\" /\u003e You can see .sync in back of model. This is new on Vue.js 2.3.0+.\nWhen child component sends event via $emit, parent will receive and update its local data property:\n1 $emit('update:input-model', $event.target.value) to update this event logically you need to add update event on component.\n1 2 3 4 \u003ccustom-icon-input v-bind:input-model=\"(app-input-model)\" v-on:update:input-model=\"(app-input-model) = $event\" /\u003e But by using .sync, it can be done more simple:\n1 2 3 \u003ccustom-icon-input v-bind:input-model.sync=\"(app-input-model)\" /\u003e Accept click events We also need click events for icon. But as in same reason, we need to send ‘clicked’ event to parent, to make this handled by parent side. One more time, child cannot effect parent directly.\n1 2 3 4 5 6 7 8 9 10 11 \u003ctemplate\u003e \u003cdiv\u003e \u003c!-- label --\u003e \u003clabel\u003e{{ label }}\u003c/label\u003e \u003c!-- icon --\u003e \u003cdiv v-on:click=\"$emit('icon-click')\" \u003e \u003cimg src=\"iconSrc\" /\u003e \u003c/div\u003e \u003c!-- input --\u003e \u003c!-- ... --\u003e \u003c/template\u003e By changing as above, clicking image div will trigger function connected with icon-click in parent side. In this template it will call (app-icon-click-event).\n1 2 3 4 5 6 7 8 \u003ccustom-icon-input icon-type=\"(app-icon-type)\" v-on:icon-click=\"(app-icon-click-event)\" input-label=\"(app-input-label)\" input-name=\"(app-input-name)\" input-placeholder=\"(app-input-placeholder)\" :input-model.sync=\"(app-input-model)\" /\u003e Reference https://vuejs.org https://michaelnthiessen.com/avoid-mutating-prop-directly/ ","description":"","tags":["vue","vuejs","frontend","component","typescript"],"title":"Basic data flow on Vue.js components","uri":"/posts/2019-06-21-vuejs-component-basic-data-flow/"},{"categories":null,"content":"This is from the draft version of document I’ve written for my colleagues(python, javascript developers) who is first in golang, by planning to use it in our company products\nGolang? Go, also known as Golang, is a computer programming language whose development began in 2007 at Google, and it was introduced to the public in 2009.\nIt has lots of similarity with C language such as:\nSimplified syntax(there are no keywords such as ‘class’, ’exception’…) You can return data by value or reference … and also has plus:\nGood readability/usability code High cost/performance + easy multi-processing One of great advantage in Golang is cost/performance, and this is why I selected this.\nComparing with Golang, Python’s grammar is bit more simple, and Rustlang is using bit less memory(about 5~10%) on runtime. But you can make way more high performance web service than pure Python, with far more simple code than Rustlang.\nGOPATH ‘GOPATH’ environment variable specifies the location of your golang-based workspace. All of golang libraries you installed will be set inside here. so your project couldn’t find installed golang libs/modules if you ignore this. If you install golang via ‘brew’, it will automatically create and define as ‘$HOME/go’. But I prefer to add your own path.\nAbout code Code Style Go is ‘almost’ forcing to use fixed format. By ‘gofmt’ program, it reads the updated state of go file, and convert to standard format. This program is included in most of golang IDE as a plugin, so it will automatically activated after you save your code.\nIdentifier Blank identifier The blank identifier is represented by the underscore character _. It serves as an anonymous placeholder instead of a regular (non-blank) identifier and has special meaning in declarations, as an operand, and in assignments.\nGolang formatter does not allow unused variable or package. So if you want to keep your unused stuff temporary, put underscore as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 package xxx import ( _ \"net/http\" } func main() { _ := addValue(1, 2) } func addValue(a int, b int) int { return a + b } Exported identifiers An identifier may be exported to permit access to it from another package. An identifier is exported if both:\nFirst character of the identifier’s name should be capital letter It is declared in the package block or it is a field name or method name. All other identifiers are not exported.\nError handling Go does not offer keyword for ‘Exception’, so you need to add error checking code often. To allow handling error outside of function, they return error values with proper data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func doSomething() (string, error) { if true { return \"success\", nil } else { return \"\", errors.New(\"cannot do something\") } } ... func main() { value, err := doSomething() if err != nil { fmt.Printf(\"Failed: %v\\n\", err) return } } Also, if you want to stop your code right away, you can shorten\n1 2 3 4 if err != nil { fmt.Printf(\"Failed: %v\\n\", err) return } as\n1 2 3 if err != nil { panic(err) } Class \u0026 Struct There are no class syntax in golang. But you can use class-like definition by using custom type and method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type User struct { FirstName string LastName string Id int64 } func (user User) FullName() string { return user.LastName + \", \" + user.FirstName } func main() { user := User { FirstName: \"Lite\", LastName: \"DeLTA\", Id: 1, } fmt.Println(user.FullName()) // =\u003e DeLTA, Lite } This is equivalent in python as:\n1 2 3 4 5 6 7 8 9 10 11 12 class User: def __init__(self, first_name, last_name, id): self.first_name = first_name self.last_name = last_name self.id = id def full_name(self): return self.last_name + ', ' + self.first_name user = User('Lite', 'DeLTA', 1) print(user.full_name()) Marshal/Unmarshal Golang’s default ‘encoding/json’ package includes marshalling/unmarshalling, which is for object-\u003ejson/json-\u003eobject.\nBy using ‘net/http’ package, you can get raw request body including byte data. So when you need to parse response body data to JSON value, unmarshal it to access in JSON-way.\nMake sure to make struct variables in Capital letter if you want to export it. You can put json:”original parameter name” in back of struct variable like below, to define which data will be migrated to the struct object. So if you fetched byte data like:\n{“name”: “JR-East”, “location”: “tokyo”} unmarshal it as:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 type Company struct { Name string `json:\"name\"` Location string `json:\"location\"` } ... res, err := http.Get(\"http://xxx.xxxx\") if err != nil { http.Error(w, err.Error(), 500) return } b, err := ioutil.ReadAll(res.Body) defer res.Body.Close() if err != nil { http.Error(w, err.Error(), 500) return } var company Company err = json.Unmarshal(b, \u0026company) if err != nil { http.Error(w, err.Error(), 500) return } fmt.Println(\"Company name is \", company.Name) // Company name is JR-East Package manager If you want to use external modules for go, you can simply do:\n$ go get 'package-repo' This command does:\nDownload and save module in $GOPATH/src/package-repo Execute ‘go install’ But because there are no list file(like package.json or requirements.txt) which defines dependency of your project, it is not good for coworking with your teammates. For example, when you are starting to working on some node project, you can clone the code and just run ‘npm install’ to install the dependencies, instead of installing it all one by one.\nFor this, most of people were recommending dep or Glide, but it seems preparing to be in state of support-only, so I prefer to go on with dephttps://golang.github.io/dep/.\nOthers Web framework Like other programming languages, go also has open-source web frameworks, but it is not being recommended like Django in Python, or Rails in Ruby.\nOne of main reason is, Go language itself is focused on developing modern web. It has built-in modules like net/http, and this offers great features for easy development. For example, default http.Serve function spawns goroutine per each request so user don’t need to worry about multithreading. You can make great service enough without depending on full frameworks.\nInstead of web framework, there are lots of middlewares for handling session, routing, and more. It will help enough for easy web development.\nhttp://www.gorillatoolkit.org/ https://github.com/urfave/negroni Using database If you look on DB connection examples, most of them will have code like:\n1 2 3 4 5 6 7 db, err := sql.Open(\"mysql\",\"user:password@tcp(127.0.0.1:3306)/db_name\") if err != nil { log.Fatal(err) } defer db.Close() // ... It seems defining opening/closing database connection, but there are several things need to remind.\nsql.Open(...) is not the code establishing connection to database. This is just ‘preparing’ process for later use. Actual connection occurs when it is needed for the first time.\nsql.DB object is designed for long-living. So create one sql.DB object for each distinct datastore and pass it around as needed, or make it available somehow globally. Keep it open. Don’t Open() and Close() from a short-lived function.\nIt means…you don’t need to define db.Close() for common web project.\nReference https://golang.org https://github.com/golang/go/wiki ","description":"Some golang guide for python/js developer","tags":["python","golang","develop"],"title":"About golang, for python/js developer","uri":"/posts/2019-03-22-py-js-to-golang/"},{"categories":null,"content":"Few days ago, I read a blog post ‘Things I Don’t Know as of 2018’ written by Dan, and it has been a great motivation to tidy up what I have done, and what I know.\nI subscribe several facebook pages and blog post about software engineering, and getting some information of new tech and trends every day. But that is way different from becoming good at that. For example, I can tell somebody about the usage of Tensorflow after I read some posts, but that does not mean I can work as deep learning engineer.\nI understand of this, but still being confused about the ’true knowledge’. Now it has been 2019, and seems good time to clean up about my knowledge.\nOf course, we know we can do (almost)everything by Google(or GitHub, StackOverflow…) power. We can get knowledge by searching with suitable keywords. So the word don't in this post, means I can do it, but needs help from internet.\nSome of articles are brought from Dan’s.\nBasic, and theories Unix command or shell script: I fill comfortable of using basic command such as ls, cd, mv, ps to edit/find file in terminal, or check thread status. I also can edit simple code/document using vi. But I cannot do more complicate things without google’s help.\nNetwork: I understand TCP/UDP protocol, IP address and 7 layer. Also know about HTTP/HTTP2 too. But that’s all. I don’t know the detail about low-level network part.\nDatabase: I know how to use simple SQL query, and work on table design for few times. I also understand about transaction. But don’t know about database tuning, and more other things.\nPattern \u0026 Paradigm: I know some design patterns such as singleton, observer, factory, proxy, etc. and simply can implement in this way. But I cannot describe the detail of usage, to optimize the system. I’m also looking on reactive programming, but didn’t used on production level. About functional programming, I just can explain what it is with pros and cons…that’s all.\nSecurity: I know basic flow of SSL layer, and how it effects in HTTPS. Also faced on issue about CORS, but don’t know the detail. I have made API service using JWT token and OAuth2 demo server, and review about the vulnerability.\nWeb/Mobile application Front-end(Client): I had developed client-side project with AngularJS(not Angular), and React. I tried using Redux pattern and styled component, and know why this is useful. I once converted JavaScript project to TypeScript, and felt the strength of typed language. I have used gulp and webpack for build process, and understand the usefulness of these.\nAndroid: I have developed android application for MDM, and NDK library for video streaming. I didn’t remember all things, but designed connection between native library and android app. I know about lifecycle of Activity and Fragment, and making AsyncTask such as network connection. I also know about the basic of image handling, and know useful modules such as Picasso and Glide. But not good on such thing as Dependency Injection, or useful network library like OkHttp3.\niOS: I have some experienced on working with Objective-C, but forgot most of them. I don’t tried Swift yet.\nBack-end, server API server: I have developed API server, and understand how they interact with client side and making connection with database. I understand credential for authentication, and REST(though I never made my APIs RESTful…)\nMessage queue, and task system: I know why message queue system is needed, and support making distribution task system based on celery(Python). But don’t know good about load balancing, or more.\nData pipeline: I know what ETL(extract+transform+load) is needed for, and designed a demo of data pipeline based on Kafka+Redis, but don’t know more than that.\nServer/Cloud: I have worked on on-premises environment, so understand the role of infrastructure team. I know the needs of duplexing, and how-to debug based on apache/nginx log. About cloud, I have tried App engine/Bigquery/Database in GCP, and run simple project in AWS. But not know deeply about optimization.\nOthers Container: I know docker, and used for containerizing my project in development environment with Dockerfile and docker-compose. But I didn’t tried k8s or swarm, and deploying container to server yet.\nCI/CD: I know code review flow and continouos integration, and have experience of guiding basic flow of to our team before. I used Jenkins on work, and Travis for my open source project.\nXML/JSON/GraphQL: I know what they are, and implemented data parser for XML/JSON format. I tried making request in GraphQL format, but don’t know more.\nData engineering and Machine learning: I know tools for using these such as scikit-learn, tensorflow, and tried several challenges including using kaggle. I understand data preprocessing, and how to do it by using pandas. But cannot go more than that. I don’t know things such as selecting efficient algorithms, or optimizing deep learning network.\nThis is just tip of the iceberg, but you could know a bit about what kind of developer I am…\n…and next Knowing about myself is important. I felt more about this while I’m working as developer, because I always bumped against the wall when I try to explain about what I’m not good about it.\nBut I think we don’t need to fill shame about it, because nobody in the world knows everything you don’t know. I truly guarantee that the greatest engineers around you also have more things they don’t know than something they knows.\nIf you don’t know, you can learn it. Google, GitHub, StackOverflow, and lots of guide in MOOC site can help you. Anyway, developer is forever-learning job, until quiting. Just learn it if there is something you need to. As Dan said in end of his post,\nI’m aware of my knowledge gaps (at least, some of them). I can fill them in later if I become curious or if I need them for a project. This doesn’t devalue my knowledge and experience. There’s plenty of things that I can do well. For example, learning technologies when I need them. There are several target I want to be better in 2019, such as:\nGetting more better in programming languages, especially Rust and Go Be better of using Cloud Understand more about ML/DL, and data engineering and more… Hope these can be list of Do in next year.\n","description":"Things I don’t know in early 2019, as a developer","tags":["developer","sw engineer"],"title":"Things of do and don’t","uri":"/posts/2019-02-28-things-of-do-dont/"},{"categories":null,"content":"This is summary which I presented in Lighting Talk on developer meeting.\nReactive programming is not a new thing. It is one of programming paradigms, oriented around data flows and the propagation of change. In reactive system, all of events inside applications are going through this stream, including request, data change, input event, and more.\nvs Imperative We can think imperative programming as the opposite word of reactive programming.\nImperative programming should the most popular way to code until now. In this, code will be executed in order of code lines and flow/values are being changed by state of program.\nCheck the difference of basic flow. This describes simple process, to add value 2 to each data inside list:\nIn imperative way, process will go on order and input data will be blocked from other process until it finished. It is synchronous flow, and we can expect the order of data.\nBut in reactive way, data in list is going into the stream. During the flow, it will be processed and subscribed by system during the flow.\nSee some other thing. This is just a pseudo code, but good to understand the difference:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 1. imperative programming b = 10 c = 20 a = b + c console.log(a) // 30 c = 40 console.log(a) // 30 // 2. reactive programming b = 10 c = 20 a = b + c console.log(a) // 30 c = 40 console.log(a) // 50 It is easy to expect the result of example in case 1. No doubt that result of ‘a’ will not be change because sum of 2 value has been done before ‘c’ is changed. But in case 2, all data is in the stream, and can be effected during process. Now you can expect, it would make debugging more difficult.\nWhat is good for? Though it can make system more complicate, it has several pros which will make you to think about it.\nMost of server systems in current days are facing with heavy traffics, as well as web/mobile apps treating variable UI events and interactive actions. By this change, all kinds of programming need to think about asynchronous handling. Reactive programming is making system events running in streams, and subscribe these asynchronously.\nFor several years, Netflix has changed their Java-based system into Reactive model. They made ‘observable’ service layer on the system to make it working asynchronous, safely use concurrency methods to cover heavy traffic.\nRx - Reactive eXtension Actually, there are already lot of ways to make things do this, like using Future in Java, or Promise in JavaScript. But you can make it more solid with more simple way, by using Rx module.\nRx is shorten of Reactive eXtension, and it offers tool for reactive coding. It includes library for composing asynchronous and event-based programs using observable sequences and LINQ-style query operators, and make user free from considering low-level stuffs, such as threading, sync/async, concurrency.\nMain features are:\nObservable: Create data stream object Observer: Create object which observes data stream created by Observable. Subscribe: Connector method to data stream …and the basic flow are:\nDefine a method that does something with the return value from the asynchronous call. This method is part of the observer. Define the asynchronous data stream as an Observable. Attach the observer to Observable object by subscribing it. Go on with your business; whenever the call returns, the observer’s method will begin to operate on its return value or values — the items emitted by the Observable. This describes the flow of data stream. Data is being attached by method in Observable, and by defining transforming method inside, it will return transformed method.\nSee more detail in below. You could find more in official site.\nRxPy example RxPy is extension for python. There are extension for most of popular languages(RxJava, RxJS, RxSwift…), so you can make it fit in your project.\nSample I created here is the method to make list of string parsed from wikipedia document(search as ‘New York City’), and find the sentence which includes string ’expensive’.\n1 2 3 4 5 6 7 8 9 KEY_STRING = 'expensive' def rx_find_string_has_key(observer): str_list = open('new_york.txt', 'r').read().split('.') for string in str_list: if KEY_STRING in string: observer.on_next(string) observer.on_completed() This method will split the raw text to list of sentences, and will filter the data by whether having string ’expensive’ or not.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class PrintObserver(Observer): ''' An Observable calls this method whenever the Observable emits an item. This method takes as a parameter the item emitted by the Observable. ''' def on_next(self, value): print('Received: {}\\n'.format(value)) ''' An Observable calls this method to indicate that it has failed to generate the expected data or has encountered some other error. It will not make further calls to onNext or onCompleted. The onError method takes as its parameter an indication of what caused the error. ''' def on_completed(self): print('Completed') ''' An Observable calls this method after it has called onNext for the final time , if it has not encountered any errors. ''' def on_error(self, error): print('Error: {}'.format(error)) This is observer part. It will define what to do with received value by event type. Observer has 3 interfaces(on_next, on_completed, on_error) inside. Other extensions are almost same, so you could refer definitions on comment above.\n1 2 source = Observable.create(rx_find_string_has_key) source.subscribe(PrintObserver()) This is the activation part. It will create stream and source setter by using Observable. Data source will be sent by parameter rx_find_string_has_key defined above, and subscribe will be done by PrintObserver(). Check the result.\n... Received: [369] In 2019, the most expensive home sale ever in the United States achieved completion in Manhattan, at a selling price of US$238 million, for a 24,000 square feet (2,200 m2) penthouse apartment overlooking Central Park Received: 95 billion, making it the world's most expensive hotel ever sold Received: Four of the ten most expensive stadiums ever built worldwide (MetLife Stadium, the new Yankee Stadium, Madison Square Garden, and Citi Field) are located in the New York metropolitan area ... If you like more ’lambda’ style, you can do same thing like this:\n1 2 3 4 5 6 source = Observable.create(rx_find_string_has_key) source.subscribe( on_next=lambda value: print('Received {}\\n\\n'.format(value)), on_completed=lambda: print('Completed'), on_error=lambda error: print('Error: {}'.format(error)) ) Reference http://reactivex.io https://gist.github.com/staltz/868e7e9bc2a7b8c1f754 https://medium.com/netflix-techblog/optimizing-the-netflix-api-5c9ac715cf19 https://developers.redhat.com/blog/2017/06/30/5-things-to-know-about-reactive-programming/ https://medium.com/@kevalpatel2106/what-is-reactive-programming-da37c1611382 ","description":"What is reactive programming, and rx(reactive extension)","tags":["reactive","rx","rxpy"],"title":"Reactive programming, and Rx","uri":"/posts/2019-01-29-reactive-programming-rx/"},{"categories":null,"content":"One way to show your output as a developer, is releasing a module which offers specific function to help development. Most of modern programming language has its own package manager(Python-pip, Rust-cargo, Java-maven, …), so it helps to release and introduce your stuff and make it simple to be used by engineers.\nI once write a note about releasing project in bower package manager. But because of several problems, most of people suggest to go over to npm, and even bower project itself suggest to do as that way.\nSo this post will just focus with npm.\nHow I started to do this When I was developing front-end side(React + TypeScript), back-end side was developed in PHP and Python. Our team wanted to keep front-end side code to follow camelCase rule, but parameters sent from back-end side was snake_case. It forced some of codes to follow snake case, so I thought of wrapping the response data, and convert parameter style directly.\nAbove this, I also added type converter logic. It supports to change integer value 0 or 1 to boolean, and date string to Date type. Because our project is based on TypeScript, this became quite useful.\nThis is the project tucson I updated.\nSo, it is useful? This works are just focusing to reduce the code. It is small project, with 2~300 lines of code only, and no high-level mathematical logic. Somebody could think they can be do it by themself. But actually, more than half of the modules can be refused by same reason.\nI really want to suggest that, don’t become so serious for starting up. It will hesitate your work to create something, as I did. It is important to think the usefulness, but it does not need to be so great.\nSo if somebody ask about this ‘Is it useful?’, than I would say ‘Not sure, but it could be to somebody.’\nStart from empty space If you are JavaScript(or TypeScript) user, just start with npm init for now. This command will generate skeleton code for project.\n$ npm init ... ... Press ^C at any time to quit. package name: (your-package) version: (1.0.0) description: entry point: (index.js) test command: git repository: keywords: author: license: (ISC) About to write to /path/to/your-package/package.json: { \"name\": \"module-name\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } If you are TypeScript user, using tsc command will help quick setting.\n$ tsc --init message TS6071: Successfully created a tsconfig.json file. Now you will have tsconfig.json file for typescript compile, and package.json file for node project configuration. Look on tsconfig first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \"compilerOptions\": { \"target\": \"es5\", \"module\": \"commonjs\", \"declaration\": true, \"outDir\": \"./lib\", \"strict\": true, \"moduleResolution\": \"node\", \"suppressImplicitAnyIndexErrors\": true }, \"include\": [\"src\"], \"exclude\": [\"node_modules\", \"**/__tests__/*\"] } This is configuration for compiling typescript files. TypeScript files for project will be included inside ‘/src’ directory, so defined as \"include\": [\"src\"]. compilerOptions/outDir is the location which compiled result(JavaScript file) will be located. It’s your decision, but I setup here to be go on inside ‘/lib’.\nThis compiled file is a build output, not coded result by developer. So it is better to exclude on your git repository.\nMake some function Let’s make very simple thing here. This is function reversing letters of words inside phrase.\n[/src/index.ts]\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 export function reverseWordsInPhrase(phrase: string): string { let wordList = phrase.split(/\\b\\s+/) let reversed = wordList.map((word) =\u003e { return reverseWord(word) }) return reversed.join(' ') } function reverseWord(word: string): string { let newString = '' for (let a = word.length - 1; a \u003e= 0; a--) { newString += word[a] } return newString } By using this, phrase news in NBA trade deadline becomes swen ni ABN edart enildaed. Do not care how meaningless this function is…it’s just for test.\nCommand tsc will generate ‘index.js’ file in ’lib’ directory. This is the actual function to be used for the user.\nAfter file is built, make a sample file for test this.\n1 2 3 let reverse = require('./lib/index.js').reverseWords console.log(reverse('news in NBA trade deadline')) If you could see the function working, you are ready to go on.\nSome good things to do before release - test \u0026 CI I once updated post testing and releasing module for Android, wrote about testing and CI(continous integration). I’ll do the same thing for this. Working for npm package also has the same process.\nLet’s make simple unit test with mocha. We just check the result of function with example, so let’s use that example for test.\nAdd new file in ‘/test’ directory.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 let assert = require('assert'); let reverse = require('../lib/index.js').reverseWords describe('first test', () =\u003e { let testString = '' before(() =\u003e { testString = 'news in NBA trade deadline' }) describe('check reverse', () =\u003e { it('words in test string should be reversed', function() { assert.equal(reverse(testString), 'swen ni ABN edart enildaed') }); }); }); Install mocha, and run test to check the module works well.\n$ mocha first test check reverse ✓ words in test string should be reversed 1 passing (8ms) Now setup CI process, by making .travis.yml file. But before that, let’s register command for test in package.json file:\n1 2 3 4 5 6 ... \"scripts\": { \"test\": \"mocha\", \"build\": \"tsc\", }, ... than add command to travis config file. It’s way more simple than Android:\n1 2 3 4 5 6 7 8 language: node_js node_js: - 'stable' before_script: - npm install script: - npm run build - npm run test Now test will be triggered by commitment(this can be changed by setting), so you could check your commit breaks the module or not.\nPre-setup Now prepare to release your package to npm repository, to make people can use it. You should connect to your npm account, so login with:\n$ npm login and setup with your username and password.\nThan check package.json:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"name\": \"module-name\", \"version\": \"module-version\", \"description\": \"module-description\", \"main\": \"lib/index.js\", \"scripts\": { \"test\": \"mocha\", \"build\": \"tsc\", \"prepublishOnly\": \"yarn run build \u0026\u0026 yarn run test\" }, \"author\": \"your-name-or-email\", \"repository\": \"your-repository\", // ... } scripts/prepublishOnly is the command triggered by npm publish command, which makes your local package published to npm repository. So put on build(and test) process here to make your compiled file updated.\nOne more, npm publish will update all files in your project. But actually, some of files are useless for your module, such as files in ‘/test’ or examples you made. Add .npmignore file and add a list of files which will be not be published.\n…and release This is the log of npm publish from module tucson I introduced here.\n$ npm publish \u003e tucson@0.2.1 prepublishOnly . \u003e yarn run test \u0026\u0026 yarn run build yarn run v1.7.0 $ mocha to camelcase check key ✓ key should be changed as camelcase check makeDate ✓ value with `makeDate` should be converted as Moment 2 passing (22ms) ✨ Done in 0.67s. yarn run v1.7.0 $ tsc ✨ Done in 2.96s. npm notice npm notice 📦 tucson@0.2.1 npm notice === Tarball Contents === npm notice 682B package.json npm notice 709B README.md npm notice 293B tsconfig.json npm notice 5.6kB yarn.lock npm notice 151B lib/config.d.ts npm notice 330B lib/config.js npm notice 52B lib/index.d.ts npm notice 236B lib/index.js npm notice 99B lib/jsonOption.d.ts npm notice 77B lib/jsonOption.js npm notice 59B lib/snakeCase.d.ts npm notice 490B lib/snakeCase.js npm notice 234B lib/tucson.d.ts npm notice 1.6kB lib/tucson.js npm notice 65B lib/types.d.ts npm notice 77B lib/types.js npm notice === Tarball Details === npm notice name: tucson npm notice version: 0.2.0 npm notice package size: 4.4 kB npm notice unpacked size: 10.7 kB npm notice shasum: d91ca07f1f78aa4a5192b8c5a66e8f99219b8924 npm notice integrity: sha512-azW6hamx99C82[...]KySN/WYb+1BDw== npm notice total files: 16 npm notice + tucson@0.2.1 Now all done. You could check on npm website.\nReference https://www.npmjs.com https://hackernoon.com/how-to-publish-your-package-on-npm-7fc1f5aae600 ","description":"How-to create/publish your own node module to npm repository","tags":["npm","package","nodejs","ci"],"title":"Publish your npm module","uri":"/posts/2018-12-21-publish-module-npm/"},{"categories":null,"content":"It has been several years since deeplearning became common in our life. Lots of services are based on it, and you can find guide books/documents easily in bookstore or google. But it is hard to go on deeply by understanding theories and logics, for most of engineer who is not majored this. Though I’m working on deep learning company(as a web engineer), still suffering with its principle and mathematical formulas. This note, is the challenge for going with this from basic.\nTensorflow, and deep learning libraries Though you are not a deep learning engineer, you would heard about tensorflow. Since google released as open source on 2015, it has been most popular deep learning libraries. Somebody could just think it is the power of google brand, but it is really a thing. It has well-designed APIs, good documents translated with various languages, and most important, it is being used in lots of real world service including google photo, google translate, gmail, android, and more.\nNow other major IT companies(Facebook, Microsoft, Amazon…) are releasing their deep learning libraries as open source, and competiting with tensorflow. It’s good news for engineers having lot of options. But I think it is meaningless thinking which option you will select, if you are not used to on logic of how deep learning are working on and how to use it. So I’ll only deal with tensorflow here. As I said, I’m really a beginner in deep learning yet.\nArtificial Neural network If you heard of deep learning, you would also heard about artificial neural network(ANN). As you could expect in word ‘artificial’, it is something imitate neural network in human brain to replicate how human learns from data.\nThis is basic form of ANN. There are input layer, hidden layer(can be more than 1), and output layer. These layers are made of nodes, which are workin as neuron in human. Each node receives multiple data, calculate based on some function, and send result to connected nodes.\nWith input, weight and bias, by logistic regression logic, it will return the rate of possibility that which class input is. Softmax regression is, one general regression logic, which normalize return values to make sum value as 1. Truely, actual logic is way more complicate, but I’ll not discuss here. It will need a textbook to write it all.\nHope on to the example.\nSetup for research First, import tensorflow library, and image data for research. I’ll use fashion-mnist data, 60,000 imageset of clothing/shoes. Each of them are 28x28 grayscale image, classified to 10 classes. Data includes label list of class, as below.\nLabel Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot You can download dataset here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data fashion_mnist = input_data.read_data_sets(\"path/of/fashion-mnist\", one_hot=True) class_names = [ 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot' ] Import tensorflow module, and datasets to your project. By using tensorflow.examples.tutorials.mnist.input_data, you can receive data as tensorflow DataSet object from formatted zip files. This DataSet type offers APIs to make training code more simple. Labels are defined as 0 to 9 as we saw on table above. So let’s set as a name in class_name array.\nNow check received data. It has 2 DataSet as train and test. Each of it includes 2-dimension numpy array, including information of dataset.\n1 2 3 4 fashion_mnist.train.images.shape #(55000, 784) fashion_mnist.train.labels.shape #(55000, 10) fashion_mnist.test.images.shape #(10000, 784) fashion_mnist.test.labels.shape #(10000, 10) Each image data are composed as an array with 784 formula. It is data of each pixel in 28x28(=784) image. Labels are more simple, it is just array with 10 formula, which defines what kind of image(class) it is.\nThan let’s look on few data.\n1 2 3 4 5 6 7 8 9 10 11 import matplotlib.pyplot as plt fig = plt.figure(figsize=(8, 8)) for i in range(4): target_train_num = i + 5 fig.add_subplot(2, 2, i+1) plt.imshow(fashion_mnist.train.images[target_train_num].reshape(28, 28), cmap='Greys') label_num = np.where(fashion_mnist.train.labels[target_train_num] == 1)[0][0] print(class_names[label_num]) plt.show() This picks 5th to 8th data, and show label name and image. Because image is defined as 1d array, I changed to 28x28 2d array.\nOkay, data seems good. Let’s look on the logic bit more.\nEntropy, and Cross-entropy Surprisal analysis is one technique of Information theory, which calculates the degree/amount of ‘surprised’ by getting certain information. In formula, it can be show as\nFor example, let’s say I have 2% possibility of winning the lottery. In this case:\nWinning the lottery =\u003e log(1/0.02) = 1.69897000434 Not winning the lottery =\u003e log(1/0.98) = 0.0087739243 In information theory, it assume ‘degree of uncertain event’ is larger than certain ones. By this theory, it shows the surprisal degree of getting information ‘winning lottery’ is almost 200 times larger than the other case.\nOkay, now go with Entropy(a.k.a ‘Shannon Entropy’). This is mean value of this ‘surprisal’ in one event. It is defined as:\nIn case above, it can described as =\u003e (0.98 x 0.0087739243) + (0.02 x 1.69897000434)\nWe can say Entropy of my lottery game is 0.04256.\nBut because we cannot expect the future, this can be known after when lottery game is ended. So what we need to see is Cross-Entropy. By this name, we could think it is some kind of combining logic between Entropy. Let’s think the actual probability as ‘p’, and estimated probability is ‘q’. In this situation, we need to make ‘q’ close to ‘p’. In information theorical way, Cross-Entropy means ‘average number of bits needed to identify an event drawn from the set’. More simple, it can be said ’the range between expected / actual event’.\nThis formula will return larger value as this ‘range’ become longer. It means we need to think of reducing this value, to make expectation as close as the real one. There are lot of functions for this, but I’ll use Gradient Descent algorithm here, which are usually used for basic tutorial.\nLet’s get back to source.\nMaking layers, and training in TF 1 2 3 4 5 6 7 8 9 ... X = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) k = tf.matmul(X, W) + b y = tf.nn.softmax(k) y_ = tf.placeholder(tf.float32, [None, 10]) ... This is to make input placeholder for network. X is 784 dimension vector to put in image data. W and b means weight and bias. Model’s variable in tensorflow usually defined as Variable.\n1 2 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=k)) cost = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy) First line is to normalize inputs, and generate the result with Cross-Entropy. Thankfully, Tensorflow offer APIs to make it with single line. Also for Gradient Descent, it can be implemented simply with this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 init = tf.global_variables_initializer() sess = tf.Session() sess.run(init) epochs = 10 batch_size = 100 total_batch = int(fashion_mnist.train.num_examples / batch_size) for epoch in range(epochs): total_cost = 0 for i in range(total_batch): batch_xs, batch_ys = fashion_mnist.train.next_batch(batch_size) _, cost_val = sess.run([cost, cross_entropy], feed_dict={X: batch_xs, y_: batch_ys}) total_cost += cost_val print('Epoch:', '{:d}'.format(epoch), 'Cost =', '{:.5f}'.format(total_cost / total_batch)) batch_size means how many training image will be used for single term. We can get data of batch size with train.next_batch API which are offered by Tensorflow DataSet. It is returning random value from data source, so we don’t need additional code for this.\nEpoch means the number of training total data. It has been setup as 10. Because data is always being trained with random order, the cost(mimimized result by Gradient Decsent) will become lower by training term.\nEpoch: 0 Cost = 0.75573 Epoch: 1 Cost = 0.55515 Epoch: 2 Cost = 0.51330 Epoch: 3 Cost = 0.49104 Epoch: 4 Cost = 0.47568 Epoch: 5 Cost = 0.46544 Epoch: 6 Cost = 0.45749 Epoch: 7 Cost = 0.45073 Epoch: 8 Cost = 0.44577 Epoch: 9 Cost = 0.44050 Result, and still more things need to know. Now we can compare trained result with test data.\n1 2 3 4 correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) sess.run(accuracy, feed_dict={X: fashion_mnist.test.images, y_: fashion_mnist.test.labels}) # 0.8362 It returns 0.8362. Seems not good enough, but nevermind. It is not the purpose here. This is just note for basic understanding.\nThough I write over about this until now, still lot of things are uncomprehended.\nWhat is difference between softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits_v2? How does tuning the value of GradientDescentOptimizer effects the result? Is there other optimizer, and what are the pros and cons of them? How nunber of ‘batch’ and ’epoch’ effect to the result? and more… I’ll keep look on this with next post for deeplearning and Tensorflow.\nReferences https://www.tensorflow.org/tutorials/ https://hackernoon.com/artificial-neural-network-a843ff870338 https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow https://github.com/zalandoresearch/fashion-mnist https://ml-cheatsheet.readthedocs.io/ https://en.wikipedia.org/wiki/Information_content https://en.wikipedia.org/wiki/Cross_entropy https://medium.com/@vijendra1125/understanding-entropy-cross-entropy-and-softmax-3b79d9b23c8a ","description":"","tags":["tensorflow","neuralnetwork","deeplearning","python"],"title":"Starting neural network with tensorflow","uri":"/posts/2018-11-25-tensorflow-basic-neural-network/"},{"categories":null,"content":"Nowadays, most of the service requires massive size of database. For example, let’s think you are making e-commerce service. In early days, it only needed to save data for items(item image, description…) and user(ID, email, address…) which are necessary for commerce. But, to attract more user, it started to offer more personalized information, such as ‘recommend items’. To investigate user likes, they need to research more detail user pattern/logs , and database increased explosively.\nElasticsearch, and ELK stack Elasticsearch is search/analytics engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with HTTP REST web interface. In official guide, it saids:\nElasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data in near real time... Most important point is, it is very fast, much more than searching from RDBMS using SQL query. In case of e-commerece service explained above, it can be used to store full catalog, and send auto-complete result to user quickly.\nBecause of these advantages, it was being used as an engine for massive raw text data, such as log data of server, network, and more. As Elastic knew about this usage, they created an end-to-end stack which calls ELK stack(or Elastic stack) that delivers actionable insights in real time.\nELK is acronym of Elasticsearch, Logstash, and Kibana. Logstash is data processing pipeline that ingest data from multiple source and transform it. Kibana is a tool which visualize data inside Elasticsearch with chart/graph.\nBoot Elasticsearch and Kibana You can download each package from official site, or by brew if you are MacOS user.\n$ brew install elasticsearch $ brew install kibana $ brew install logstash If installed well, first run Elasticsearch and Kibana.\n$ $ elasticsearch Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file logs/gc.log due to No such file or directory [2018-10-21T22:32:45,707][INFO ][o.e.n.Node ] [] initializing ... [2018-10-21T22:32:45,800][INFO ][o.e.e.NodeEnvironment ] [XwChsFl] using [1] data paths ... ... [2018-10-21T22:32:56,772][INFO ][o.e.n.Node ] [XwChsFl] started [2018-10-21T22:32:56,784][INFO ][o.e.g.GatewayService ] [XwChsFl] recovered [0] indices into cluster_state [2018-10-21T22:33:26,751][INFO ][o.e.c.r.a.DiskThresholdMonitor] [XwChsFl] low disk watermark [85%] exceeded on [XwChsFluRIGjqONsrmaPuA][XwChsFl][/usr/local/var/elasticsearch/nodes/0] free: 27.5gb[11.5%], replicas will not be assigned to this node As mentioned above, it offers HTTP interface to access data. Default access point is http://localhost:9200, so let’s try some simple access to check it runs well.\n$ curl -X GET localhost:9200 { \"name\" : \"XwChsFl\", \"cluster_name\" : \"elasticsearch_user\", \"cluster_uuid\" : \"_BvcZeP5RkW9W9Gix726Dw\", \"version\" : { \"number\" : \"6.4.2\", \"build_flavor\" : \"oss\", \"build_type\" : \"tar\", \"build_hash\" : \"04711c2\", \"build_date\" : \"2018-09-26T13:34:09.098244Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.4.0\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" }, \"tagline\" : \"You Know, for Search\" } Okay, now open new terminal window and start Kibana.\n$ kibana log [03:27:18.694] [info][status][plugin:kibana@6.4.2] Status changed from uninitialized to green - Ready log [03:27:18.745] [info][status][plugin:elasticsearch@6.4.2] Status changed from uninitialized to yellow - Waiting for Elasticsearch log [03:27:18.943] [info][status][plugin:timelion@6.4.2] Status changed from uninitialized to green - Ready log [03:27:18.953] [info][status][plugin:console@6.4.2] Status changed from uninitialized to green - Ready log [03:27:18.957] [info][status][plugin:metrics@6.4.2] Status changed from uninitialized to green - Ready log [03:27:19.042] [info][listening][server][http] Server running at http://localhost:5601 log [03:27:19.302] [info][status][plugin:elasticsearch@6.4.2] Status changed from yellow to green - Ready ... Now access to http://localhost:5601 and you’ll see main template page of kibana.\nPull logs with Logstash Logstash pulls data from multisource, and transform with unified format. Because this is just basic, I’ll try with single file, with single Nginx log format. As you see below, Nginx log includes requested IP, Date, Type, API, etc..\n93.180.71.3 - - [17/May/2018:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Now I’ll put some log data into logstash. Because I don’t have one, I’ll download dummy log data from elastic/example repository. I changed date a bit because it was too old.\nTo setup logstash to pull the data, it needs to be defined as conf file. This is how I did.\ninput { file { path =\u003e [\"/path/to/log/data/*.txt\"] start_position =\u003e \"beginning\" sincedb_path =\u003e \"/path/to/store/sincedb\" } } filter { grok { match =\u003e { \"message\" =\u003e '%{IPORHOST:remote_ip} - %{DATA:user_name} \\[%{HTTPDATE:time}\\] \"%{WORD:request_action} %{DATA:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:response} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\"' } } date { match =\u003e [ \"time\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale =\u003e en } geoip { source =\u003e \"remote_ip\" target =\u003e \"geoip\" } useragent { source =\u003e \"agent\" target =\u003e \"user_agent\" } } output { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"nginx_elastic_stack_example\" } } First, data in input { } and output { } defines where log comes in/out. So as you can expect array defined in ‘input \u003e file \u003e path’ is the target where logstash will pull the data. You could know what is ‘start_position’ well. It only has 2 option, ‘beginning’ and ’end’. ‘sincedb_path’ defines the path of database which will keep track of the current position of monitored log files. Setup to make logstash keep tracking the status of tracking data.\nfilter is pretty important point, because this part take charge of transforming raw data into easily readable/researchable form.\n‘grok’ is to parse arbitrary text and structure it. Inside match, you need to define text pattern matches with your log. It has lots of defined pattern(and could make your own, but will not go on that further) which fits to various type of log data, such as ‘HTTPDATE’, ‘IPORHOST’, ‘NUMBER’. And, defined data in here, can be used and transformed into new format in other filters. For example, in ‘geoip’, it reads the source ‘remote_ip’ which defines in ‘grok’, and transform into location information.\nFor more information, look on input and filter guide.\nVisualize logs in Kibana Now, send log into elasticsearch\n$ logstash -f your-logstash-conf-file.conf [2018-11-04T17:57:55,384][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=\u003e\"6.4.2\"} [2018-11-04T17:58:01,272][INFO ][logstash.pipeline ] Starting pipeline {:pipeline_id=\u003e\"main\", \"pipeline.workers\"=\u003e4, \"pipeline.batch.size\"=\u003e125, \"pipeline.batch.delay\"=\u003e50} [2018-11-04T17:58:02,130][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=\u003e{:removed=\u003e[], :added=\u003e[http://localhost:9200/]}} [2018-11-04T17:58:02,147][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=\u003ehttp://localhost:9200/, :path=\u003e\"/\"} [2018-11-04T17:58:02,501][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=\u003e\"http://localhost:9200/\"} ... [2018-11-04T17:58:04,251][INFO ][filewatch.observingtail ] START, creating Discoverer, Watch with file and sincedb collections [2018-11-04T17:58:04,789][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=\u003e9600} Now go into Kibana, and you will see something:\nyou could see ’nginx_elastic_stack_example’ which defined in ‘output’ setting in logstash conf file. Now create index pattern:\nNow this index pattern stores the field which recognized from output we got from logstash. Go into menu ‘discover’ in left navigation menu.\nIf you select saved index pattern, it shows the summary data of log. Because we generated location data from IP, we can figure out not only the count of log by date, but also where this has been requested.\nWith collected data, we could generate chart/graph by given templates. Below one is simple Pie chart of log count by date.\nReference https://www.elastic.co/ https://github.com/elastic/examples/ https://hackernoon.com/elastic-stack-a-brief-introduction-794bc7ff7d4f ","description":"","tags":["elasticsearch","kibana","logstash"],"title":"Basic of ELK stack","uri":"/posts/2018-10-29-basic-of-elk-stack/"},{"categories":null,"content":"TypeScript is a superset of JavaScript that compiles to pure JavaScript, like CoffeeScript or Dart. When I first heard of this, thought as ‘Why do we need to learn a new thing, which will create JavaScript file? Is it better to make JavaScript directly?’. But now it has been standard development language of Angular framework, become official programming language in Google, and it is being spread more widely in various projects. Yeah, there should be a reason.\nWhy TypeScript? JavaScript has been started as script language, and usage was limited for handling DOM activities, supporting html in web page. Now, client-side application has been very complicated by rising of client-side rendering, and it handles not only rendering, but also flow of data received from server. More than that, NodeJS allowed JavaScript language to be used in system level. Now, there are lots of large-scale application which created by JavaScript only, and they found it is missing several features necessary to be able to productively write and maintain these system.\nTypeScript has lot of similarity with JavaScript, and make it as type safe. It means it can avoid lots of problem caused by unmatching type between values, and catch estimated errors in developing. It helps making stable large-scale web services, and this make TypeScript as one of the popular trend.\nAlso, specification of JavaScript is being changed, such as ES6, ES7, and it is quite a pain to followup every year. Thanks to TypeScript developers, its transpiler is being update to support newest features of JavaScript, so you don’t need to think about this deeply.\nIt has been a few week since I started to look on TypeScript. I was on a work refactoring legacy service based on React to new UI, and one of joined engineer suggested of this. So before working on, I plan to make simple TypeScript based React application.\nReact-redux sample https://redux.js.org/advanced/exampleredditapi\nThis is one of famouse asyncrounous web application example. This includes lot of important features for developing actual React web apps such as action and reducer from Redux pattern, middleware usage, and how to handle asyncrounous process like http connection.\nThis app has dropdown with value ‘reactjs’ and ‘frontend’ included. These are the subreddits, and it will fetch the headlines from API server when selected. It has 3 possible state which can happen.\nSelect subreddit Request headlines from Reddit API server Receive headlines from Reddit API server This cases has been defined as action.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 export const REQUEST_POSTS = 'REQUEST_POSTS' export const RECEIVE_POSTS = 'RECEIVE_POSTS' export const SELECT_SUBREDDIT = 'SELECT_SUBREDDIT' export function selectSubreddit(subreddit) { return { type: SELECT_SUBREDDIT, subreddit } } function requestPosts(subreddit) { return { type: REQUEST_POSTS, subreddit } } function receivePosts(subreddit, json) { return { type: RECEIVE_POSTS, subreddit, posts: json.data.children.map(child =\u003e child.data), receivedAt: Date.now() } } This actions will be changed by status of application. When subreddit has been selected, status is set as SELECT_SUBREDDIT first, and will prepare to fetch data from server. Just before fetching, status become REQUEST_POSTS and keep it until request is finished. After data has been fetched from server, status changes again to RECEIVE_POSTS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class AsyncApp extends Component { ... handleChange(nextSubreddit) { this.props.dispatch(selectSubreddit(nextSubreddit)) this.props.dispatch(fetchPosts(nextSubreddit)) } ... } function fetchPosts(subreddit) { return dispatch =\u003e { dispatch(requestPosts(subreddit)) // REQUEST_POSTS return fetch(`https://www.reddit.com/r/${subreddit}.json`) .then(response =\u003e response.json()) .then(json =\u003e dispatch(receivePosts(subreddit, json))) // RECEIVE_POSTS } } Reducers, will define how data in components needs to be changed by received data in actions. Data type is depend on action type such as REQUEST_POSTS, RECEIVE_POSTS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function posts(state = {},action) { switch (action.type) { case REQUEST_POSTS: return Object.assign({}, state, { isFetching: true }) case RECEIVE_POSTS: return Object.assign({}, state, { isFetching: false, items: action.posts, lastUpdated: action.receivedAt }) default: return state } } And function mapStateToProps is connecting returning objects from reducer and properties defined in components like below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function mapStateToProps(state) { const { selectedSubreddit, postsBySubreddit } = state const { isFetching, lastUpdated, items: posts} = postsBySubreddit[selectedSubreddit] || { isFetching: true, items: [] } return { selectedSubreddit, posts, isFetching, lastUpdated } } export default connect(mapStateToProps)(AsyncApp) This is the rough description of what is happening in this example application. For more details about react+redux, please look on guide page.\nGo on to TypeScript Now I’ll go on making similar service based on TypeScript. I’ll focus on only differences compare to JS version here. For quick work, I’ll make basic template with create-react-app. It offers option to generate skeleton as TypeScript.\n$ create-react-app my-app --scripts-version=react-scripts-ts Let’s look on component first.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ... interface IProps { selectedSubreddit: string, lastUpdated: number, posts: [], handleChange: () =\u003e void } const mapStateToProps = (appState: AppState) =\u003e ({ selectedSubreddit: appState.subreddit, lastUpdated: appState.posts.lastUpdated, posts: appState.posts.posts }) const mapDispatchToProps = (dispatch: any) =\u003e ({ handleChange: (subreddit: string) =\u003e { dispatch(selectSubreddit(subreddit)) dispatch(fetchPosts(subreddit)) } }) class AsyncApp extends React.Component\u003cIProps\u003e { render() { const { selectedSubreddit, posts, lastUpdated, handleChange } = this.props ... } } export default connect(mapStateToProps, mapDispatchToProps)(AsyncApp) I defeind properties which will be used in this component as interface. In JavaScript, it defined value with PropTypes, but because TypeScript can define value types(‘string’, ’numbers’), you don’t need this here. And when making state, dispatch to properties, it needs to define the type of parameter.\n1 2 3 4 5 6 7 8 9 export type AppState = { subreddit: string, posts: PostState, } export type PostState = { posts: [], lastUpdated: number } ‘AppState’ is a defined type which includes all kinds of state values managed in application. It stores ‘subreddit’ as value of selected subreddit, and ‘posts’ for post data.\nTo define actions and reducers, it also need to define format of returning values as interface, and param types.\n1 2 3 4 5 6 7 8 9 10 11 12 13 export interface IRequestPosts { type: ActionTypeStates.REQUEST_POSTS, subreddit: string } ... export function selectSubreddit(subreddit: string): ISelectSubreddit { return { type: ActionTypeStates.SELECT_SUBREDDIT, subreddit } } ... I updated full sources on https://github.com/kination/my-scratch/tree/master/web/react-redux-ts\nSo, is it good? Good thing is, you can catch lot of expected error while typing, not after running. For example, if I change type value like below:\n1 2 3 4 5 6 7 8 9 export type AppState = { subreddit: string, posts: PostState, } export type PostState = { posts: [], // lastUpdated: number } the code editor(I’m using VS code…it’s good!)…\n…will find value is missing and show error immediately. This means you don’t need to worry about problems related with type when it compiled well. The more application becomes large, more state values will be included in service. By avoiding this kind of issue will reduce lots of latent bugs.\nBut bad thing is, because it requires type definitions strictly, you have to type more than JavaScript for same process. So the thing you are planning to make are just a simple scratch, this could only make you more tired. Thankfully, there are lots of modules to reduce typing for type-safe React application, so you could get more help there.\nReference https://www.typescriptlang.org/ https://github.com/Microsoft/TypeScript-React-Conversion-Guide https://blogs.msdn.microsoft.com/somasegar/2012/10/01/typescript-javascript-development-at-application-scale/ https://basarat.gitbooks.io/typescript/ https://github.com/Lemoncode/react-typescript-samples https://redux.js.org ","description":"","tags":["react","redux","javascript","typescript"],"title":"Reloading React project, JavaScript to TypeScript","uri":"/posts/2018-09-29-react-js-to-ts/"},{"categories":null,"content":"It was one IT news that how I know about, and starting to look on Rust language. The most attractive phrase in that time was guaranteed memory safety, but actually there are more pros than that.\nIn document, you can find like:\nzero-cost abstractions move semantics guaranteed memory safety threads without data races efficient C bindings and more… Still, it is not widely-used major language. It is complecate to learn(for me…comparing with python, javascript), and there are substitutions like golang. Though, by strengths above, it is being used widely as sub-language.\nPython performance This topic is still being discussed in lots of community. Because it is script language, or because of GIL(Global Interpreter Lock), it has limitation in performance. Though it is still one of the most popular language because it has enough performance to do something with it, proved by lots of major companies/services(Google, Instagram…) you’ve heard of.\nActually, the most strength is, you can learn and develop quickly with way more short coding.\n[Java] {% highlight java %} {% raw %} public class HelloWorld { public static void main (String[] args) { System.out.println(“Hello, world!”); } } {% endraw %} {% endhighlight %}\n[Python] {% highlight python %} {% raw %} print(‘Hello, world!’) {% endraw %} {% endhighlight %}\nMaybe the reason of this discussion is because it is being used widely, and anyway, it has performance issue though it is enough to use or not. So some of modules wrap-up compile language and handle process which could take time in there. For example, Numpy seems using C for the part that requires performance.\nAttach with Rust I found good comment about merit of binding rust module:\nRust is a language that can usually detect, during compilation, the worst parallelism and memory management errors (such as accessing data on different threads without synchronization, or using data after they have been deallocated), but gives you a hatch escape in the case you really know what you’re doing.\nUsing Rust also can make performance better, and moreover, it can avoid memory management problem, which offtenly caused in C. And as written in document, Rust library can expose a C ABI(application binary interface) to Python with zero overhead.\nYou can work with only using native Rust API, but there are already good extension modules which will save your pain and code.\nrust-cpython pyo3 Actually, root of both are same because pyo3 are the fork project of rust-cpython. It requires bit less coding than rust-cpython, but it only supports nightly version of rust for now, so need to aware of using it.\nThe motivation for this work was post in RedHat developer blog. My work refered lot of parts from here.\nBasic wrapper I’ll use rust-cpython here, with newest rust stable version 1.28.0.\nI create empty rust project with cargo: {% highlight shell %} {% raw %} $ cargo new pyrust –lib Created library pyrust project {% endraw %} {% endhighlight %}\nFix Cargo.toml file to make sample uses cpython: {% highlight shell %} {% raw %} [lib] name = “pyrust” crate-type = [“cdylib”]\n[dependencies.cpython] version = “0.2” features = [“extension-module”] {% endraw %} {% endhighlight %}\nBefore testing performance, I created basic rust method, to check calling in python works well. This is simple function, which print out project version: {% highlight rust %} {% raw %} #[macro_use] extern crate cpython;\nuse cpython::{Python, PyResult};\nfn print_from_rs(_: Python) -\u003e PyResult { const VERSION: \u0026‘static str = env!(“CARGO_PKG_VERSION”); println!(“PyRust version : {}”, VERSION); Ok(1) } {% endraw %} {% endhighlight %}\nThis method has only single Python instance, imported from cpython. This is zero-size marker struct that is required for most Python operations. This is used to indicate that the operation accesses/modifies the Python interpreter state. It is default parameter for wrapper method, but not being used in this method, so set as underbar. Return value is being given as PyResult. {% highlight rust %} {% raw %} … py_module_initializer!(libpyrust, initlibpyrust, PyInit_libpyrust, |py, m| { try!(m.add(py, “print_from_rs”, py_fn!(py, print_from_rs()))); Ok(()) }); … {% endraw %} {% endhighlight %}\nThis is extension to an extern \"C\" function to make python loading the rust code. First param libpyrust is the module name, which will be used in python. Second and third value is necessary for python initiation, but don’t need to think about it for now. Last |py, m| is a lambda of Fn(Python, \u0026PyModule) -\u003e PyResult\u003c()\u003e and it makes importing the module inside initializer.\nNow you need to build library to use. If you are MacOS user like me, make sure that you need to change *.dylib file to *.so to import. Put in library file in same directory with your python file.\n$ cargo build --release Compiling pyrust v0.1.0... ... Finished release [optimized] target(s) in 2.14s $ mv target/release/libpyrust.dylib \u003cpath/to/pythonfile/libpyrust.so\u003e Import libpyrust and call the method. It will show PyRust version : 0.1.0 if correct. {% highlight python %} {% raw %} import libpyrust\nlibpyrust.print_from_rs() {% endraw %} {% endhighlight %}\nPerformance test #1: Search text I make 2 method for test. One is for searching word in text and return the number of appearance in text, and other one is sum calculation in array. This is Rust code: {% highlight rust %} {% raw %} // search target word from text fn search_text(_: Python, target: \u0026str, text: \u0026str) -\u003e PyResult { let mut total = 0u64; // split word by whitespace let iter = text.split_whitespace();\n// compare word with target, and total += 1 if it is same for word in iter { match word == target { true =\u003e total += 1, false =\u003e {} } } Ok(total) }\n// add all value in list fn sum_list(_: Python, list: Vec) -\u003e PyResult { let mut total = 0u64;\nfor num in list { total += num } Ok(total) } … py_module_initializer!(libpyrust, initlibpyrust, PyInit_libpyrust, |py, m| { … try!(m.add(py, “search_text”, py_fn!(py, search_text(target: \u0026str, val: \u0026str)))); try!(m.add(py, “sum_list”, py_fn!(py, sum_list(list: Vec)))); … }); {% endraw %} {% endhighlight %}\n…and Python part. I tried to make code form similar as possible: {% highlight python %} {% raw %} … def search_word(target, val): word_list = val.split(’ ‘) total = 0 for word in word_list: if word == target: total += 1\nreturn total def sum_list(val): num_list = random_list sum = 0 for num in num_list: sum += num\nreturn sum … {% endraw %} {% endhighlight %}\nIt is good to use python benchmark module for test. It has attached module with pytest, to show performance result by test. Install pytest-benchmark in your project, and create test module like this. Text I used for test is part of text from Wikipedia-New York State Route 22. I copy/paste itself to make file more bigger(size of ’nystreet.txt’ is about 2Mb). This test will return how often word ‘NY’ appeared in text file. {% highlight python %} {% raw %} import librust2py\n… ny_data = ’’ with open(’nystreet.txt’, ‘r’) as myfile: ny_data = myfile.read().replace(’\\n’, ‘’)\ndef test_python(benchmark): benchmark(search_word, ‘NY’, ny_data)\ndef test_rust(benchmark): benchmark(librust2py.search_text, ‘NY’, ny_data) {% endraw %} {% endhighlight %}\nThis is the result. Though I just used very rough code, you could find positive side of using this.\nPerformance test #2: Add number in array Now I’ll use another method sum_list to test simple calculation. In this test, there are array of size 10, including random value 1~99999. {% highlight python %} {% raw %} import librust2py\n… random_list = random.sample(range(1, 99999), 10)\ndef test_python(benchmark): benchmark(sum_list, random_list)\ndef test_rust(benchmark): # \u003c– Benchmark the Rust version benchmark(libpyrust.sum_list, random_list) {% endraw %} {% endhighlight %}\nTo the contrary, pure python is more fast than before. Than how about making array more bigger? I changed array size as 10000. {% highlight python %} {% raw %} … random_list = random.sample(range(1, 99999), 10000) … {% endraw %} {% endhighlight %}\nOkay, it shows expected result. Maybe loading to rust could make little delay, and that makes performance worse in small calculation.\nPerformance test #2-2: Compare calculation with NumPy For the last, I’ll try to compare with NumPy array. It uses C language for performance upgrade, so we can expect that it would be faster than at least of pure python version.\n{% highlight python %} {% raw %} import librust2py import numpy as np … def sum_np_list(val): sum = val.sum(0)\nreturn sum …\ndef test_python(benchmark): benchmark(sum_list, random_list)\ndef test_rust(benchmark): # \u003c– Benchmark the Rust version benchmark(libpyrust.sum_list, random_list)\ndef test_np(benchmark): benchmark(sum_np_list, random_list) {% endraw %} {% endhighlight %}\nOkay, something seems wrong. Using NumPy is showing the worst result. After researching stackoverflow, I found loading array to NumPy array is causing delay. So I created NumPy array outside.\n{% highlight python %} {% raw %} … // put array in numpy array, and calculate sum value def sum_np_list(val): np_random_list = np.array(val) sum = np_random_list.sum(0)\nreturn sum … random_list = random.sample(range(1, 99999), 10000) np_random_list = np.array(random_list) …\ndef test_python(benchmark): benchmark(sum_list, random_list)\ndef test_rust(benchmark): # \u003c– Benchmark the Rust version benchmark(libpyrust.sum_list, random_list)\ndef test_np(benchmark): benchmark(sum_np_list, np_random_list) {% endraw %} {% endhighlight %}\nNow you could find out why NumPy is being used in lots of data research modules. It shows overwhelming performance than competitors.\nPros For now, you could find out wrapping Rust module for Python could be worth in particular case. Actually, there are some modules like NumPy which already has its own wrapped C/C++ module for performance, but if there is not where you need, this can be one alternative. Moreover, if you can make better performanced Rust code, you can make more better result, without worrying memory crash.\nReference https://www.rust-lang.org/ https://developers.redhat.com/blog/2017/11/16/speed-python-using-rust/ https://github.com/dgrunwald/rust-cpython https://hackernoon.com/yes-python-is-slow-and-i-dont-care-13763980b5a1 https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust.html ","description":"","tags":["rust","python","numpy","benchmark"],"title":"Will rust make your python happy?","uri":"/posts/2018-08-15-power-up-python-with-rust/"},{"categories":null,"content":"In big data era, by increasement of data size, lots of companies who treated this needed to concern about speed. But HDFS, which usually being used to store big data, couldn’t show enough performance, by blocking from I/O. Data engineers started to research on this, and lots of projects has been introduced, still now. Apache Spark is one of them, and now become most popular between competitors. It has not been a long time since I’ve looking on data engineering, so though I’ve heard of the name and try simple shell command, it is the first time to go on bit deeper.\nApache Spark Apache Spark has been started about 6 years ago. When project first disclosed, it focused on processing data in parallel across a cluster, but the biggest difference was, it works in-memory to increase performance.\nYes, in that time, it was the most impactive point. But now, that’s not all.\nBy document, it saids: {% highlight shell %} {% raw %} Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming. {% endraw %} {% endhighlight %}\nCurrently, the most strength of Spark, is that it includes everything for data engineering. Not only streaming, it supports SQL query by Spark SQL for data process, machine learning with MLlib, and more. Now they don’t need to study about connecting these feature in their system. Also, it supports API with various languages.\nSimple start-on with word count example You can install spark by downloading binary, or if you are MacOS user, just can use brew. When downloading binary, you need to select Hadoop version you are using.\nSetup path in .bash_profile(for MacOS) after download {% highlight shell %} {% raw %} … export SPARK_HOME=/path/to/spark/ export PATH=$PATH:$SPARK_HOME/bin … {% endraw %} {% endhighlight %}\nIf you succeed, you can run spark shell command with spark-shell:\n{% highlight shell %} {% raw %} $ spark-shell 2018-08-11 13:34:55 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform… using builtin-java classes where applicable Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://192.168.3.3:4040 Spark context available as ‘sc’ (master = local[*], app id = local-1533962107204). Spark session available as ‘spark’. Welcome to ____ __ / / ___ / / \\ / _ / _ `/ __/ ‘/ // .__/_,// //_\\ version 2.3.0 //\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102) Type in expressions to have them evaluated. Type :help for more information.\nscala\u003e {% endraw %} {% endhighlight %}\nThis shows that install has been done. Now let’s check this works well.\nThis is the word count example, the most basic tutorial for Spark, and all other data processing tools. If you are trying to run code in spark-shell, it will load sc automatically to make use directly without defining. This is SparkContext, kind of like driver to work with functions in spark cluster.\nPrepare dummy text data into hdfs, to use it in spark shell. Default directory is /user/\u003cusername\u003e.\n{% highlight shell %} {% raw %} scala\u003e val lines = sc.textFile(“spark-guide.txt”) lines: org.apache.spark.rdd.RDD[String] = spark-guide.txt MapPartitionsRDD[1] at textFile at :24\nscala\u003e val words = lines.flatMap(_.split(\" “)) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at :25 {% endraw %} {% endhighlight %}\nWe can figure out lines is some kind of instance which includes text file, and words contains the set of words inside text. Before going on further, we need to know in detail, what happened here.\nRDD, DAG Resilient Distributed Dataset, a.k.a. RDD, is a fault-tolerant collection of elements that can be operated on in parallel, used inside Spark cluster. It can be generated by parallelizing an existing collection in Spark, or referencing dataset from external storage such as HDFS, or else.\nIn example above, lines are text file RDD using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. The textFile method also takes an optional second argument, which defines the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS).\nRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. As you can understand now, words are a transformed new RDD which has set of words from text.\nBut in current status, it has not been ‘actually’ done.\nAll transformations in Spark are lazy. It means actual work is not being done though code has been processed. In cluster, it is just remembering the flow of RDDs, and the graph of flow is called as DAG(Directed Acyclic Graph).\nThe transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently.\nProcess in word count Now let’s go back to the word count again.\n{% highlight shell %} {% raw %} scala\u003e val pairs = words.map(word =\u003e (word, 1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at :25\nscala\u003e val wordCounts = pairs.reduceByKey(+) wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at :25 {% endraw %} {% endhighlight %}\nThese lines also creates RDDs and put it on to DAG. pairs are mapping the set of words, to make each of them as ‘(word, 1)’, and wordCounts reduce these pairs by word, and add on number. It is exactly same process with MapReduce.\n{% highlight scala %} {% raw %} scala\u003e wordCounts.foreach(r =\u003e println(r)) (“added”,1) (Spark,6) (languages.,1) (it,2) (shared,3) … (supported,1) (shell,2) (variable,2) (Users,1) (program,,1) (supports,1) (function,3) (one.,1) (program,1)\nscala\u003e {% endraw %} {% endhighlight %} Now, the print action is required, so DAG will be executed to show the mapped data.\nFor now, this is just a beginning of Apache Spark. I’ll try to keep on working other samples, ‘how-to-use’s, and configuration of system.\nReference https://spark.apache.org/ https://www.tutorialspoint.com/apache_spark https://data-flair.training/blogs/ https://www.youtube.com/watch?v=x8xXXqvhZq8\u0026t=1077s https://www.cognixia.com/spark-less-stages-optimization ","description":"","tags":["apache","hadoop","spark","data engineering","open source"],"title":"A day with starting Apache Spark","uri":"/posts/2018-07-23-day-with-starting-spark/"},{"categories":null,"content":"asyncio is a high-level API for supporting to implement asyncrounous code, which has been added to python default module from version 3.4. There were some workaround for asyncrounous before, but asyncio is supporting process to run asyncrounous in language level. It will reduce lots of code comparing with old-style way.\nWhy asyncrounous?? Well, developer knows about what asyncronous is…so what is it good for? Though I have started python programming since 3.5, I didn’t need to think about this module for about a year. Service I worked had request that can be handle enough with common syncrounous logic, and some of heavy task is processed with distribution queue like celery.\nThe reason I have been interested is while making some scraper for test. There are 2 code, which does same thing, implemented in old way, and with asyncio and aiohttp, the client/server module based on asyncio.\n[sync.py] {% highlight python %} {% raw %} import requests import time\nURL_LIST = [ ‘https://www.python.org/’, ‘https://www.javascript.com/’, ‘https://www.amazon.com/’, ‘https://www.netflix.com/’, ‘https://www.scala-lang.org/’ ]\ns = time.time() for url in URL_LIST: res = requests.get(url) print(f’requested : {url} / {res.status_code}’)\nprint(time.time() - s) {% endraw %} {% endhighlight %}\nThis is simple syncrounous code to fetch data from urls in the list. Most of delays(in my experience, more than 90%…) in scraping process causes on request/response. You really don’t need to worry about speed of parsing, or else. So let’s only focus about requesting, and receiving result.\n{% highlight shell %} {% raw %} requested : https://www.python.org/ / 200 requested : https://www.javascript.com/ / 200 requested : https://www.amazon.com/ / 200 requested : https://www.netflix.com/ / 200 requested : https://www.scala-lang.org/ / 200 6.307806968688965 {% endraw %} {% endhighlight %}\nIt takes about 6.3 sec to fetch urls.\nNow, this is the scraper using async/aiohttp. Talking about code later, and go on first. [async.py] {% highlight python %} {% raw %} import asyncio import aiohttp import requests import time\nURL_LIST = [ ‘https://www.python.org/’, ‘https://www.javascript.com/’, ‘https://www.amazon.com/’, ‘https://www.netflix.com/’, ‘https://www.scala-lang.org/’ ]\nasync def fetch(url): async with aiohttp.ClientSession() as session: async with session.get(url) as res: print(url + ’ / ’ + str(res.status))\ns = time.time() print(‘start: ’ + str(s))\ntasks = [fetch(url) for url in URL_LIST] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks))\nprint(’end: ’ + str(time.time() - s)) {% endraw %} {% endhighlight %}\n{% highlight shell %} {% raw %} start: 1532329882.60325 https://www.python.org/ / 200 https://www.javascript.com/ / 200 https://www.amazon.com/ / 200 https://www.scala-lang.org/ / 200 https://www.netflix.com/ / 200 end: 2.1449880599975586 {% endraw %} {% endhighlight %}\nThis logic took 2.1 sec for response. What makes the difference?\nSync / Async sync.py is the most generic way to handle list data we used to saw in lots of codes. It reads the data from the list, and do something withit one by one. The problem is, action of requesting server causes delay by waiting response of request. So in platform which should not allow freezing, like Android, it forces to implement http request actions in sub-thread to avoid lack on main(UI) thread.\nAs you can see, main client thread is doing nothing while waiting for response, and this makes the delay of process. Surely, there are ways to use thread in python. But because issues on memory management, python setup GIL(global interpreter lock) to keep it thread-safe, so multi-threading usually cannot improve performance for this.\nSo, how can asyncio make this better?\nEvent loop and coroutine This is sample code in python document: {% highlight python %} {% raw %} import asyncio\nasync def compute(x, y): print(“Compute %s + %s …” % (x, y)) await asyncio.sleep(1.0) return x + y\nasync def print_sum(x, y): result = await compute(x, y) print(\"%s + %s = %s\" % (x, y, result))\nloop = asyncio.get_event_loop() loop.run_until_complete(print_sum(1, 2)) loop.close() {% endraw %} {% endhighlight %}\nBy calling asyncio.get_event_loop(), you can get event loop, the repeated process of registering for events and reacting to them as they arrive. Think like kind of message queue. In this loop, we can put in coroutine which needs to be done. If you see the module compute, and print_sum, you can find async def in front of it. This is for to make these as coroutine.\nNow loop.run_until_complete(print_sum(1, 2)), it will put in print_sum(1, 2) to the loop and wait until it finish. If you see inside print_sum, there are await compute. This await keyword is being used in front of coroutine or task, and it means to wait for the result from it. And in compute, there are one more thing await asyncio.sleep(1.0), and as you can expect, it will wait for 1 second before returning value x + y. So the process will be:\nPut coroutine to event loop Run event loop Start print_sum(1, 2) Call compute(1, 2) Wait 1 second Return 3(1 + 2) Print “1 + 2 = 3” Close loop You can see this in flow chart: Asyncio and aiohttp Back to the code again: [async.py] {% highlight python %} {% raw %} import asyncio import aiohttp import requests import time\nURL_LIST = [ ‘https://www.python.org/’, ‘https://www.javascript.com/’, ‘https://www.amazon.com/’, ‘https://www.netflix.com/’, ‘https://www.scala-lang.org/’ ]\nasync def fetch(url): async with aiohttp.ClientSession() as session: async with session.get(url) as res: print(url + ’ / ’ + str(res.status))\ns = time.time() print(‘start: ’ + str(s))\ntasks = [fetch(url) for url in URL_LIST] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(tasks))\nprint(’end: ’ + str(time.time() - s))\nloop.close() {% endraw %} {% endhighlight %} It will be more easier to understand now. aiohttp.ClientSession().session is a method to request data with url. It has the same role with request.get in sync sample. But to use blocking I/O method(which stops the process until ends) such as request.get, you need to execute parallel by using run_in_executor. To avoid this, aiohttp offers lot of module to make this more simple.\nThis is the process:\ntasks will keep list of coroutines which are fetching each url in URL_LIST. Get event loop Put in coroutines to fetch url, and run the loop. Fetch the data from url asynchronous. Close the loop after all url data has been fetched. Really multi-thread? As mentioned about GIL above, this process is not actually using lots of thread, by python specification. It means it is not always useful. Moreover, it has more complexity of implementation comparing with coding in previous ways. But at least in scraping case, and other cases that blocking I/O could effect performance, it can use blocking time(request/response) to not wait and do other things such as request other data, or processing it.\nReference https://docs.python.org/ http://www.gevent.org/ https://stackoverflow.com/questions ","description":"","tags":["python","asyncio","aiohttp"],"title":"asyncio and aiohttp in Python 3.5↑","uri":"/posts/2018-06-16-python-async-aiohttp/"},{"categories":null,"content":"It has been few year since I started about knowing data research. And during now, I had some chance to study/use machine learning algorithm with great ML/DL modules, but didn’t think deep about data engineering to store or handle big data, because the data I treated was not so big. So for now, I’m trying to go on this by setting up hadoop, the most popular system for data engineering, and some of useful modules in hadoop echo system to getting know about this.\nApache Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. For storing quickly growing data, the scalability of Hadoop was fit to big data managing. And with this advantage, lots of data engineering project has been created based on Hadoop, and they formed as hadoop ecosystem.\nI will keep going on these projects, and setting up Hadoop is the first step for this.\nSetup Hadoop First, download hadoop from apache. Most of the link I found for hadoop was broken. This link works well for know, but could be lost any time. Download, decompress, and place the file where you want. {% highlight shell %} {% raw %} $ wget http://mirror.apache-kr.org/hadoop/common/hadoop-2.8.4//hadoop-2.8.4.tar.gz $ tar xvf hadoop-2.8.4.tar.gz {% endraw %} {% endhighlight %}\nAfter unzip, setup directory path in .bash_profile {% highlight shell %} {% raw %} …\nSet Hadoop export HADOOP_HOME=/path/to/hadoop export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin … {% endraw %} {% endhighlight %}\nNow you need to setup configuration files in this package. Update lines for file\netc/hadoop/core-site.xml: {% highlight xml %} {% raw %} fs.defaultFS hdfs://localhost:9000 {% endraw %} {% endhighlight %}\nThis shows the url to access hdfs system. Usual default is localhost:9000.\netc/hadoop/hdfs-site.xml: {% highlight xml %} {% raw %} dfs.replication 1 dfs.name.dir file:///path/to/save/namenode dfs.data.dir file:///path/to/save/datanode {% endraw %} {% endhighlight %}\ndfs.replication is a property which defines the number of replication block. Because hdfs system is designed as fault-tolerant, there are replicated blocks for keeping data available. HDFS system will keep number of ‘dfs.replication - 1’ backup blocks. Because this is just a testing, no replication is needed, so 1 will be enough. dfs.name.dir and dfs.data.dir is to define the local path to store hdfs data. This is important because if you don’t setup this, every data in hdfs will be vanished when you stop hdfs. Add file:// in front of local path to make it find from your computer.\nBefore you start, you could enable to access ssh localhost in your console. If it returns like Permission Denied, you need to generate passphrase. {% highlight shell %} {% raw %} $ cd ~/ $ ssh-keygen -t rsa -P ’’ -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub » ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys {% endraw %} {% endhighlight %}\nNow start with command in sbin directory. Using start-all.sh to run Yarn and HDFS(it will recommend to use start-dfs and start-yarn, but it does not cause a problem). {% highlight shell %} {% raw %} $ ./sbin/start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/kwangin/utils/hadoop/logs/hadoop-kwangin-namenode-kwangin-ui-MacBook-Pro.local.out localhost: starting datanode, logging to /Users/kwangin/utils/hadoop/logs/hadoop-kwangin-datanode-kwangin-ui-MacBook-Pro.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/kwangin/utils/hadoop/logs/hadoop-kwangin-secondarynamenode-kwangin-ui-MacBook-Pro.local.out 18/06/10 20:53:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable starting yarn daemons starting resourcemanager, logging to /Users/kwangin/utils/hadoop/logs/yarn-kwangin-resourcemanager-kwangin-ui-MacBook-Pro.local.out localhost: starting nodemanager, logging to /Users/kwangin/utils/hadoop/logs/yarn-kwangin-nodemanager-kwangin-ui-MacBook-Pro.local.out {% endraw %} {% endhighlight %}\nNow you can store data in hdfs system. {% highlight shell %} {% raw %} $ hdfs dfs -mkdir your_dir … $ hadoop fs -put /root/MyHadoop/file1.txt your_dir {% endraw %} {% endhighlight %}\nSetup Hive To execute SQL applications and queries over distributed data, you should implement it with MapReduce Java API, and It was quite a job. Some of people thought about it in early time, and implemented projects which called SQL-on-Hadoop. It a class set of analytical application tools that combines SQL-like querying with newer Hadoop data framework elements. By supporting (something like)SQL queries, it had been a great help to lot of data engineer and scientist work in Hadoop clusters. In nowadays(2018), there are over 10 projects which offers query for hadoop(though all of them has there own purpose…). In here, I’ll look on with project Hive.\nApache Hive is a data warehouse software project built on top of Apache Hadoop for providing data summarization, query and analysis. It is one of the oldest, and popular ‘SQL-on-Hadoop’ project.\nDownload it first(newest version is 2.3.3): {% highlight shell %} {% raw %} $ cd /usr/local $ wget wget http://mirror.apache-kr.org/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz $ tar xvf apache-hive-2.3.3-bin.tar.gz {% endraw %} {% endhighlight %}\nHive needs to know the path of Hadoop installed, so define it in environment file in conf/hive-env.sh\n{% highlight shell %} {% raw %} … HADOOP_HOME=/path/to/hadoop … {% endraw %} {% endhighlight %}\nAnd you need to give information where you will keep data for hive in conf/hive-site.xml. You can copy file hive-default.xml.template(it is in conf/), change name to hive-site.xml and edit it. Or just make a new one.\n{% highlight xml %} {% raw %}\n\u003c?xml version=\"1.0\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e ... hive.metastore.warehouse.dir /user/hive/warehouse ... {% endraw %} {% endhighlight %} Create directory for warehouse in hdfs.\n{% highlight shell %} {% raw %} $ cd /usr/local/hadoop-2.8.4 $ bin/hadoop fs -mkdir /tmp $ bin/hadoop fs -mkdir /user $ bin/hadoop fs -mkdir /user/hive $ bin/hadoop fs -mkdir /user/hive/warehouse $ bin/hadoop fs -chmod g+w /tmp $ bin/hadoop fs -chmod g+w /user/hive/warehouse {% endraw %} {% endhighlight %}\nThis defines the location of metastore warehouse. Metastore is for keeping informations related to databases, tables, and their relations in metadata form.\nNow you could start hive by hive command. But you could face on error something like this when trying to run any query, such as show databases;.\n... Exception in thread \"main\" java.lang.RuntimeException: Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string ... Okay, it said metastore is not initialized. Metadata is stored in an embedded database whose disk storage location is determined by the Hive configuration variable named javax.jdo.option.ConnectionURL. Add properties for this.\n{% highlight xml %} {% raw %}\n\u003c?xml version=\"1.0\"?\u003e \u003c?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\u003e ... javax.jdo.option.ConnectionURL jdbc:derby:metastore_db;create=true javax.jdo.option.ConnectionDriverName org.apache.derby.jdbc.EmbeddedDriver ... {% endraw %} {% endhighlight %} This means I’ll store metastore data with Apache Derby. You can also use mysql, but using Derby would be more simple. Recent version of hive includes derby module, so you don’t need to install it. If you want to use seperate derby application, set in to ConnectionURL.\nNow create initiate database. {% highlight shell %} {% raw %} $ cd bin $ schematool -initSchema -dbType derby SLF4J: Class path contains multiple SLF4J bindings. … Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore Connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting metastore schema initialization to 2.3.0 Initialization script hive-schema-2.3.0.derby.sql Initialization script completed schemaTool completed … {% endraw %} {% endhighlight %}\nIf it works, you will see metastore_db directory created.\nImport CSV, and generate database Installation is done, so setup file directory to create data table. Create a dummy directory for test, and put CSV file for test. The file is the training CSV file from titanic competition in Kaggle. {% highlight shell %} {% raw %} $ bin/hadoop fs -mkdir /user/hive/test $ hdfs dfs -put /user/hive/test/titanic /path/to/train.csv {% endraw %} {% endhighlight %}\nNow start hive, and import with sql query. I’ll just create table with first 4 columns in file(there are too many cols here…query becomes too long).\n{% highlight sql %} {% raw %} hive\u003e CREATE TABLE IF NOT EXISTS titanic(PassengerId INT, Survived INT, Pclass INT, Name STRING) \u003e ROW FORMAT DELIMITED \u003e FIELDS TERMINATED BY ‘,’ \u003e STORED AS TEXTFILE \u003e LOCATION ‘/user/hive/test/’; OK Time taken: 0.053 seconds hive (default)\u003e select * from titanic limit 5; titanic.passengerid\ttitanic.survived\ttitanic.pclass\ttitanic.name NULL\tNULL\tNULL\tName 1\t0\t3\t“Braund\tMr. Owen Harris” 2\t1\t1\t“Cumings\tMrs. John Bradley (Florence Briggs Thayer)” 3\t1\t3\t“Heikkinen\tMiss. Laina” 4\t1\t1\t“Futrelle\tMrs. Jacques Heath (Lily May Peel)” Time taken: 0.078 seconds, Fetched: 5 row(s) {% endraw %} {% endhighlight %}\nThese are pretty odd because column title are included…but anyway we could check how to add-on csv to hive database.\nFor now… The case I wrote will be pretty different with the product in field of work. It would use streaming service, or else for storing the data instead of porting CSV. There will be lots of ways to tune-up for upgrading performances of Hive, or some of will use other SQL-on-Hadoop project(like Presto, Drill, etc.). I’ll keep working on, and share here as much as possible.\nReference https://hadoop.apache.org/docs/stable/ https://cwiki.apache.org/confluence/display/Hive/GettingStarted https://www.tutorialspoint.com/hive/ https://princetonits.com/blog/technology/ https://www.dotnettricks.com/learn/hadoop/ ","description":"","tags":["apache","hadoop","hive","data engineering"],"title":"Start-on Hadoop, with Hive","uri":"/posts/2018-05-18-hadoop-hive-setup/"},{"categories":null,"content":"First time I heard of Akka, was the blog post written about case of Twitter, that they implemented their service based on this to handle massive twit data in real-time. I’ve had some experiences developing web server, and currently having interests of concurrency issue, and it makes me looking on this.\nActor model As you could see in other documents, theory of Actor has been more than 40 years. But before early 21 century, we usually did not need to think about concurrency or multi-threading deeply, because there were no smartphones which causes heavy requests and no big data to handle in real-time. Now, massive internet services like Facebook and Twitter needs to handle millions of requests per minute, and so they are finding/developing platform to satisfy this.\nActor is an object, composed with behavior, state, mailbox. Behavior is the job which Actor need to do, state is the current state of Actor, and mailbox is to send/receive the message from other Actors.\nYes, this has some similarity with Thread. In programming we could use these to run process asyncrounously. The point which makes it different is, each of them cannot access to other actor’s memory or state. Each of them are only doing their job without sharing other’s area and every communication is being done by mailbox. As there is no sharing resources, there is no state like lock in Thread because it does not need to wait for waiting memory from others. It just sends the message without blocking so you can get benefits of multi-thread programming without worring about the problems you can face with ‘sharing’ issue.\nAkka Akka is a toolkit + runtime for building highly concurrent, distributed, and fault-tolerant applications which is based on actor model. It is OSS(Open Source Software), and currently managed by Lightbend. You could find this in lots of module for handling concurrency and distribution web service. Core Akka is implemented in Scala, but they also offer Java SDK for development.\nBefore creating Hello world stuff, let’s check the relationships between actors you create in your code and what happens below the modules.\nAn actor in Akka always belongs to a parent. If you generate new actor in your code, it will be created as child of /user actor, which are being created by system. When you setup the Akka library and start the code, it will create three actors including /user. The names of these built-in actors contain guardian because they supervise every child actor in their path.\nThe one in top is root guardian. It is the parent of the all actors in system, and will be removed after system is destroyed. /user is parent actor for all user created actors. Every actor you create will have the path ‘/user/’ prepended to it. /system is the system guardian. Akka implementations The ‘quickstart’ code used here is from official page of Lightbend. You can download full project from here.\nNow let’s see the main file. It starts with creating actors.\n{% highlight java %} {% raw %} final ActorSystem system = ActorSystem.create(“helloakka”);\nfinal ActorRef printerActor = system.actorOf(Printer.props(), “printerActor”); final ActorRef howdyGreeter = system.actorOf(Greeter.props(“Howdy”, printerActor), “howdyGreeter”); final ActorRef helloGreeter = system.actorOf(Greeter.props(“Hello”, printerActor), “helloGreeter”); final ActorRef goodDayGreeter = system.actorOf(Greeter.props(“Good day”, printerActor), “goodDayGreeter”); … {% endraw %} {% endhighlight %}\nFirst line is to create container which actors will be placed. Now actor instance create by system.actorOf will be child of user node which we saw in diagram above. In this code, it creates three ‘Greeter’ actor and one ‘Printer’ actor. Now we need to see what ‘Printer’ and ‘Greeter’ is for.\n{% highlight java %} {% raw %} public class Printer extends AbstractActor { static public Props props() { return Props.create(Printer.class, () -\u003e new Printer()); }\nstatic public class Greeting { public final String message;\npublic Greeting(String message) { this.message = message; } }\nprivate LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\npublic Printer() { }\n@Override public Receive createReceive() { return receiveBuilder() .match(Greeting.class, greeting -\u003e { log.info(greeting.message); }) .build(); } } {% endraw %} {% endhighlight %}\n‘Printer’ extends akka.actor.AbstractActor class to implement actor instance. Implemented method createReceive will be called when message from other instance is received. The role of this class is to create log instance and show the message received from ‘Greeter’ class.\n{% highlight java %} {% raw %} public class Greeter extends AbstractActor { static public Props props(String message, ActorRef printerActor) { return Props.create(Greeter.class, () -\u003e new Greeter(message, printerActor)); }\nstatic public class WhoToGreet { public final String who;\npublic WhoToGreet(String who) { this.who = who; } }\nstatic public class Greet { public Greet() { } }\nprivate final String message; private final ActorRef printerActor; private String greeting = “”;\npublic Greeter(String message, ActorRef printerActor) { this.message = message; this.printerActor = printerActor; }\n@Override public Receive createReceive() { return receiveBuilder() .match(WhoToGreet.class, wtg -\u003e { this.greeting = message + “, \" + wtg.who; }) .match(Greet.class, x -\u003e { printerActor.tell(new Greeting(greeting), getSelf()); }) .build(); } } {% endraw %} {% endhighlight %}\n‘Greeter’ class also extends akka.actor.AbstractActor class. You can see constructor in here requests two parameter, ‘message(String)’ and ‘printerActor(ActorRef)’. As you can see in createReceive method, it returns receiveBuilder to define the behavior depends on the message received. It expects two types of messages, ‘WhoToGreet’ and ‘Greet’. The former will update the greeting state of the Actor with received message string while the latter one will trigger a sender of updated greeting message to the Printer Actor.\nNow, let’s see the remain codes in main class.\n{% highlight java %} {% raw %} … howdyGreeter.tell(new WhoToGreet(“Akka”), ActorRef.noSender()); howdyGreeter.tell(new Greet(), ActorRef.noSender());\nhowdyGreeter.tell(new WhoToGreet(“Lightbend”), ActorRef.noSender()); howdyGreeter.tell(new Greet(), ActorRef.noSender());\nhelloGreeter.tell(new WhoToGreet(“Java”), ActorRef.noSender()); helloGreeter.tell(new Greet(), ActorRef.noSender());\ngoodDayGreeter.tell(new WhoToGreet(“Play”), ActorRef.noSender()); goodDayGreeter.tell(new Greet(), ActorRef.noSender()); … {% endraw %} {% endhighlight %}\nActorRef.tell method is to putting the message into Actor’s mailbox. By this codes, it calls createReceive implemented in each ActorRef class. As we see above, first line will add “Akka” string in greeting message, while second will send greeting message to ‘Printer’ class to display this via Log method, and so on.\nMessage flow This is the process flow which happens in code. Each of ‘Greeting’ instance generate, and sends message to ‘Printer’ instance to show this.\nOne thing to make sure is, all of actor is independent and work asyncrounously. This means it does not guarantee the order of process. Though we call the classes in order of ‘howdyGreeter’ -\u003e ‘helloGreeter’ -\u003e ‘goodDayGreeter’, the printed result can be changed by process handling duration.\nThis is the first term… {% highlight shell %} {% raw %} $ gradle run\nTask :run\nPress ENTER to exit «\u003c [INFO] [04/11/2018 19:43:02.692] [helloakka-akka.actor.default-dispatcher-4] [akka://helloakka/user/printerActor] Hello, Java [INFO] [04/11/2018 19:43:02.693] [helloakka-akka.actor.default-dispatcher-4] [akka://helloakka/user/printerActor] Good day, Play [INFO] [04/11/2018 19:43:02.693] [helloakka-akka.actor.default-dispatcher-4] [akka://helloakka/user/printerActor] Howdy, Akka [INFO] [04/11/2018 19:43:02.693] [helloakka-akka.actor.default-dispatcher-4] [akka://helloakka/user/printerActor] Howdy, Lightbend \u003c=========—-\u003e 75% EXECUTING [19s] … {% endraw %} {% endhighlight %}\nand this is second term… {% highlight shell %} {% raw %} $ gradle run\nTask :run\nPress ENTER to exit «\u003c [INFO] [04/11/2018 19:43:25.298] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/printerActor] Howdy, Akka [INFO] [04/11/2018 19:43:25.299] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/printerActor] Howdy, Lightbend [INFO] [04/11/2018 19:43:25.299] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/printerActor] Hello, Java [INFO] [04/11/2018 19:43:25.299] [helloakka-akka.actor.default-dispatcher-3] [akka://helloakka/user/printerActor] Good day, Play \u003c=========—-\u003e 75% EXECUTING [33m 14s] … {% endraw %} {% endhighlight %}\nOkay, this is the first access with Akka. Hope this could help you to go on. If you are more interested in this, you could find more from official site, GitHub, or Google.\nReference https://doc.akka.io https://developer.lightbend.com/guides/ ","description":"","tags":["actor","akka","java"],"title":"Actor model, and implementation with Akka","uri":"/posts/2018-03-20-actor-and-akka/"},{"categories":null,"content":"As I wrote in previous post, most dataset in real world are not clean. They are usually messed up, incompatible(ex:’-1’ is included in data which defines people’s age), and some of are missing. These kind of dataset will mostly cause error, or return wrong result when you just put on your elaborated logic. That’s why you need to concern pre-process before researching it.\nThis is the survey from data scientists. You can find out this work will be unpleasure, takes lot of time, but also shows the importance of this work.\nI’ll add on some pre-processing logic in data which I worked on previous post, to see how this process makes change.\nData transform Look values in Name column, and you could find there are additional title such as ‘Mr’, ‘Mrs’, ‘Sir’, ‘Dr’, or else. By this data, you could assume target’s gender or position. Because this is story of early 20 century, which was hierarchical society, so we could estimate this could have relation with survival rate. Let’s add new column Title to use this.\n{% highlight python %} {% raw %} train_df[‘Title’] = train_df[‘Name’].str.extract(’([A-Za-z]+).’) pd.crosstab(train_df[‘Title’], train_df[‘Sex’]) {% endraw %} {% endhighlight %}\nNow column Title keeps parsed value from name, but there are bit varied so need to wrap similar values. Purpose for this column is to find out relation between ’title’ and ‘survival’ so I’ll wrap values with new name by its rank. Also, unify value of different spelling but has similar meaning, such as ‘Mlle’ and ‘Ms’ which all can be equal with ‘Miss’.\n‘Capt’, ‘Col’, ‘Don’, ‘Dr’, ‘Major’, ‘Rev’, ‘Jonkheer’, ‘Dona’ =\u003e Rare(others) ‘Countess’, ‘Lady’, ‘Sir’ =\u003e Royal(people of high standing) ‘Mlle’, ‘Ms’ =\u003e Miss ‘Mme’ =\u003e Mrs Now redefine value of Title, and compare survival rates by this.\n{% highlight python %} {% raw %} train_df[‘Title’] = train_df[‘Title’].replace([‘Capt’, ‘Col’, ‘Don’, ‘Dr’, ‘Major’, ‘Rev’, ‘Jonkheer’, ‘Dona’], ‘Rare’) train_df[‘Title’] = train_df[‘Title’].replace([‘Countess’, ‘Lady’, ‘Sir’], ‘Royal’) train_df[‘Title’] = train_df[‘Title’].replace([‘Mlle’, ‘Ms’], ‘Miss’) train_df[‘Title’] = train_df[‘Title’].replace(‘Mme’, ‘Mrs’) train_df[[‘Title’, ‘Survived’]].groupby([‘Title’], as_index=False).mean() {% endraw %} {% endhighlight %}\nSurvival rate of people with title ‘Miss’/‘Mrs’, who are female, appears high, as we look before. And also, who has title ‘Royal’/‘Master’, that we could assume high-positioned people, also survived more than others. So we could define as ‘people’s rank is related with survival rate’.\nMissing value treatment - Age There are some ways for treating missing values\nDelete Replace with mean or mode value Replace with estimated value using regression or else etc. In this post, I’ll go on with 2, but not just fill mean value of all data in all missing data. I’ll work with assumption:\nPeople who has same Title will be in same age group.\nSo first look on mean value of age by Title. {% highlight python %} {% raw %} train_df.groupby(‘Title’)[‘Age’].mean() {% endraw %} {% endhighlight %}\n…it will be: {% highlight shell %} {% raw %} Title Master 4.574167 Miss 21.845638 Mr 32.368090 Mrs 35.788991 Rare 45.894737 Royal 43.333333 Name: Age, dtype: float64 {% endraw %} {% endhighlight %}\nNow fill in value to missing data. {% highlight python %} {% raw %} train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Master’),‘Age’] = 5 train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Miss’),‘Age’] = 22 train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Mr’),‘Age’] = 33 train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Mrs’),‘Age’] = 36 train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Rare’),‘Age’] = 45 train_df.loc[(train_df[‘Age’].isnull())\u0026(train_df[‘Title’]==‘Royal’),‘Age’] = 43 {% endraw %} {% endhighlight %}\nData binning - Age The case we are looking is binary classification, as there are only two label for result(live or die). In this case, continuous data needs to be grouped as category to deduct classifed result.\nData binning is a process wraping continuous data with group by specific criteria. My rule is:\nLess than 7 : Baby \u0026 Kids (0) 8~20: Students (1) 21~30: Young Adults (2) 31~40: Adults (3) 41~60: Seniors (4) More than 60: Elders (5) I’ll make a column AgeGroup, and update these value. {% highlight python %} {% raw %} train_df[‘AgeGroup’] = 0 train_df.loc[ train_df[‘Age’] \u003c= 7, ‘AgeGroup’] = 0 train_df.loc[(train_df[‘Age’] \u003e 7) \u0026 (train_df[‘Age’] \u003c= 18), ‘AgeGroup’] = 1 train_df.loc[(train_df[‘Age’] \u003e 18) \u0026 (train_df[‘Age’] \u003c= 30), ‘AgeGroup’] = 2 train_df.loc[(train_df[‘Age’] \u003e 30) \u0026 (train_df[‘Age’] \u003c= 40), ‘AgeGroup’] = 3 train_df.loc[(train_df[‘Age’] \u003e 40) \u0026 (train_df[‘Age’] \u003c= 60), ‘AgeGroup’] = 4 train_df.loc[ train_df[‘Age’] \u003e 60, ‘AgeGroup’] = 5 f,ax=plt.subplots(1,1,figsize=(10,10)) sns.countplot(‘AgeGroup’,hue=‘Survived’,data=train_df, ax=ax) plt.show() {% endraw %} {% endhighlight %}\nPeople in group 0(Baby \u0026 Kids) has high survival rate, while other groups looks same. We can think that elder people make way to live to yongsters.\nOutlier, and data binning - Family member There are also data which defines family members, SibSp(Sibling \u0026 Spouse) and Parch(Parent \u0026 children). In my mind, priority of both are same, so I’ll merge these 2 into new column FamilyMember.\n{% highlight python %} {% raw %} train_df[‘FamilyMembers’] = train_df[‘SibSp’] + train_df[‘Parch’] + 1 pd.crosstab([train_df[‘FamilyMembers’]],train_df[‘Survived’]).style.background_gradient(cmap=‘summer_r’) {% endraw %} {% endhighlight %}\nSomething weird is, there are 7 people wrote as they are with 11 family member, and it means at least 4 people wrote wrong number. So you could think there are several ‘outlier’ in this data. So I’ll not use this column directly, but create new one based on this data with rule:\n‘Existence’ of family member would be important, more than the number.\nbecause family member should help each other(maybe not…) to survive. New columne IsAlone is defined as 0 if it has family member, else 1.\n{% highlight python %} {% raw %} train_df[‘IsAlone’] = 0 train_df.loc[train_df[‘FamilyMembers’] == 1, ‘IsAlone’] = 1 f, ax=plt.subplots(1,2,figsize=(20,8)) ax[0].set_title(‘Survived, with family’) train_df[‘Survived’][train_df[‘IsAlone’]==0].value_counts().plot.pie(explode=[0,0.1],autopct=’%1.1f%%’,ax=ax[0],shadow=True) train_df[‘Survived’][train_df[‘IsAlone’]==1].value_counts().plot.pie(explode=[0,0.1],autopct=’%1.1f%%’,ax=ax[1],shadow=True) ax[1].set_title(‘IsAlone Survived’) plt.show() {% endraw %} {% endhighlight %}\nAs assumed, people with family’s survival rate(50%) was higher than who is alone(30%).\nModeling with pre-processed data It used same modeling algorithm with previous result(Decision tree), but add columns we generated in this post, TitleKey, AgeGroup and IsAlone.\n{% highlight python %} {% raw %} train, test = train_test_split(train_df,test_size=0.3,random_state=0) target_col = [‘Pclass’, ‘Sex’, ‘Embarked’, ‘TitleKey’, ‘AgeGroup’, ‘IsAlone’] train_X=train[target_col] train_Y=train[‘Survived’] test_X=test[target_col] test_Y=test[‘Survived’] features_one = train_X.values target = train_Y.values tree_model = DecisionTreeClassifier() tree_model.fit(features_one, target) dt_prediction = tree_model.predict(test_X) print(‘The accuracy of the Decision Tree is’,metrics.accuracy_score(dt_prediction, test_Y)) {% endraw %} {% endhighlight %}\nThe result of accuracy is 0.813432835821, bit higher than before(0.809701492537).\nActually in pre-processing, as much as understanding algorithm, you need to have knowledge about period/situation when data occured. By knowing the background, you can make data more closer to the real world. So for good data research, you need to think about ‘story’ of the data and having imagination from it based on situation.\nReference https://www.kaggle.com/c/titanic/kernels https://en.wikipedia.org/wiki/Data_pre-processing https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#770575b56f63 ","description":"","tags":["python","data","pre-process"],"title":"Pre-process data, for research","uri":"/posts/2018-01-15-data-pre-processing/"},{"categories":null,"content":"kaggle is a platform for competiting data analytic and predictive modeling. Lots of companies post their raw data, and researchers compete to find best prediction from here. You can use Python, R, or Julia for a research. I’ll work on with Python here. Each researched records are managed as kernel, and you can make your own or look on other user’s data.\nTitanic research This is one of most famous topic in data research, and kaggle suggest this as tutorial for kaggle platform, and data research. I’ll not explain in detail about Titanic cause it is most infamous disasters and all of you will know about this very well. In this research, given data is list of passengers and crew, and informations of them like age, embarked, sex, and more. Goal of this analysis is to find out what sort of people were likely to survive.\nIf you are first in data research so don’t know how to start on, you can refer other people’s kernel. Some of good kernels are commented very well to look and follow up how they work on. Or you can just look through blog posts in Google about ‘Titanic data research’ cause it is very popular tutorial. Also I’m a starter in this field, I followed research tutorial in Datacamp, and public notebooks of other great engineers.\nLook on data, and startup your notebook First thing you have to do when face on dataset, is to read and find out what is in it.\nThis is the given data, which you need to research on. Each of column means…\nsurvival Survival 0 = No, 1 = Yes pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd sex Sex Age Age in years sibsp # of siblings / spouses aboard the Titanic parch # of parents / children aboard the Titanic ticket Ticket number fare Passenger fare cabin Cabin number embarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton There are 2 given files. train.csv is for training, and you need to create research model based on this. test.csv is a data you need to predict survivers with your model, so there is no survival column here.\nNow when you create new notebook for research, you would find default code already been set.\n{% highlight python %} {% raw %}\nThis Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python For example, here’s several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nInput data files are available in the “../input/” directory. For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory from subprocess import check_output print(check_output([“ls”, “../input”]).decode(“utf8”)) {% endraw %} {% endhighlight %}\nThis code first imports the basic module for data research. Most of the Python user would need numpy and pandas for calculation, data process, and data I/O(maybe someone should not, but I cannot expect). Second, it shows the path of data file is in ‘../input’ directory. So we could import and check the data like\n{% highlight python %} {% raw %} train_df = pd.read_csv(’../input/train.csv’) # training dataframe test_df = pd.read_csv(’../input/test.csv’) # test dataframe train_df.head() {% endraw %} {% endhighlight %}\nVisualize dataset Now the data is set in pandas Dataframe. It makes more simple to use plot modules for visualization, such as matplotlib, seaborn, bokeh, and other. To see the precise data, it would be good to see it as numbers in table. But to compare multiple parameter and look on data approximatively, make the data in chart would reduce your headache.\n{% highlight python %} {% raw %}\nvisualization import seaborn as sns import matplotlib.pyplot as plt\nf, ax=plt.subplots(1, 2, figsize=(18,8)) train_df[‘Survived’].value_counts().plot.pie(explode=[0,0.1],autopct=’%1.1f%%’,ax=ax[0],shadow=True) ax[0].set_title(‘Survived’) ax[0].set_ylabel(’’) sns.countplot(‘Survived’,data=train_df,ax=ax[1]) ax[1].set_title(‘Survived’) plt.show() {% endraw %} {% endhighlight %}\nThis is the chart which compares the survivals and deads. You can clearly see it that more than twice of survival people are dead.\n{% highlight python %} {% raw %} f, ax = plt.subplots(1, 2, figsize=(18, 8)) train_df[‘Survived’][train_df[‘Sex’]==‘male’].value_counts().plot.pie(explode=[0,0.1],autopct=’%1.1f%%’,ax=ax[0],shadow=True) train_df[‘Survived’][train_df[‘Sex’]==‘female’].value_counts().plot.pie(explode=[0,0.1],autopct=’%1.1f%%’,ax=ax[1],shadow=True) ax[0].set_title(‘Survived (male)’) ax[1].set_title(‘Survived (female)’)\nplt.show() {% endraw %} {% endhighlight %}\nThis compares the survivals by Sex, and you can find out survival rate of female is much higher than man(almost 3 times more!).\n{% highlight python %} {% raw %} pd.crosstab([train_df[‘Sex’],train_df[‘Survived’]],train_df[‘Pclass’],margins=True).style.background_gradient(cmap=‘summer_r’) {% endraw %} {% endhighlight %}\nThis is the table which shows the number of survived and dead by Sex, Pclass. This is bit hard to look on, but you can figure out that passenger in higher class are more survived.\n{% highlight python %} {% raw %} f, ax = plt.subplots(2, 2, figsize=(20,15)) sns.countplot(‘Embarked’, data=train_df,ax=ax[0,0]) ax[0,0].set_title(‘No. Of Passengers Boarded’) sns.countplot(‘Embarked’,hue=‘Sex’,data=train_df,ax=ax[0,1]) ax[0,1].set_title(‘Male-Female Split for Embarked’) sns.countplot(‘Embarked’,hue=‘Survived’,data=train_df,ax=ax[1,0]) ax[1,0].set_title(‘Embarked vs Survived’) sns.countplot(‘Embarked’,hue=‘Pclass’,data=train_df,ax=ax[1,1]) ax[1,1].set_title(‘Embarked vs Pclass’) plt.show() {% endraw %} {% endhighlight %}\nThis shows the passenger data based on where they aboard on. Based on chart, more than half of people came in from S(Southampton), and rate of 1st class passenger from C(Cherbourg) is higher than other port.\nLike this, making data as chart or table helps overviewing on data, and compare each columns.\nSimple pre-processing Now, I’ll try to make a simple data model with these information(Sex, Pclass, Embarked). There are other more columns in the table, but others are not good to use without data clensing. Most ‘pure’ datasets are not good to use directly. There are lots of missing/noisy data, outlier, duplicated value. Also, to make data more useful, it needs to be reduce, and grouped. This process is called Data pre-processing. To explain this will be very long story, so I’ll try to show more about this in next post. This time, I’ll just fill in the missing value because you could not generate model with this.\n{% highlight python %} {% raw %}\nshow numbers of nulls in each column train_df.isnull().sum() {% endraw %} {% endhighlight %}\n{% highlight shell %} {% raw %} PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 {% endraw %} {% endhighlight %}\nThere are 2 nulls in Embarked, so I’ll fill this with S because more than half of the current passenger has S. Actually you need lots of research to estimate missing value, but I’ll do it in simple way cause this is not the main story.\n{% highlight python %} {% raw %} train_df[‘Embarked’].fillna(‘S’,inplace=True) {% endraw %} {% endhighlight %}\nNow it is ready.\nGenerate data model, and predict the result. I will generate data model via DecisionTree algorithm with ‘Pclass/Sex/Embarked’, which is in scikit-learn module. I’ll split train data by 7:3, create model with 7 and check the result with other 3.\n{% highlight python %} {% raw %} from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics #accuracy measure\ntrain, test = train_test_split(train_df, test_size=0.3,random_state=0) target_col = [‘Pclass’, ‘Sex’, ‘Embarked’]\ntrain_X=train[target_col] train_Y=train[‘Survived’] test_X=test[target_col] test_Y=test[‘Survived’]\nfeatures_one = train_X.values target = train_Y.values\ntree_model = DecisionTreeClassifier() tree_model.fit(features_one, target) dt_prediction = tree_model.predict(test_X)\nprint(‘The accuracy of the Decision Tree is’,metrics.accuracy_score(dt_prediction, test_Y)) {% endraw %} {% endhighlight %}\nData model is created as tree_model, and predict accuracy of splited data and predicted result from this model shows ‘0.809701492537’.\nTo generate the data for competition, you need to follow the format which they suggest. In this competition, it requires to create a csv file format with 2 columns, PassengerID and Survived. PassengerID is the ID of passengers in test.csv, and Survived is where you need to put in your predict result from data model.\n{% highlight python %} {% raw %}\npredict test data with pre-trained tree model test_features = test_df[target_col].values dt_prediction_result = tree_model.predict(test_features)\nCreate a data frame with two columns: PassengerId \u0026 Survived. Survived contains your predictions PassengerId = np.array(test_df[“PassengerId”]).astype(int) dt_solution = pd.DataFrame(dt_prediction_result, PassengerId, columns = [“Survived”])\nWrite your solution to a csv file with the name my_solution.csv dt_solution.to_csv(“my_solution_one.csv”, index_label = [“PassengerId”]) {% endraw %} {% endhighlight %}\nIt’s done. Publish your notebook with this code, and it will generate my_solution_one.csv file. If it causes no error and format is correct, you will see a new column output in competition menu. Go in, and you will find your generated file there. Submit this for join in competition.\nNow it will check your file, and show the score and rank.\nWell, I’m still in rank 4849 with accuracy score 0.77990, but it is not important here. This is just for looking on how to join in Kaggle competition, and check the basic rule of researching given data here. There are some people who has accuracy score 1, but it is quite overfitted result so most of the people think it cannot be trusted.\nThis time, I focused only on introducing Kaggle, looking in data, and involving in competition. I’ll write on more about data pre-processing, and figure out other ways to improve data model.\nReference https://www.kaggle.com/ https://www.kaggle.com/c/titanic/kernels https://www.datacamp.com/community/open-courses/kaggle-python-tutorial-on-machine-learning ","description":"","tags":["python","data research","kaggle"],"title":"First data research in kaggle","uri":"/posts/2017-11-25-first-kaggle-research/"},{"categories":null,"content":"This is the following post of Create own module project - Android, and shows the way to keep your module stable continuously.\nMake tests Test is important to make your project reliable. As your project going bigger, there will be lots of bugs, and it is tough to keep your codes stable while fixing and modifing it. It is pretty easy to setup your test for Android. Actually, it builds up when you create it as ExampleInstrumentedTest and ExampleUnitTest. Instrument test is for testing UI process flow, and it is being tested on virtual/real devices. Unit test is for testing internal data logic of classes or modules and it is running on JVM, so you cannot access Android APIs like Context or else.\nThis is to just look on process of test, so I’ll make a simple unit test.\nCheck gradle file and see module for test has been included(usually it has by default). {% highlight shell %} {% raw %} dependencies { … testCompile ‘junit:junit:4.12’ } {% endraw %} {% endhighlight %}\nNow write test code. [ExampleUnitTest.java] {% highlight java %} {% raw %} public class ExampleUnitTest {\nprivate static final String TARGET_TEST_STRING = \"My name is John. Last name is Doe. Doe family is living here.\"; private Reducer reducer; @Before public void setupParam() { reducer = Reducer.getInstance(); } @Test public void testSpaceDelimeter() throws Exception { String delimeter_space = \" \"; ArrayList\u003cWordPair\u003e parsedText = reducer.textToPair(TARGET_TEST_STRING, delimeter_space); assertEquals(parsedText.size(), 9); assertEquals(parsedText.get(1).word, \"name\"); assertEquals(parsedText.get(1).number, 2); } @Test public void moreTest() throws Exception { // more test... } } {% endraw %} {% endhighlight %}\nThis is to test textToPair logic what we just tried now.\nIn test code, there are several decorator being used to define test process. @Before is the function which will be running before starting test, and functions with @Test decorator will be running in actual test. If you need something to do after test, use @After. It once has been defined with fixed function name(runnning function with name setUp before test, and test functions which starts with name test), but it has been changed from AndroidJUnit4.\nassertEquals is to compare results with expected value. It compares 2 parameter and return exception when it is not equal value. Code in testSpaceDelimeter checks whether size of list parsedText are 9, second value of list is “name”, and number of second value is 2. Because it is all correct as we see in Activity code, test passes here. You can run test simply in Android Studio with Run button.\nOpen, and manage your project Before going on, I’ll update this project to Github.\nNow project is opened, and ready to go further. We have module for word counting and test logic to check it is working correctly. You could improve current logic, fix bugs, or create new one. Maybe other committer who are interested in this project could do it. But during project being grown up, some of commits could break module’s logic and make test fail. So it needs a logic to check status continuously, and Travis could help it.\nContinuous integration with Travis CI Travis CI is a continuous integration platform used to build and test software projects hosted in GitHub. Unless you are planning to make your repository private, you can use it for free.\nFor activate travis, you need to add .travis.yml script in root directory of project. It will include projects build definitions. {% highlight yaml %} {% raw %} language: android jdk: oraclejdk8\nandroid: components: - platform-tools - tools - build-tools-25.0.0 - android-25 - extra-android-m2repository - extra-google-m2repository - extra-android-support\nlicenses: - ‘android-sdk-preview-license-.+’ - ‘android-sdk-license-.+’ - ‘google-gdk-license-.+’\nscript: - ./gradlew test {% endraw %} {% endhighlight %}\nThis is the setting I used. It needs to define language and it is set as Android(though it is not a language, but it needs different logic with common Java). Below android, it defines components which needs for project build, and a license for android components. Version of ‘build-tools’ and ‘android’ below ‘components’ has to be same with the value defined in gradle script.\nOne thing you should know is, you need to define Java as ‘oraclejdk8’ to make it work. ‘oraclejdk7’ does not work anymore in travis, and Android does not support ‘openjdk’. You could find detail about this here.\nAnd finally put in script to define command needs to be run on deployment.\nIf you added this file, find your repo in travis-ci and make it enable to run the script you made.\nThis makes your build script triggered with push event to this repository. I pushed simple README file, and travis will recognize it like this.\nNow you are prepare to develop/manage your project.\nReference https://developer.android.com/index.html https://riggaroo.co.za/introduction-automated-android-testing/ https://travis-ci.org/ ","description":"","tags":["android","module","test","ci"],"title":"Test your module continuously - Android","uri":"/posts/2017-11-05-android-test-lib/"},{"categories":null,"content":"One of my interests is distribute computing for data research, so it made me looking on open source projects for this feature. One of important point in distribution is managing status of each server, and communicating between them. I found out lots of them(such as storm, kafka, and much more) are using zookeeper for this feature, and this is why I started to looking on it.\nWhat is zookeeper Zookeeper is a service for coordinating distributed system. It includes feature for configuration management, synchronization, group services, etc. and these are offered with simple interface to make easy usage for developers.\nThis is simple sketch of Zookeeper managed system. In this diagram, server means the machine which zookeeper actually runs, and client is the target which are being managed. Zookeeper service is made up with multiple server for managing each componenets in distribution system. One of them becomes a leader, and others are called follower. As you see in image, followers service clients and all updates go through order of follower -\u003e leader -\u003e other followers.\nZookeeper data model Data in zookeeper is for store coordination data of service, like status information, configuration, location information, and more. It uses a data model styled after the familiar directory tree structure of file systems.\nThese data nodes are being called znode. Each of them are identified with name and seperated by sequence of path. Every znode maintains a stat structure. A stat simply provides the metadata, which has version number, action control list (ACL), timestamp, and data length information.\nThere are 2 types of node, Persistence znode and Ephemeral znode. Ephemeral znode are kept alive only while client session is connected, and deleted automatically when connection is lost. On the contrary, persistence znode are persistent regardless of client state unless it has been specified changing state.\nStart zookeeper Start with install zookeeper. You can download from release page or use brew if using Mac. Use zkServer command for zookeeper.\n{% highlight shell %} {% raw %} $ brew install zookeeper … // after install $ zkServer ZooKeeper JMX enabled by default Using config: /usr/local/etc/zookeeper/zoo.cfg Usage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd} {% endraw %} {% endhighlight %}\nConfiguration for zookeeper service are being set in zoo.cfg. If you want to connect multiple server, add server list here as server.[seq number]=address. Instead just for test with single laptop, you don’t need to add this.\n{% highlight shell %} {% raw %}\nThe number of milliseconds of each tick tickTime=2000\nThe number of ticks that the initial synchronization phase can take initLimit=10\nThe number of ticks that can pass between sending a request and getting an acknowledgement syncLimit=5\nthe directory where the snapshot is stored. do not use /tmp for storage, /tmp here is just example sakes. dataDir=/usr/local/var/run/zookeeper/data\nthe port at which the clients will connect clientPort=2181\nthe maximum number of client connections. increase this if you need to handle more clients #maxClientCnxns=60 …\nadd server list for connection server.1=192.168.0.1:2888:3888 server.2=192.168.0.2:2888:3888 server.3=192.168.0.3:2888:3888 {% endraw %} {% endhighlight %}\nNow open 2 terminal window and start service.\n[1] {% highlight shell %} {% raw %} $ zkServer start-foreground ZooKeeper JMX enabled by default Using config: /usr/local/etc/zookeeper/zoo.cfg {% endraw %} {% endhighlight %}\n[2] {% highlight shell %} {% raw %} $ zkCli -server 127.0.0.1:2181 Connecting to 127.0.0.1:2181 Welcome to ZooKeeper! JLine support is enabled [zk: 127.0.0.1:2181(CONNECTING) 0] WATCHER:: {% endraw %} {% endhighlight %}\nThis is totally simple!. It tells that it has been connected to zkServer. Now let’s see the data model.\n{% highlight shell %} {% raw %} [zk: 127.0.0.1:2181(CONNECTED) 1] ls / [zookeeper] [zk: 127.0.0.1:2181(CONNECTED) 2] ls /zookeeper [quota] [zk: 127.0.0.1:2181(CONNECTED) 3] ls /zookeeper/quota [] {% endraw %} {% endhighlight %}\nThere are only one root directory ‘zookeeper’ and there are one child ‘quota’. You can make new ones with create \u003cpath\u003e \u003cdata\u003e command. With -s flag, it creates sequential node. To see information of node, use get \u003cpath\u003e.\n{% highlight shell %} {% raw %} [zk: 127.0.0.1:2181(CONNECTED) 2] create /new-node node1 Created /new-node [zk: 127.0.0.1:2181(CONNECTED) 3] get /new-node node1 cZxid = 0x1de ctime = Tue Oct 17 00:40:36 JST 2017 mZxid = 0x1de mtime = Tue Oct 17 00:40:36 JST 2017 pZxid = 0x1de cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 [zk: 127.0.0.1:2181(CONNECTED) 4] create -s /second-node node2 Created /second-node0000000003 [zk: 127.0.0.1:2181(CONNECTED) 5] get /second-node0000000003 node2 cZxid = 0x1df ctime = Tue Oct 17 00:43:00 JST 2017 mZxid = 0x1df mtime = Tue Oct 17 00:43:00 JST 2017 pZxid = 0x1df cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 {% endraw %} {% endhighlight %}\nNext goal This is just a starting point of going on zookeeper. As I mentioned, the reason I started to looking on it is to see how this coordination system are being used in lots of distribution systems.\nReference http://zookeeper.apache.org https://www.tutorialspoint.com/zookeeper ","description":"","tags":["zookeeper","open source"],"title":"Zookeeper overview","uri":"/posts/2017-10-16-lookon-zookeeper/"},{"categories":null,"content":"e2e, or end-to-end testing is a testing method that goes on to actual application usage flow. It means, it tests your applicaion in user’s point of view. It is to check whether client showing correct information what system requests. You can register test scenario like components shows/hides correctly, action(button click, drag\u0026drop) works well, and more things related with user interfaces/moves.\nAs you can expect, there are cons comparing with unit test. It takes long time, it is not reliable, and cannot isolate faliures. Though it is still important because unit test can find problems in data, but it could miss the bugs on user’s usage flow.\nLike this chart, it has different purpose with unit test. It is focusing on user interaction and UI instead of checking code method and result value. So if you are running web service and client/server project has been seperated, you could think about applying e2e test logic for client. Unit test will be suitable for your server project, cause it needs to focus on API data format and result value.\nSetup Protractor Angular Scenario Runner was a default componenet for AngulsrJS e2e testing called, but now has been deprecated. Official AngularJS guide suggests to use Protractor for test. It supports AngularJS, and Angular project.\nInstall dependencies first.\n{% highlight shell %} {% raw %} $ npm install -g protractor … $ protractor –version Version 5.1.2 $ webdriver-manager update [14:41:41] I/update - chromedriver: file exists /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/chromedriver_2.33.zip [14:41:41] I/update - chromedriver: unzipping chromedriver_2.33.zip [14:41:41] I/update - chromedriver: setting permissions to 0755 for /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/chromedriver_2.33 [14:41:41] I/update - chromedriver: chromedriver_2.33 up to date [14:41:41] I/update - selenium standalone: file exists /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/selenium-server-standalone-3.6.0.jar [14:41:41] I/update - selenium standalone: selenium-server-standalone-3.6.0.jar up to date [14:41:43] I/update - geckodriver: file exists /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/geckodriver-v0.19.0.tar.gz [14:41:43] I/update - geckodriver: unzipping geckodriver-v0.19.0.tar.gz [14:41:43] I/update - geckodriver: setting permissions to 0755 for /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/geckodriver-v0.19.0 [14:41:43] I/update - geckodriver: geckodriver-v0.19.0 up to date {% endraw %} {% endhighlight %}\nBecause it is e2e test, it needs target browser to run tests and Selenium will help this to do. After install protractor, you need to update webdriver-manager to get instance for running Selenium server. Now start Selenium server with webdriver-manager start. Your test spec will be tested above here.\n{% highlight shell %} {% raw %} $ webdriver-manager start [15:19:55] I/start - java -Dwebdriver.chrome.driver=/usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/chromedriver_2.33 -Dwebdriver.gecko.driver=/usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/geckodriver-v0.19.0 -jar /usr/local/lib/node_modules/protractor/node_modules/webdriver-manager/selenium/selenium-server-standalone-3.6.0.jar -port 4444 … 15:19:56.818:INFO:osjs.AbstractConnector:main: Started ServerConnector@5e853265{HTTP/1.1,[http/1.1]}{0.0.0.0:4444} 15:19:56.819:INFO:osjs.Server:main: Started @775ms 15:19:56.819 INFO - Selenium Server is up and running {% endraw %} {% endhighlight %}\nSetup configuration You need to write script for configuration and test specification. First create protractor.conf.js file.\n{% highlight javascript %} {% raw %} exports.config = { baseUrl: ‘http://localhost:8000/’, capabilities: { ‘browserName’: ‘chrome’ }, framework: ‘jasmine’, seleniumAddress: ‘http://localhost:4444/wd/hub’, specs: [‘protractor.spec.js’], allScriptsTimeout: 11000 }; {% endraw %} {% endhighlight %}\nParam baseUrl is to put in target url for test. I’m testing with local web server, so setup with localhost. You can define your test framework with framework, and selenium address would be http://localhost:4444/wd/hub. Define spec script file in specs, and you can include glob pattern like ‘spec/*_spec.js’. You can also setup running time limitation for each script with allScriptsTimeout. If you want to find more option, check here.\nWrite test spec {% highlight html %} {% raw %}\nEmail Password {{ vm.login_alert_msg.message }} LOG IN {% endraw %} {% endhighlight %} This is part of the template I will test on. This has simple login form, which has input for email/password, login button.\nNow write test spec file ‘protractor.spec.js’. {% highlight javascript %} {% raw %} describe(‘Protractor Demo Test’, function() { var userId; var password; var login;\nbeforeEach (function () { browser.get(browser.baseUrl) userId = element(by.model('vm.email')); password = element(by.model('vm.password')); login = element(by.buttonText('LOG IN')); }); it ('should have a title', function () { expect(browser.getTitle()).toEqual('Protractor Test'); }); ... }); {% endraw %} {% endhighlight %}\nThis is basic form of spec script, based on jasmine. beforeEach is same as setUp in usual test syntax which runs before tests in script begins, and it is for defining each test module.\nIn this script, browser loads target url from configuration file above(here it is localhost:8000) and setup elements(input, button, etc) for test. There are some ways to get these. If you want to get element by angular model, call with by.model. It will find elements defined with ng-model. You could also find using text in button with by.buttonText(I’m not sure this is a good way). This by is called locator and you could find more info here.\nNow in first test ‘should have a title’ is checking web page title. expect is something like assertTrue, which returns error when result is not true. You can get title with browser.getTitle and compare it with toEqual method.\n{% highlight javascript %} {% raw %} describe(‘Protractor Demo Test’, function() { …\nit ('should show error', function () { userId.sendKeys(''); // put in email password.sendKeys(''); // ...and wrong password login.click(); expect(element(by.css('.error-msg-area')).isPresent()).toBe(true); }); it ('should success login', function () { var EC = protractor.ExpectedConditions; userId.sendKeys('koni50@naver.com'); // put in email password.sendKeys('12341234'); // ...and correct password login.click(); browser.wait(EC.urlContains('/home'), 5000); }) }); {% endraw %} {% endhighlight %}\nThese part, checks bit more actions.\nTest case ‘should show error’ tests that error message is being shown when trying to log in with wrong password. Define action to put in email and password by using sendKeys at input element and to click login button. Error message has class error-msg-area so you can get this element with by.css, and check it has been visible with isPresent method. If you want to check detail result with expect, put toBe after and give expected value.\nLast one is for checking login succedded with correct email/password. There are several ways to check this, and I’ll define success with comparing url after login. In this test, url after login should include /home. After click event happens, make test to wait browser result for 5 seconds cause it needs time to complete login. protractor.ExpectedConditions is a set of built-in conditions you can get from browser, which related with Selenium server. Here urlContains has been used for comparing url. You could find more APIs in document.\nReference http://www.protractortest.org/ https://rocketeer.be/articles/testing-with-angular-js/ ","description":"","tags":["angularjs","test","protractor"],"title":"e2e test for AngularJS","uri":"/posts/2017-09-05-angularjs-e2e-test/"},{"categories":null,"content":"There are lots of modules for Android, and it helps you to develop your own app more easier, and faster. Most of them are open source and it is still being managed/updated by many contributer. You also could be one of them as contributor, or main producer by making your own module. By develop/manage your own project, you could receive variable feedbacks by lots of great engineers in world. It would improve your skill, learn new things, and it will be good for your career as developer. If you have good idea, go on!\nStart on Let’s create new Android project with Android Studio. Though we are focusing on module, it is better to create both app and module for making sample and tests. First create new project that has Empty Activity with Android Studio. Project will be look like this.\nNow go to File \u003e New \u003e New Module and select Android Library to create new module. If you done it right, project branch will has 2 package, for app and module.\nNow look on gradle script to import module mylibrary project to app. Find settings.gradle and see packages has included well, and add it if not. {% highlight shell %} {% raw %} include ‘:app’, ‘:mylibrary’ {% endraw %} {% endhighlight %}\nNow go to build.gradle file in app module file and add dependency for mylibrary module. {% highlight shell %} {% raw %} … dependencies { compile fileTree(dir: ’libs’, include: [’*.jar’]) androidTestCompile(‘com.android.support.test.espresso:espresso-core:2.2.2’, { exclude group: ‘com.android.support’, module: ‘support-annotations’ }) compile ‘com.android.support:appcompat-v7:26.+’ compile ‘com.android.support.constraint:constraint-layout:1.0.2’ testCompile ‘junit:junit:4.12’\ncompile project(path: ':mylibrary') // \u003c= Add this code } … {% endraw %} {% endhighlight %}\nNow your app module can import classes in mylibrary package.\nCreate simple module I’ll make a simple module which returns list of pair which include word and number. Word are the word included in text, and number is the appearance frequency of word in text. For example,\n“My name is John. Last name is Doe. Doe family is living here” =\u003e (My, 1), (name, 2), (is, 3), (John, 1), (Last, 1), (Doe, 2), (family, 1), (living, 1), (here, 1)\nNow I’ll put on 2 file into mylibrary package, Reducer.java and WordPair.java. Reducer.java includes logic to count word in text and returns as list of word and number. {% highlight java %} {% raw %} public class Reducer {\nprivate static Reducer reducerInstance; private Reducer() { } public static Reducer getInstance() { if (reducerInstance == null) { reducerInstance = new Reducer(); } return reducerInstance; } public ArrayList\u003cWordPair\u003e textToPair(String text, String delimeter) { String[] wordList = text.replaceAll(\"\\\\.\", \"\").split(delimeter); ArrayList\u003cWordPair\u003e pairList = new ArrayList\u003c\u003e(); boolean isIncluded = false; for (String word : wordList) { isIncluded = false; for (int i = 0; i \u003c pairList.size(); i++) { if (pairList.get(i).word.equals(word)) { // is already included. Give +1 isIncluded = true; int currentNum = pairList.get(i).number; pairList.get(i).number = currentNum + 1; break; } } if (!isIncluded) { // Add new pair if word not exists pairList.add(new WordPair(word, 1)); } } return pairList; } } {% endraw %} {% endhighlight %}\nWordPair.java is for put in word and count number. There can be some of getter and setter, but this time I’ll not think about it. {% highlight java %} {% raw %} public class WordPair { public String word; public int number;\npublic WordPair(String word, int number) { this.word = word; this.number = number; } } {% endraw %} {% endhighlight %}\nNow try to use this module in app. I’ll make a simple Activity that shows the result of example above. [MainActivity.java] {% highlight java %} {% raw %} public class MainActivity extends AppCompatActivity {\n@Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); Reducer reducer = Reducer.getInstance(); ArrayList\u003cWordPair\u003e parsedText = reducer.textToPair(\"My name is John. Last name is Doe. Doe family is living here\", \" \"); StringBuilder sBuilder = new StringBuilder(); TextView parsed = (TextView) findViewById(R.id.text_pair); for (WordPair wordPair : parsedText) { sBuilder.append(\"(\" + wordPair.word + \":\" + wordPair.number + \")\\n\"); } parsed.setText(sBuilder); } } {% endraw %} {% endhighlight %}\n[activity_main.xml] {% highlight xml %} {% raw %}\n\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003candroid.support.constraint.ConstraintLayout xmlns:android=“http://schemas.android.com/apk/res/android\" xmlns:app=“http://schemas.android.com/apk/res-auto\" xmlns:tools=“http://schemas.android.com/tools\" android:layout_width=“match_parent” android:layout_height=“match_parent” tools:context=“io.kination.myapplication.MainActivity”\u003e\n\u003cTextView android:id=\"@+id/text_pair\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" app:layout_constraintBottom_toBottomOf=\"parent\" app:layout_constraintLeft_toLeftOf=\"parent\" app:layout_constraintRight_toRightOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\" /\u003e \u003c/android.support.constraint.ConstraintLayout\u003e {% endraw %} {% endhighlight %}\nIt will show parsed result in empty page. Now we saw module is working fine on main project. I’ll go on some more with this next time.\nReference https://developer.android.com ","description":"","tags":["android","library"],"title":"Create own module project - Android","uri":"/posts/2017-08-11-android-make-lib/"},{"categories":null,"content":"‘Web scraping’ is a logic to get web page data as HTML format. With this information, not only could get text/image data inside of target page, we could also find out which tag has been used and which link is been included in. When you need lots of data for research in your system, this is one of the common way to get data.\nUse parser But not like CSV or Excel sheet, raw HTML is pretty rough and disordered data.\n{% highlight python %} {% raw %} import requests\nurl = ‘http://bleacherreport.com/' TIMEOUT_SECONDS = 30\nresponse = requests.get(url, timeout=30) {% endraw %} {% endhighlight %}\nThis is the case of getting raw HTML data from Bleacher Report. It will return data like this.\n{% highlight HTML %} {% raw %}\n... {% endraw %} {% endhighlight %} It is inconvenient for workers to find target data from here. You need a process of organizing before going further. Maybe you could make a parser yourself, but that’s not an effective way. Python have some great modules for this, and I will use one of this named BeautifulSoap.\nBefore going on, install it via python package manager with pip install beautifulsoup4.\n{% highlight python %} {% raw %} import requests from bs4 import BeautifulSoup\nurl = ‘http://bleacherreport.com/' TIMEOUT_SECONDS = 30\nresponse = requests.get(url, timeout=30) bs_object = BeautifulSoup(response.text, “html.parser”) {% endraw %} {% endhighlight %}\nWith this code, raw HTML data has been converted to beautifulsoap object. Now you can get data with text or tag info.\n{% highlight python %} {% raw %}\n‘find’ method returns the first searched data, while ‘findAll’ method returns all finded data in HTML as list. this will find data which has ‘p’ tag p_tag_data = bs_object.find(‘p’) p_tag_data = bs_object.findAll(‘p’)\nfind exact text ‘NBA’ in HTML nba_text = bs_object.find(text=‘NBA’) nba_text = bs_object.findAll(text=‘NBA’)\nshow all data which includes text ‘NBA’ import re nba_text = bs_object.findAll(text=re.compile(‘NBA’))\nreturn all p tag data which includes word ‘fans’ fans_data = bs_object.findAll(‘p’, text=re.compile(‘fans’))\nfind link tag which includes ‘rel=“canonical”’ element bs_object.findAll(“link”, {“rel”: “canonical”}) {% endraw %} {% endhighlight %}\nWhy it needs to use browser There are a problem on process above, not in parser, but in HTML request process. If we just request data by http request method, it cannot get dynamically rendered part because they are not in HTML file before loading process. Here is some example for this case.\nThis is the main page of Samsung SDS official site.\nThis is the ordinary form of main page. Now let’s check how this will be look like after disabling javascript.\nMenu in top has been disappeared, because they are being rendered dynamically from javascript controller. And also, menu texts will not being scraped when you get this HTML page with http request process. So, to get all of these data, you need to get fully rendered result data. For getting it, it has to be rendered in some kind of ‘fake browser’, and that’s why we will use ‘headless browser’.\nThis is description of headless browser in wikipedia. {% highlight html %} {% raw %} A headless browser is a web browser without a graphical user interface.\nHeadless browsers provide automated control of a web page in an environment similar to popular web browsers, but are executed via a command-line interface or using network communication. {% endraw %} {% endhighlight %}\nScraping via headless browser To make scraper via headless browser, we need headless browser, and module to make this run in virtual. I will use PhantomJS here for headless browser, and will make it run with Selenium. You need to install these first.\nPhantomJS can be installed by downloading from main page, but can be installed with brew or npm. You can use one of 2 command below. {% highlight shell %} {% raw %} $ brew phantomjs $ npm -g install phantomjs-prebuilt {% endraw %} {% endhighlight %}\nTry make a class for scraper.\n{% highlight python %} {% raw %} from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nclass HBScraper():\ndef __init__(self): self.web_driver = webdriver.PhantomJS(desired_capabilities=dict(DesiredCapabilities.PHANTOMJS)) self.web_driver.set_window_size(960, 540) self.web_driver.set_page_load_timeout(60) self.web_driver.set_script_timeout(60) self.web_driver.delete_all_cookies() def scrape_page(self, page_url): try: self.web_driver.get(page_url) except: return None return self.web_driver.page_source {% endraw %} {% endhighlight %}\nHBScraper class is the class for getting ’loaded’ page data. It initiates browser setting like max loading time, window size, etc. on __init__ method. You can do scraping and get scraped result from browser with scrape_page.\nUse it like below. {% highlight python %} {% raw %} url = ‘http://bleacherreport.com/'\nhb_scraper = HBScraper() bs_object = BeautifulSoup(hb_scraper.scrape_page(url), “html.parser”) {% endraw %} {% endhighlight %}\nBecause it needs time for loading, scraping with headless browser needs more time to get result than just getting data with HTTP request. But to get exact data of target page, you will need to consider of using it.\nReference https://en.wikipedia.org/wiki/ http://phantomjs.org/ http://www.seleniumhq.org/ https://stackoverflow.com/ ","description":"","tags":["python","web","scraper"],"title":"Web scraping, and headless browser","uri":"/posts/2017-07-23-headless-python-scraping/"},{"categories":null,"content":"As front-end projects are being complecated, there are many process to think on working such as compressing, linting, transpiling, and more. These are not mandatory stuffs, but it helps managing your project, and upgrade its performance. Surely, there are several tools for this such as Grunt or Gulp, and you could set an option in Webpack to make these process. I’ll look on Gulp here.\nWhy Gulp? Gulp is a command line task runner based on Node.js. It runs defined tasks and manages processes automatically.\nThe function Grunt and Gulp offers are mostly same. It makes process which I mentioned above working during build process. There are lots of blogs which explained about comparison, but the big reason I choose Gulp in my project is, its configuration ‘seems more easy’.\nSettings in Grunt is based on JSON, while Gulp is using javascript. These are simple usage.\n——- Grunt ——- {% highlight javascript %} {% raw %} grunt.initConfig({ uglify: { my_target: { files: { ‘dest/output.min.js’: [‘src/input.js’] } } } }); {% endraw %} {% endhighlight %}\n——- Gulp ——- {% highlight javascript %} {% raw %} gulp.task(‘uglifyJs’, function () { return gulp.src(‘src/input.js’) .pipe(js_minify()) .pipe(gulp.dest(‘dest/output.min.js’)); }); {% endraw %} {% endhighlight %}\nIt seems okay while it has only few configuration to do, but the more settings being complecated, Grunt will have very unattractive form. Maybe this is subjective suggestion, but maybe this is the reason why I feel Gulp more comportable.\nGrunt focuses on configuration, while Gulp focuses on code\nBecause I’m used on looking code more than config file, it could explain my suggestion. There are some more comparing point such as performance, supporting 3rd party module, and more. But what I think is, both tool are good enough, so it is developer’s choice, and I select Gulp.\nSetup You can install gulp package through npm. {% highlight shell %} {% raw %} $ npm install -g gulp-cli // install gulp cli tool $ cd /PATH/TO/YOUR/PROJECT $ npm install gulp –save-dev // install gulp in development dependencies {% endraw %} {% endhighlight %}\nAnd create a gulpfile.js in root directory of your project. {% highlight javascript %} {% raw %} var gulp = require(‘gulp’);\ngulp.task(‘default’, function() { // place code for your default task here }); {% endraw %} {% endhighlight %}\nThis is a basic form of gulpfile. Now try {% highlight shell %} {% raw %} $ gulp [12:15:04] Using gulpfile ~/PATH/TO/YOUR/PROJECT/gulpfile.js [12:15:04] Starting ‘default’… [12:15:04] Finished ‘default’ after 123 μs {% endraw %} {% endhighlight %}\nCommand gulp will run task default in gulp file. If you want to use other task, you could make it run by using gulp TASK-NAME. Now you are ready. It does nothing because you didn’t setup any process in here. You will now add gulp submodules in here by your purpose.\nCompress Compressing(or minifing) html/js files in your project improves performance, by reducing file load time. Let’s make this work first. First you need to add minifier modules. {% highlight shell %} {% raw %} $ npm install gulp-uglify –save-dev $ npm install gulp-minify-html –save-dev {% endraw %} {% endhighlight %}\nand add these in your gulp file. {% highlight javascript %} {% raw %} var minify_js = require(‘gulp-uglify’); var minify_html = require(‘gulp-minify-html’);\ngulp.task(‘minifyJs’, function () { return gulp.src(‘JS-PATH/**/*.js’) .pipe(minify_js()) .pipe(gulp.dest(‘COMPRESSED-JS-PATH’)); });\ngulp.task(‘minifyHtml’, function () { return gulp.src(‘HTML-PATH/**/*.html’) .pipe(minify_html()) .pipe(gulp.dest(‘COMPRESSED-HTML-PATH’)); }); {% endraw %} {% endhighlight %}\nIn gulp, it uses Stream logic with pipe method for passing data. It makes multiple tasks being work together, and improve performance of gulp.\nBy code above, task ‘minifyJs’ and ‘minifyHtml’ has been added for compressing .js file and .html files. It sends all files in gulp.src to minifing module, and throws result files to locations in gulp.dest.\n{% highlight shell %} {% raw %} $ gulp minifyJs [12:30:26] Using gulpfile ~/PATH/TO/YOUR/PROJECT/gulpfile.js [12:30:26] Starting ‘minifyJs’… [12:30:30] Finished ‘minifyJs’ after 4.25 s $ gulp minifyHtml [12:30:49] Using gulpfile ~/PATH/TO/YOUR/PROJECT/gulpfile.js [12:30:49] Starting ‘minifyHtml’… [12:30:49] Finished ‘minifyHtml’ after 546 ms $ {% endraw %} {% endhighlight %}\nIf you want to make it all running by single command, add new task like this. {% highlight javascript %} {% raw %} gulp.task(‘release’, [‘minifyJs’, ‘minifyHtml’]); {% endraw %} {% endhighlight %}\nTry this. {% highlight shell %} {% raw %} $ gulp release [20:55:29] Starting ‘minifyJs’… [20:55:29] Starting ‘minifyHtml’… [20:55:33] Finished ‘minifyJs’ after 4.19 s [20:55:34] Finished ‘minifyHtml’ after 4.5 s [20:55:34] Starting ‘release’… [20:55:34] Finished ‘release’ after 277 μs $ {% endraw %} {% endhighlight %}\nThere are several useful options in gulp-uglify module. For example, if you want to get rid of all console.log stuff in minify process, add parameter in module like this. {% highlight javascript %} {% raw %} gulp.task(‘minifyJs’, function () { return gulp.src(‘JS-PATH/**/*.js’) .pipe(minify_js({ compress: { drop_console: true } })) .pipe(gulp.dest(‘COMPRESSED-JS-PATH’)); }); {% endraw %} {% endhighlight %}\nLinting ‘Linting’ in modern programming language usually means keeping your code quality(there are other meaning in wikipedia though…), so make code developer to follow the writing standard and remove potential errors. Especially javascirpt, because it is not strictable, it is hard to keep code clean, and that’s why linting is important here.\nThere are several options for linting for javascript. I’ll select eslint here. Surely, there are a plugin for gulp. Install it first…\n{% highlight shell %} {% raw %} $ npm install –save-dev gulp-eslint $ npm install –save-dev eslint-plugin-angular # only need in angularjs project {% endraw %} {% endhighlight %}\nand add task. {% highlight javascript %} {% raw %} var eslint = require(‘gulp-eslint’); gulp.task(’esLint’, function () { return gulp.src(‘JS-PATH/**/*.js’) .pipe(eslint()) .pipe(eslint.format()); }); {% endraw %} {% endhighlight %}\neslint() module checks lint in target directory, and shows the result by eslint.format(). You could see the result like this. {% highlight shell %} {% raw %} $ gulp esLint [14:17:42] Using gulpfile ~/PATH/TO/YOUR/PROJECT/gulpfile.js [14:17:42] Starting ’esLint’… … /JS/PATH/xxx.js 9:20 error Strings must use singlequote quotes 9:61 error ‘$filter’ is defined but never used no-unused-vars 68:1 error Expected indentation of 20 spaces but found 0 indent 79:34 error Strings must use singlequote quotes 79:56 error ‘value’ is defined but never used no-unused-vars 86:14 error Missing semicolon semi … ✖ 167 problems (162 errors, 5 warnings) 107 errors, 0 warnings potentially fixable with the --fix option. $ {% endraw %} {% endhighlight %}\nIf you want to add some function which only works when there is no lint error, add pipe for failAfterError. This will show ‘success!’ alert only when lint process is passed for all files in source.\n{% highlight javascript %} {% raw %} var eslint = require(‘gulp-eslint’); gulp.task(’esLint’, function () { return gulp.src(‘JS-PATH/**/*.js’) .pipe(eslint()) .pipe(eslint.format()) .pipe(eslint.failAfterError());\ngulp.task(’lintResult’, [’testLint’], function () { alert(‘success!’); }); {% endraw %} {% endhighlight %}\nRemove cache When once static files being load, browser will keep it as a cache data for a pretty long time. So though you update your code into server, your browser could show you the cached templates instead of updated ones. Most of browser will do it like this, for performance purpose. It can make loading quickly, but could be a big problem on your service. Add custom parameter in request query like this is the usual way to solve the problem. In this case, parameter ‘ver=1.0’ is attached in it.\nhttp://PATH/OF/FILE/abcd.js?ver=1.0 This is one of example. You could attach random code, or timestamp instead of version. But you can’t put on this in every path every time, so let’s make it done in gulp process.\n{% highlight shell %} {% raw %} $ npm install –save-dev gulp-cache-bust {% endraw %} {% endhighlight %}\nI will add this process inside minify task. {% highlight javascript %} {% raw %} var cache_bust = require(‘gulp-cache-bust’);\ngulp.task(‘minifyJs’, function () { return gulp.src(‘JS-PATH/**/*.js’) .pipe(minify_js()) .pipe(cache_bust({ type: ’timestamp’ })) .pipe(gulp.dest(‘COMPRESSED-JS-PATH’)); });\ngulp.task(‘minifyHtml’, function () { return gulp.src(‘HTML-PATH/**/*.html’) .pipe(minify_html()) .pipe(cache_bust({ type: ’timestamp’ })) .pipe(gulp.dest(‘COMPRESSED-HTML-PATH’)); });\n{% endraw %} {% endhighlight %} Very simple! Just add gulp-cache-bust module with parameter. This is the case using timestamp, but you could use other option such as MD5 data, or just constant value like version.\nAnd… The function I commented here is really just a part of gulp environment. There are tons of more functions you can do inside gulp process which will help your project. You could find it here.\nResources http://gulpjs.com/ https://www.keycdn.com/blog/gulp-vs-grunt/ https://medium.com/@preslavrachev/gulp-vs-grunt-why-one-why-the-other-f5d3b398edc4 https://ics.media/entry/9199 ","description":"","tags":["frontend","javascript","gulp"],"title":"Construct front-end project with gulp","uri":"/posts/2017-07-04-construct-frontend-with-gulp/"},{"categories":null,"content":"One of the trend on developer is AI and machine learing, and I think most of engineer, like me, are trying to going into this field. To start on machine learning, we need a knowledge about data analysis, and for that we have to remind statistics, which we suffered when we are a student.\nLinear Regression? Linear Regression is one way to find relation between variable, with given dataset. This theory has been more than a hundread years, but it is still broadly used and printed in most of machine learning text books.\nLinear Regression tries to make a linear line like y = Ax + B which closely match values in data. It is the way to figure out A and B value from predictor value x and target value y. There also can be more than one variable in predict value, it is called as ‘Multiple Linear Regression’, and then it will have form like y = A1x + A2x + A3x + A4x + ... + B. Similarly, first case with single depandant value are being called as ‘Single Linear Regression’. I’ll focus looking in this case on this post.\nBefore starting I’ll use numpy, but try not to use other python modules for machine learning this time, such as pandas or scikit-learn, to look on detail process of this. I prepared simple dataset for test. Here are information about ‘age’, ‘blood fat’ of 25 anonymous people. You can find this, and some other dataset in http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html\nImplementation Let’s think about the first case, with single dependant variable. I’ll look on how age is related with blood fat.\n{% highlight python %} {% raw %} import csv import matplotlib.pyplot as plt\ncsv_file = open(‘data.csv’, ‘r’) reader = csv.reader(csv_file) x = [] # age y = [] # blood fat\nfor value in reader: x.append(value[1]) y.append(value[2])\nplt.scatter(x, y, color = “b”, marker = “*”, s = 100) plt.show() {% endraw %} {% endhighlight %}\nYou could find result like below. Now, we need to think about how to get proper point to make a linear line to cover these points, which called regression line. The best fit line is the line for which the sum of the distances between each of data points and the line is as small as possible. For this, I’ll make an implementation of Least square method.\nThis is the implementation for this logic. I used numpy for this to make calculation in list more simple.\n{% highlight python %} {% raw %}\ninput ‘x_arr’, ‘y_arr’ is np.array def least_square_method(x_arr, y_arr): x_mean, y_mean = np.mean(x_vec), np.mean(y_vec) set_size = np.size(x_vec)\nnumerator = np.sum(y_vec*x_vec - set_size*y_mean*x_mean) denominator = np.sum(x_vec*x_vec - set_size*x_mean*x_mean) b_1 = numerator / denominator b_0 = y_mean - b_1*x_mean return(b_0, b_1) {% endraw %} {% endhighlight %}\nNow let’s look on main code.\n{% highlight python %} {% raw %} def main(): x_vector = np.array(x) y_vector = np.array(y) b = least_square_method(x_vector, y_vector) y_prediction = b[0] + b[1] * x_vector\nplt.scatter(x_vector, y_vector, color = \"b\", marker = \"*\", s = 100) plt.plot(x_vector, y_prediction, color = \"g\") plt.show() {% endraw %} {% endhighlight %}\nLet’s look on the result.\nNext time I’ll look on Multiple Linear Regression, and other theories about regression.\n","description":"","tags":["python","statistic","machine learning"],"title":"Single Linear regression","uri":"/posts/2017-06-11-single-linear-regression/"},{"categories":null,"content":"During Android being update A to N(and soon O will be updated), messaging library has been updated as C2DM -\u003e GCM -\u003e GCM in google-play-service -\u003e FCM(Firebase cloud messaging). The names are all different, but doing same thing - send push message to target device. Actually there are plenty of updates and functions in Firebase, but I’ll focus on messaging implementation here.\nRemove legacy codes Push messaging of current project I’m working on is implemented with old version of GCM, which is the one before included inside google-play-service module. It has been deprecated more than 4 years, and there are lots of bugs which will never be fixed.\nLet’s look on legacy stuffs. If your project is based on old GCM, you will find code like below. {% highlight java %} {% raw %} public class MyGcmIntentService extends GCMBaseIntentService {\n@Override protected void onRegistered(Context context, String registrationId) { // Something needs to be done after gcm registered } @Override protected void onUnregistered(Context context, String registrationId) { // Something needs to be done after gcm unregistered } @Override protected void onMessage(Context context, Intent intent) { // Something needs to be done when message received } ... } {% endraw %} {% endhighlight %} This is the receive handler for push message. If there is something to do when GCM is registered or message is received, it will be handled inside here.\nAnd go on to AndroidManifest.xml, there will be some lines for GCM permission and part for registering MyGcmIntentService class. {% highlight xml %} {% raw %} … {% endraw %} {% endhighlight %}\nThese parts are now useless for new Firebase messaging. Permissions for this will be added automatically in FCM by library. Also, there will be codes for GCM registration when app starts. They don’t need anymore either. {% highlight java %} {% raw %} … GCMRegistrar.checkDevice(context); GCMRegistrar.checkManifest(context); GCMRegistrar.register(context, “sender-id”); … {% endraw %} {% endhighlight %}\nSetup for FCM implementation To use Firebase modules, you need to setup Firebase tool in your Android Studio. Go to Tools \u003e Firebase and you will see Assistant menu like this.\nSelect Set up Firebase Cloud Messaging \u003e 1. Connect your app to Firebase. You can make new project, or call recent project to register. After registration complete, you will see page like this in your browser.\nIf you clear this part, you will find that file google-services.json being added in your project’s /app folder. This file includes the information which needs for using Firebase service in your app. You don’t need for registration process code in your app as before. It will be done automatically by library.\nNow open project build.gradle file and add line for google service\n{% highlight groovy %} {% raw %} buildscript { repositories { jcenter() } dependencies { … classpath ‘com.google.gms:google-services:3.0.0’ //Add this line } } {% endraw %} {% endhighlight %}\nand open app build.gradle file\n{% highlight groovy %} {% raw %} … dependencies { … compile ‘com.google.firebase:firebase-messaging:9.0.0’ //Add this line }\napply plugin: ‘com.google.gms.google-services’ //Add this line {% endraw %} {% endhighlight %}\nIf you did not go on Connecting app process, you will see error for applying com.google.gms.google-services plugin while on gradle sync.\nYou could see Tools \u003e Firebase \u003e Set up Firebase Cloud Messaging change like this if you follow up correctly. Now setup is all cleared.\nImplements for FCM In FCM, you need services for handling registeration, and receiver. In old GCM it is handled in single service which extended GCMBaseIntentService, but here it is divided with FirebaseInstanceIdService and FirebaseMessagingService. The first one are being called for handle create/update of registration token, while second one are handling push messages.\nFCMService.java {% highlight java %} {% raw %} public class FCMService extends FirebaseMessagingService {\n@Override public void onMessageReceived(RemoteMessage remoteMessage) { // Something needs to be done when message received } } {% endraw %} {% endhighlight %}\nYou can call override method onMessageReceived in this service to add code that needs to be done when message received. In GCMBaseIntentService.onMessage method from old GCM, there were Intent param to get push message data. This has been changed to use RemoteMessage class. For example…\n{% highlight java %} {% raw %} // Code in GCMBaseIntentService @Override protected void onMessage(Context appCtx, Intent intent) { String data = intent.getStringExtra(“push_key”); } {% endraw %} {% endhighlight %}\nyou can have same data value here.\n{% highlight java %} {% raw %} // Code in FirebaseMessagingService @Override public void onMessageReceived(RemoteMessage remoteMessage) { String data = remoteMessage.getData().get(“push_key”); } {% endraw %} {% endhighlight %}\nAnd this is the service to get token information. You can call override method onTokenRefresh if something needs to be done when token refreshed.\nFBIdService.java {% highlight java %} {% raw %} public class FBIdService extends FirebaseInstanceIdService {\n@Override public void onTokenRefresh() { // Something needs to be done when token refreshed } } {% endraw %} {% endhighlight %}\nDon’t forget to register these services in manifest file. Each of them needs intent filter action like below.\n{% highlight xml %} {% raw %} {% endraw %} {% endhighlight %}\nNow it is done. Because you don’t need register process, it will working without kinds of initiating stuffs. There are some more things like sending topic message and upstream message, but this will be done for migrating old one.\nWhile working on this, I found lots of great stuffs in Firebase. Hope to get on more in this later.\n","description":"How-to move legacy GCM to Firebase cloud messaging in Android client","tags":["android","firebase"],"title":"Going on to Firebase cloud messaging(FCM) in Android","uri":"/posts/2017-04-11-going-firebase-messaging/"},{"categories":null,"content":"Ibis is a platform(or toolbox) implemented by Cloudera, which helps connecting remote storage and local python codes. The newest version is 0.8, and currently supports Hadoop components(HDFS, Impala, Kudu) and SQL DBs(SQLite, PostgreSQL). Main purpose of this is to simplify analytical workflows with remote data. For this goal, there are lots of useful APIs for data analysis. If you are friendly with python Pandas, you could find familiar expression with it.\nExpression Before working with remote stuffs, you can test with local SQLite db to get the hang of it. First, download data file Crunchbase from here, and make python code. {% highlight python %} {% raw %} import ibis\nconnect = ibis.sqlite.connect(‘crunchbase.db’) tables = connect.list_tables() {% endraw %} {% endhighlight %}\nYou can open SQLite DB to use on Ibis simply by sqlite.connect. Call list_tables() method to list up the tables which are in DB file. There would be ['acquisitions', 'companies', 'investments', 'rounds'] in sample.\nLet’s see what is in table companies. {% highlight python %} {% raw %} companies = con.table(‘companies’) companies.info() {% endraw %} {% endhighlight %}\nYou will see something like this. {% highlight shell %} {% raw %} $ Table rows: 54292\nColumn Type Non-null #\npermalink string 51166 name string 51166 homepage_url string 47583 category_list string 47826 market string 46416 funding_total_usd float 42237 … {% endraw %} {% endhighlight %}\nIf you want to see what is in funding_total_usd column, go on like this.\n{% highlight python %} {% raw %} companies = con.table(‘companies’) companies.funding_total_usd.value_counts() {% endraw %} {% endhighlight %}\n{% highlight shell %} {% raw %} funding_total_usd count 0 NaN 12055 1 1.0 1 2 2.0 1 3 5.0 1 … 9998 10024049.0 1 9999 10025000.0 3\n[10000 rows x 2 columns] {% endraw %} {% endhighlight %}\nIf you just want to see mean value, it also can be known simply.\n{% highlight python %} {% raw %} companies = con.table(‘companies’) mean_value = companies.funding_total_usd.mean() #16198405.416388474 {% endraw %} {% endhighlight %}\nI’ll try bucket this time, which compute a discrete binning with given numeric array. {% highlight python %} {% raw %} companies = con.table(‘companies’) funding_buckets = [0, 10, 1000, 1000000, 10000000, 50000000]\nbucket = (companies .funding_total_usd .bucket(funding_buckets, include_over=True)) bucket.value_counts() {% endraw %} {% endhighlight %}\nIt will return each sum value of each bucket. {% highlight shell %} {% raw %} unnamed count 0 NaN 12055 1 0.0 4 2 1.0 42 3 2.0 15919 4 3.0 15754 5 4.0 7921 6 5.0 2597 {% endraw %} {% endhighlight %}\nGoing on more, you can use group_by and aggregate to compute group summaries. group_by creates an intermediate grouped table expression, and aggregate aggregates table with a given set of reductions, grouping expressions, and post-aggregation filters. {% highlight python %} {% raw %} metrics = (companies.group_by(bucket.name(‘bucket’)) .aggregate(count=companies.count(), total_funding=companies.funding_total_usd.sum()))\nprint(metrics) {% endraw %} {% endhighlight %}\nOut: {% highlight shell %} {% raw %} bucket count total_funding 0 NaN 12055 NaN 1 0.0 4 1.700000e+01 2 1.0 42 1.518000e+04 3 2.0 15919 4.505161e+09 4 3.0 15754 5.712283e+10 5 4.0 7921 1.765166e+11 6 5.0 2597 4.460274e+11 {% endraw %} {% endhighlight %}\nThis is just a part of Ibis framework. You could find more features which will make you more productive in here.\nPrepare VB access For testing remote access, Ibis prepared demo VirtualBox image to test. You can download ova file here. It contains Impala DB and demo table to test.\nNow you need to make it connective with local environment. If you are first in VB, you need to setup for this. Let’s check network configuration in VB image.\n{% highlight shell %} {% raw %} $ ifconfig eth0 | grep Mask inet addr:10.0.2.15 Bcase:10.0.2.255 Mask:255.255.255.0 {% endraw %} {% endhighlight %}\nGo to VB setting -\u003e network tab and check IPv4 Address.\nNow right click in ibis-demo image and press setting. Go to network -\u003e Adapter 1 and press Port Forwarding\nPress plus button in right of the list, and create new one. Input Host IP as IPv4 Address of VB, and Guest IP with inet address of VB image. Give port number with 22.\nCheck with ping to look if it connected well.\n{% highlight shell %} {% raw %} $ ping 192.168.56.1 PING 192.168.56.1 (192.168.56.1): 56 data bytes 64 bytes from 192.168.56.1: icmp_seq=0 ttl=64 time=0.070 ms 64 bytes from 192.168.56.1: icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from 192.168.56.1: icmp_seq=2 ttl=64 time=0.081 ms 64 bytes from 192.168.56.1: icmp_seq=3 ttl=64 time=0.130 ms ^C — 192.168.56.1 ping statistics — 4 packets transmitted, 4 packets received, 0.0% packet loss {% endraw %} {% endhighlight %}\nIt’s done.\nRemote access Now let’s make a code for remote access.\n{% highlight python %} {% raw %} import ibis import os\nwebhdfs_host = ‘192.168.56.1’ webhdfs_port = 22\nibis.options.interactive = True\n{% endraw %} {% endhighlight %}\nYou can setup global configuration with ibis.option. To make expressions to be executed immediately when printed to the console, setup ibis.option.interactive as True. {% highlight python %} {% raw %} hdfs_port = os.environ.get(‘IBIS_WEBHDFS_PORT’, 50070) hdfs = ibis.hdfs_connect(host=webhdfs_host, port=hdfs_port) con = ibis.impala.connect(host=webhdfs_host, database=‘ibis_testing’, hdfs_client=hdfs)\n{% endraw %} {% endhighlight %}\nNow you can open Impala DB in remote VB. Try to get table data from here. {% highlight python %} {% raw %}\ntables = con.list_tables() print(tables) table = con.table(‘functional_alltypes’) print(table) {% endraw %} {% endhighlight %}\n{% highlight shell %} {% raw %} [‘functional_alltypes’, ’tpch_ctas_cancel’, ’tpch_customer’, ’tpch_lineitem’, ’tpch_nation’, ’tpch_orders’, ’tpch_part’, ’tpch_partsupp’, ’tpch_region’, ’tpch_region_avro’, ’tpch_supplier’] id bool_col tinyint_col smallint_col int_col bigint_col 0 5460 True 0 0 0 0\n1 5461 False 1 1 1 10\n2 5462 True 2 2 2 20\n… {% endraw %} {% endhighlight %}\nNow you and I are at the starting point of this platform. It has not been spread out wide yet, but it seems availability for data engineer as it can make research directly to remote database with local codes, and offers intuitive modules for research. It seems it worth to look on more deeply for now. Hope you could find more possibility in here.\n","description":"What is, and how to go on with Ibis","tags":["python","data engineering"],"title":"Looking on Ibis, python data analysis framework for Hadoop components","uri":"/posts/2017-03-01-getting-start-ibis/"},{"categories":null,"content":"MapReduce is a programming model for processing large dataset. Theory has been introduced more than 10 years ago, and it has been widely known to developer by Hadoop. This is the heart of Hadoop, and now it is being used in many distribute systems.\nThough it has been a core of many complex system, the principle is very simple. It is a compound of map and reduce, which is being used in many data structure. In MapReduce, map receives dataset and convert each to key/value form with same logic, and reduce merge outputs to smallser dataset from map.\nExample of MapReduce Most guide documents are showing these with ‘word counting inside vocabulary’ example. I’ll test with “Welcome to blog. This is blog. Welcome to mapreduce.”\nWhen vocab set comes in, it parses each word and make all as key/value form, which all set has value = 1. And then sort all key/value data with key and add values. Because there are two ‘Welcome’, ’to’, ‘blog’ word inside test vocab, value for these key are added.\n{% highlight python %} {% raw %} def mr_map(file_data): mapped = [] words = file_data.split() for word in words: mapped.append({word: 1}) return mapped\ndef mr_reduce(mapped): reduced = {} for item in mapped: key = item.keys()[0] if key in reduced: reduced[key] += 1 else: reduced[key] = 1 return reduced\nfile_object = open(‘vocab.txt’, ‘r’) line = file_object.read().strip() print(‘original data:\\n’ + line)\nmap_data = mr_map(line) print(‘mapped data:\\n’ + str(map_data))\nreduced_data = mr_reduce(map_data) print(‘reduced data:\\n’ + str(reduced_data)) {% endraw %} {% endhighlight %}\n[vocab.txt]\nWelcome to blog This is blog Welcome to mapreduce In this code, ‘mr_map’ functino makes all word in vocab to key/value which key is word and value is 1, so it makes data like this.\n[{'Welcome': 1}, {'to': 1}, {'blog': 1}, {'This': 1}, {'is': 1}, {'blog': 1}, {'Welcome': 1}, {'to': 1}, {'mapreduce': 1}]\nAnd reduce process, it sorts each data by key and add the value, which ‘mr_reduce’ does in this code. This is the result.\n{'This': 1, 'is': 1, 'Welcome': 2, 'mapreduce': 1, 'blog': 2, 'to': 2}\nActually, there are more simple and well-performed modules in Collection and dict on Python to get this result. This is just for showing how data in MapReduce ins changing on.\nDistribute file system In distribute system, MapReduce divides number of operations on the set of data to each nodes in system. Each node reports back to master system(or node) in certain duration to update current status and check if nodes are working well or not. When any node do not send report, master will define it as dead-server and will send job to other node which works well. Similary, if it needs scale-up, it can be done by adding more nodes and connect with master. That is to say, it makes architecture more stable and becomes manageable.\n","description":"How MapReduce is working on","tags":["python","mapreduce"],"title":"Map, Reduce, and MapReduce","uri":"/posts/2017-01-10-map-reduce/"},{"categories":null,"content":"Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. It has been more than 10 years since created, and released its 1.10 version few months ago. During that time it proved how stable it is with lots of massive projects(such as Instagram, Pinterest, and more) which are based on it. Project I’m currently working on are based on this too. I have small experience on RoR and NodeJS too. Each of them has pros and cons, and are already being used on services in world. But one thing I think the strong point of Django is, it is written in Python. It means it can use variable python packages/libraries. This is, I think, it is more better point than other frameworks.\nThis is simple note about creating API server based on Django. Open the terminal and scaffold project.\n{% highlight bash %} {% raw %} $ django-admin startproject api_server $ cd api_server $ pip install django … $ python manage.py migrate Operations to perform: Apply all migrations: admin, contenttypes, sessions, auth Running migrations: Rendering model states… DONE Applying contenttypes.0001_initial… OK Applying auth.0001_initial… OK Applying admin.0001_initial… OK Applying admin.0002_logentry_remove_auto_add… OK Applying contenttypes.0002_remove_content_type_name… OK Applying auth.0002_alter_permission_name_max_length… OK Applying auth.0003_alter_user_email_max_length… OK Applying auth.0004_alter_user_username_opts… OK Applying auth.0005_alter_user_last_login_null… OK Applying auth.0006_require_contenttypes_0002… OK Applying auth.0007_alter_validators_add_error_messages… OK Applying sessions.0001_initial… OK $ python manage.py startapp apis $ python manage.py runserver Django version 1.9.7, using settings ‘api_server.settings’ Starting development server at http://127.0.0.1:8000/ Quit the server with CONTROL-C. {% endraw %} {% endhighlight %}\nThis is the basic way to build, start default django web server. Your project directory tree will be like this.\nNow we will use Django REST framework to implement API server. We can also make without this, but it has lots of useful features for serialization, authentication, and more. Use PyPI for install these.\n{% highlight bash %} {% raw %} $ pip install djangorestframework # Install REST framework $ pip install markdown # Markdown support for the browsable API. $ pip install django-filter # Filtering support {% endraw %} {% endhighlight %}\nLet’s add installed packages in project. Find settings.py file and put 2 line for this.\n{% highlight python %} {% raw %} … INSTALLED_APPS = [ ‘django.contrib.admin’, ‘django.contrib.auth’, ‘django.contrib.contenttypes’, ‘django.contrib.sessions’, ‘django.contrib.messages’, ‘django.contrib.staticfiles’,\n#Add these for REST framework 'rest_framework' ] … {% endraw %} {% endhighlight %}\nNow our project is prepared. Before going on, I’ll make super user account. This will be used for authentication.\n{% highlight bash %} {% raw %} $ python manage.py createsuperuser Username (leave blank to use ‘…’): admin Email address: admin@gmail.com Password: Password (again): Superuser created successfully. {% endraw %} {% endhighlight %}\nThere are several things to keep in mind when implementing web API service. Django REST framework helps us to concern more less on details and make us to focus on functions which API offers. For example, we could use ‘Serializer’ in this framework to serialize/deserialize API more simple. First, let’s try to get account list. There would be 1 account we just created with ‘createsuperuser’ command. Write below in ‘apis/serializer.py’\n{% highlight python %} {% raw %} from django.contrib.auth.models import User, Group from rest_framework import serializers\nclass UserSerializer(serializers.ModelSerializer): class Meta: model = User fields = (‘username’, ’email’, ‘groups’)\n{% endraw %} {% endhighlight %}\nThis is serializer for model in user list. It serialize fields ‘username’,’email’, ‘group’ column in ‘User’ database. Then go on to ‘apis/views.py’\n{% highlight python %} {% raw %} from django.contrib.auth.models import User from rest_framework import viewsets from apis.serializers import UserSerializer\nclass UserViewSet(viewsets.ModelViewSet):\nqueryset = User.objects.all().order_by('-date_joined') serializer_class = UserSerializer {% endraw %} {% endhighlight %}\n‘UserViewSet’ inherits ‘ModelViewSet’ in rest framework. It is for representing values in model. As in code, by defining ‘serializer_class’, it will serialize ‘User’ objects. Now let’s run this server and see how it works. Open another terminal and call API. Now we need to define API url for get user list. This will be defined in ‘apis/urls.py’\n{% highlight python %} {% raw %} from django.conf.urls import url, include from rest_framework.routers import DefaultRouter from apis.views import UserViewSet\nrouter = DefaultRouter() router.register(r’users’, UserViewSet)\nurlpatterns = [ url(r’^’, include(router.urls)), ] {% endraw %} {% endhighlight %}\nRouter can be defined with ‘DefaultRouter’. By register viewsets with name ‘users’, this will be linked in ‘http://your.api.server.ip/users/'. Also, for secure service, we need to make this API only be accessible to admin user. Add some lines in ‘settings.py’.\n{% highlight python %} {% raw %} REST_FRAMEWORK = { ‘DEFAULT_PERMISSION_CLASSES’: [ ‘rest_framework.permissions.IsAdminUser’, ] } {% endraw %} {% endhighlight %}\nNow, try to call this API.\n{% highlight bash %} {% raw %} $ curl -H ‘Accept: application/json; indent=4’ -u [username]:[password] http://127.0.0.1:8000/users/ { “count”: 1, “next”: null, “previous”: null, “results”: [ { “username”: “admin”, “email”: “admin@gmail.com”, “groups”: [] } ] } $ curl -H ‘Accept: application/json; indent=4’ http://127.0.0.1:8000/users/ { “detail”: “Authentication credentials were not provided.” } {% endraw %} {% endhighlight %}\nBecause we defined in serializer, we can see ‘groups’ key in result though it is empty. Also it returns no result when no account/password has been put in.\n","description":"Make good, simple API server based on Django","tags":["python","django","rest"],"title":"To make good API server","uri":"/posts/2016-10-11-django-api/"},{"categories":null,"content":"Now, in web, d3 \u0026 nvd3 has been the first option for graph rendering(maybe not…but I think so) in open source projects. And one of them, line chart, are being used to show the movement of some value by timeline.\nBut when using on this, there are some parts, that makes this tool untidy. Codes are worked with angular-nvd3(1.0.8).\n{% highlight html %} {% raw %} … …\n{% endraw %} {% endhighlight %}\nThis is simple example for daily line chart, and here is the result with 2 datasets.\nIt looks clean, but there are some problem.\nLike this, grid on graph(which are being called tick) and guideline are not matching. The reason is that in nvd3, distance of each grid line are changing dynamically, depends on width of rendering area. In my case, input comes as unix timestamp value, and reformed by d3.time.format(). Distance of timestamp value are always fixed(in my case, it is 86400000) but grids are not.\nTo avoid this, you need to customize tick value.\n{% highlight javascript %} {% raw %} … vm.graph_option = { chart: { type: ’lineChart’, height: 300,\nxAxis: { axisLabel: 'Date', tickFormat: function(d) { return d3.time.format('%m/%d')(new Date(d)); }, tickValues: function(values) { return _.map(values[0].values, function(v) { return new Date(v.graph_date); }); }, rotateLabels: -45, showMaxMin: true }, interpolate: 'linear', } }; … {% endraw %} {% endhighlight %}\nOverride tickValues and return map data. It will fix grid with guideline.\nThis looks clear, but now grid value and range are being locked up, and will show another problem.\nBecause of range value are fixed, it will show all grids though total range becomes very large. You need additional work for this.\n{% highlight javascript %} {% raw %} … vm.tick_list = [];\nvm.graph_data = function() { … var graph_data = []; var need_custom_tick = (total_data.length \u003e 5); var tickRange = 1;\nif(need_custom_tick) tickRange = Math.round(total_data.length / 5); for (var i = 0; i \u003c total_data.length; i++) { graph_data.push({ graph_date: moment(total_data[i]).valueOf(), }); if(i % tickRange == 0) vm.tick_list.push(moment(total_data[i]).valueOf()); } ... } … vm.graph_option = { chart: { type: ’lineChart’, height: 300,\nxAxis: { axisLabel: 'Date', tickFormat: function(d) { return d3.time.format('%m/%d')(new Date(d)); }, tickValues: function(values) { return _.map(values[0].values, function(v) { if(vm.tick_list.includes(v.graph_date)) { return new Date(v.graph_date); } else { return ''; } }); }, rotateLabels: -45, showMaxMin: true }, interpolate: 'linear', } }; … {% endraw %} {% endhighlight %}\nI made custom tick range when registering graph data. As you can see in ‘vm.graph_data’ function, it shows all values when there are 5 or less data(this is just temporarily definition) and add custom tick value when there are more than this. These ticks divides the range into 5. Now in ’tickValues’ option, return proper date value only for this value.\nAs you can find out, this process cannot divide all range equally. But anyway it cannot make same range value for all distance as you want to make all tick match with guideline. Though you can make closely equal with more work, and hope this could be help to someone.\n","description":"Managing tick values on nvd3 chart","tags":["javascript","angularjs","nvd3"],"title":"Control linechart on NVD3","uri":"/posts/2016-09-28-nvd3-customize-tick/"},{"categories":null,"content":"Now, most of projects are implemented with group of open sources, and package managers are helping us to add these easily. If you are working on project using javascript, you will be familiar with bower and npm. I am working on AngularJS project currently, and most of library are being managed with bower.\nAs you know, not all of open sources are being developed as you want. Some of are not being updated with critical issues, though it needs to be fixed urgently. One way to handle this situation is download the file, fix the source, and add to your local project. But your library packages will be seperated(one in bower component, one in local folder), and it is not good to maintain source codes.\nThis is my current situation, so I tried to add my forked project on bower. It is simple. First, I made fork project from original one.\nI will register this on bower.\n{% highlight bash %} {% raw %} $ bower register ion-range-slider-angularjs https://github.com/kination/ion-range-slider-angularjs.git Registering a package will make it visible and installable via the registry. Proceed (y/n)? y {% endraw %} {% endhighlight %}\nMake sure to do not use name already registered. There are plenty of packages, so first check your name with bower search.\n{% highlight bash %} {% raw %} $ bower search ion-range-slider Search results:\nion-range-slider https://github.com/IonDen/ion.rangeSlider.git ion-range-slider-angularjs https://github.com/kination/ion-range-slider-angularjs.git angular-foundation-range-slider https://github.com/pxdunn/RangeSlider.git foundation-range-slider-angular https://github.com/csaftoiu/foundation-range-slider-angular.git {% endraw %} {% endhighlight %}\nNow your project has been enrolled. Before install this project, first look on your package.\n{% highlight bash %} {% raw %} $ bower info ion-range-slider-angularjs bower cached https://github.com/kination/ion-range-slider-angularjs.git#1.0.4 bower validate 1.0.4 against https://github.com/kination/ion-range-slider-angularjs.git#* … {% endraw %} {% endhighlight %}\nWhen you look on line after bower cached, sometimes original package(here, it is ion.rangeslider-angularjs) are being indicated. In this case, bower install ion-range-slider-angularjs will install package on cache instead of mine. So let’s clean the cache first.\n{% highlight bash %} {% raw %} $ bower cache clean … $ bower info ion-range-slider-angularjs bower not-cached https://github.com/kination/ion-range-slider-angularjs.git#* bower resolve https://github.com/kination/ion-range-slider-angularjs.git#* bower checkout ion-range-slider-angularjs#1.0.4 bower resolved https://github.com/kination/ion-range-slider-angularjs.git#*\n{ name: ‘ion-range-slider-angularjs’, version: ‘1.0.4’, homepage: ‘https://github.com/kination/ion-range-slider-angularjs.git', ‘original authors’: [ ‘Geoffrey Bauduin bauduin.geo@gmail.com’ ], description: ‘fork project of ion.rangeslider-angularjs’, main: ‘ionic-range-slider.js’, keywords: [ ‘angularjs’, ‘angular’, ‘ionic’, ‘slider’, ‘rangeslider’, ‘ion.rangeslider’ ], license: ‘MIT’, ignore: [ ’node_modules’, ‘bower_components’, ’test’, ’tests’ ], dependencies: { ‘ion.rangeslider’: ‘~2.1.2’ } }\nAvailable versions:\n1.0.4 1.0.3 1.0.2 1.0.1 1.0.0 {% endraw %} {% endhighlight %} Your package has been setup well, so now fix issue on fork project.\nAfter clearing issue, see bower.json on your project and update version number. You can fix package information like description, publisher, etc. here. What you see in bower info comes from here. Also, you need to setup tag after update, or bower package will still keep looking on old version.\n- Your package should use semver Git tags. The v prefix is allowed. - Your package must be publically available at a Git endpoint (e.g., GitHub). Remember to push your Git tags! This is mentioned on official bower web page.\nI fixed the issue, updated version to 1.0.5. Now updating tag:\n{% highlight bash %} {% raw %} $ git tag 1.0.5 $ git push origin 1.0.5 Username for ‘https://github.com’: kination Password for ‘https://kination@github.com’: Total 0 (delta 0), reused 0 (delta 0) To https://github.com/kination/ion-range-slider-angularjs.git\n[new tag] 1.0.5 -\u003e 1.0.5 {% endraw %} {% endhighlight %} Clear the cache, and check package info to see version has been changed successfully.\n{% highlight bash %} {% raw %} $ bower cache clean $ bower info ion-range-slider-angularjs bower not-cached https://github.com/kination/ion-range-slider-angularjs.git#* bower resolve https://github.com/kination/ion-range-slider-angularjs.git#* bower checkout ion-range-slider-angularjs#1.0.5 bower resolved https://github.com/kination/ion-range-slider-angularjs.git#1.0.5\n{ name: ‘ion-range-slider-angularjs’, version: ‘1.0.5’, homepage: ‘https://github.com/kination/ion-range-slider-angularjs.git', ‘original authors’: [ … {% endraw %} {% endhighlight %}\nNow bower package with issue cleared has been ready to launch.\nI registered to handle known issue quickly, but some of people could use this to customize open source package to make it fit on their working project. Just always think to clear cache, and setup tag when updating. I have wasted so much time because of this. Hope nobody being stuck like me.\n","description":"How to register own package to bower repository","tags":["javascript","bower"],"title":"Publish package to Bower","uri":"/posts/2016-08-19-bower-register/"},{"categories":null,"content":"If you are googling to find how to internalizate your angularjs, you would find angular-translate or angular-gettext mostly. The first reason I choose angular-translate is because this has setting for reading translation data through json file. angular-gettext is using .po format and they have own tool to edit, but I don’t want to use other specific tool for editing. Because I have to ask for translation to other team, and it is a bugging job to guide how to work on with this. Editing text or json is more simple, and this is why I made this decision.\nYou can find more details about angular-translate on https://angular-translate.github.io\nAdd library file with bower for setup…\n{% highlight bash %} {% raw %} bower install angular-translate {% endraw %} {% endhighlight %}\nand add path and module for this. Module name is pascalprecht.translate.\n{% highlight html %} {% raw %} …\n… {% endraw %} {% endhighlight %}\nNow go on for simple translation.\n{% highlight html %} {% raw %}\n{{ 'TITLE' | translate }} ... {% endraw %} {% endhighlight %}\nTranslation data is being defined with module $translateProvider. After config is setup, you can select your target language by using $translate.use('your-target-language') on your controller. You don’t even need to refresh your page. Just calling this command switches your resources directly.\nThis will be enough if you have only few of strings to handle. But if you have more resource to translate, and want to manage list on seperate file, you need more things to do.\nGet several libraries with bower again…\n{% highlight bash %} {% raw %} bower install angular-translate-loader-static-files bower install angular-translate-storage-local bower install angular-translate-storage-cookie {% endraw %} {% endhighlight %}\nand add path for these.\n{% highlight html %} {% raw %} …\n… {% endraw %} {% endhighlight %}\nNow you need to call your JSON list. First, as you know, you cannot inject $http service inside configuration on AngularJS. There are some of detour for this, but it is not recommended. Thankfully, there are prebuild module translateProvider.useStaticFilesLoader for this. Change your config file like this:\n{% highlight javascript %} {% raw %} … var app = angular.module(‘appName’, [‘pascalprecht.translate’]); app.config( function ($translateProvider) {\n$translateProvider .useStaticFilesLoader({ prefix: 'json/file/path/', suffix: '.json' }) .preferredLanguage('de') .fallbackLanguage(['en']).useLocalStorage(); }); … {% endraw %} {% endhighlight %}\nfile: json/file/path/en.json {% highlight json %} {% raw %} { “TITLE”: “Hello” } {% endraw %} {% endhighlight %}\nfile: json/file/path/de.json {% highlight json %} {% raw %} { “TITLE”: “Hallo” } {% endraw %} {% endhighlight %}\nThis is it. Now this will work just same as first configurations.\nThere is also a way to lazy-load language resource. Maybe that way could be another answer, but unless you have massive size of resource, this is enough fast, and more simple to implement. I am working with about 500 resouce, but couldn’t find any delay loading this.\n","description":"How to setup language translation on AngularJS project","tags":["javascript","angularjs"],"title":"Translate Angular Project","uri":"/posts/2016-08-05-translate-angular-project/"},{"categories":null,"content":"If you are looking on multiple python projects, you would have experienced a problem with different package version between project. For example, one of the project are using django 1.7.x, while other only supports 1.9.x. In this case, you need to re-setup your packages everytime changing working project. Because there are lots of packages which not support backward compatibility, to avoid this issue, it is good to make seperate environment on each project. For this purpose, virtualenv is quite a good answer.\nInstall first…\n{% highlight bash %} {% raw %} pip install virtualenv {% endraw %} {% endhighlight %}\nthen go to your python project page, and type: {% highlight bash %} {% raw %} $ cd your-python-project $ virtualenv venv $ source venv/bin/activate {% endraw %} {% endhighlight %}\nNow your project is seperated with your system. You could see ‘(venv)’ on left of terminal. You can use other name than ‘venv’. Do as you want. One thing for sure, your project is only isolated with python pakages installed on system. You can run your project just same as before except that.\nYou can also create virtual environment working on other version of python. Use -p option:\n{% highlight bash %} {% raw %} $ cd your-python-project $ python –version 3.5.1 $ virtualenv -p /usr/bin/python2.7 venv $ source venv/bin/activate (venv) $ python –version 2.7.10 {% endraw %} {% endhighlight %}\nGo on project, and you could see folder created namd ‘venv’. All of package settings will be setup under this folder, and package setup here only effects on your ‘venv’ area. Install packages for your project. {% highlight bash %} {% raw %} (venv) $ pip install -r requirements.txt {% endraw %} {% endhighlight %}\nThis is it. Now you could work just same as before. If you want to get out of ‘venv’, just type:\n{% highlight bash %} {% raw %} (venv) $ deactivate $ {% endraw %} {% endhighlight %}\nIn my case, I am working on 2~3 python projects. There is 1 main project, and others are just for study. Because all of them are using different package versions, I installed main project packages to system, and use virtualenv on sub projects. It may be bothering to type these commands everytime for projects you work oftenly.\nIt could be fine with this. But if you want something that could simplify switching python version of your system, there is another thing named pyenv.\n{% highlight bash %} {% raw %} $ brew update $ brew install pyenv … $ echo ’eval “$(pyenv init -)”’ » ~/.bash_profile $ pyenv –version pyenv 20160726 {% endraw %} {% endhighlight %}\nNow you can use pyenv. It is pretty simple. First look on some commands. {% highlight bash %} {% raw %} $ pyenv versions system 2.7 2.7.12\n3.5.1 $ python –version Python 3.5.1 $ pyenv shell 2.7.12 $ python –version Python 2.7.12 {% endraw %} {% endhighlight %} This shows installed versions of python. I have downloaded 2.7, 2.7.12, 3.5.1, and selected version is marked with *. If you want to switch the version, just use pyenv shell (version number). Installing version is simple as this. Check the list of versions, and install it. {% highlight bash %} {% raw %} $ pyenv install -list Available versions: 2.1.3 2.2.3 2.3.7 … $ pyenv install 3.6-dev Cloning https://hg.python.org/cpython… Installing Python-3.6-dev… Installed Python-3.6-dev to /Users/kwangin/.pyenv/versions/3.6-dev\n$ pyenv versions system 2.7 2.7.12\n3.5.1 3.6-dev {% endraw %} {% endhighlight %} This does not offer seperated environment on same project, but it is more simple to solve python version dependency problem. You don’t need disk storage to load python version on each project like virtualenv. There are pros and cons on both of it, so think about good combinations.\n","description":"Develop python projects with virtualenv \u0026 pyenv","tags":["python","environment"],"title":"Setup python environment","uri":"/posts/2016-07-30-using-virtualenv/"},{"categories":null,"content":"This is a description about creating web app by angularjs, running based on django. It does not explain complicate logic, and only get focus on how to show simple page using these two framework.\nDefine that Django has been already installed, run commands below to create basic django project.\n{% highlight groovy %} {% raw %} django-admin startproject djangoServer cd djangoServer python manage.py startapp index {% endraw %} {% endhighlight %}\nNow there are django project with app name index. This will be used to run our angular web application. There are some places to fix. First look on settings.py in django.\n{% highlight python %} {% raw %} … TEMPLATES = [ { ‘BACKEND’: ‘django.template.backends.django.DjangoTemplates’, ‘DIRS’: [’/Users/kwangin/WebstormProjects/angular-django/app’], ‘APP_DIRS’: True, ‘OPTIONS’: { ‘context_processors’: [ ‘django.template.context_processors.debug’, ‘django.template.context_processors.request’, ‘django.contrib.auth.context_processors.auth’, ‘django.contrib.messages.context_processors.messages’, ], }, }, ]\nSTATICFILES_DIRS = [ ‘/Users/kwangin/WebstormProjects/angular-django/app’,os.path.join(BASE_DIR, “static”), ] … {% endraw %} {% endhighlight %}\nYou need to tell where your front-end project will be placed. In my case ‘/Users/kwangin/WebstormProjects/angular-django/app’ is the location. In django, files for web project such as CSS, JS, HTML files are being managed as ‘static’ files, and we need to tell where these files are. Put this address on TEMPLATES, and STATICFILES_DIRS to define the front-end dir.\nFront-end part is more simple than this. Here is my sample project.\nThis is based on AngularJS seed project. You could find it on https://github.com/angular/angular-seed. It only shows 2 link, and comment “Angular seed app” on single page. There are no codes to point out backend location, but has bit bothering work to do. This part will be cons for using django with angularjs because you don’t need to do this on nodeJS(or maybe such other back-end frameworks). Django project defines to put on front-end project files as static file, we need to define the location of all front-end file as below of path ‘static’. As you see in image above, file path all has been changed to be located below ‘/static/’\n{% highlight javascript %} {% raw %} …\n… {% endraw %} {% endhighlight %}\nwill be\n{% highlight javascript %} {% raw %} …\n… {% endraw %} {% endhighlight %}\nAll of path includes ‘/static/’ at front of it. This path is defined at settings.py as STATIC_URL parameter.\n{% highlight python %} {% raw %} … STATIC_URL = ‘/static/’ … {% endraw %} {% endhighlight %}\nOne more, if you need to input values on controller to template page like below, you need to wrap double-brace with ‘verbatim’. The first page(usually index.html) is being called by django, and it cannot recognize double-brace form usually used on AngularJs. This issue pretty bothered me when I first created angularjs-django project.\n{% highlight javascript %} {% raw %} …\n{% verbatim %}{{appName}}{% endverbatim %} seed app: v ... {% endraw %} {% endhighlight %} This is it. Let’s run django and open on local browser.\n","description":"This is a description about creating web app by angularjs, running based on django.","tags":["angularjs","django"],"title":"AngularJS running on Django","uri":"/posts/2016-06-11-angularjs-django/"},{"categories":null,"content":"While working on updating old codes on Android app, one of my plan is to switch ListView to RecyclerView. It has been supported after Android 5.0. This is a container which has better performance on displaying large dataset and scrolling up/down than ListView or GridView. It looks helpful who would need to make a list with large data, or showing lots of images.\nTo use it, first thing to do is to add library on gradle.\n{% highlight groovy %} {% raw %} dependencies { … compile “com.android.support:recyclerview-v7:23.0.1” } {% endraw %} {% endhighlight %}\nEvery guide I check mentioned to add this, but my code works well without it. It looks like it has been included to support UI library I added.\nWhat on the list is showing bunch of files inside the device, and what it needs to be shown is only icon and text.\n{% highlight java %} {% raw %} @Override public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) { View view = inflater.inflate(R.layout.fragment_sample, container, false); ListView listView = (ListView) view.findViewById(R.id.fragment_listView); listView.setAdapter(new ItemAdapter()); }\n{% endraw %} {% endhighlight %}\nNow, I changed to RecyclerView.\n{% highlight java %} {% raw %} @Override public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) { View view = inflater.inflate(R.layout.fragment_sample, container, false); RecyclerView recyclerView = (RecyclerView) view.findViewById(R.id.fragment_listView); LinearLayoutManager layoutManager = new LinearLayoutManager(getContext()); layoutManager.setOrientation(LinearLayoutManager.VERTICAL); recyclerView.setLayoutManager(layoutManager); recyclerView.setAdapter(new ItemAdapter()); } {% endraw %} {% endhighlight %}\nThis part is almost same, except the code about LinearLayoutManager. In Android dev site, it said… “In contrast to other adapter-backed views such as ListView or GridView, RecyclerView allows client code to provide custom layout arrangements for child views. These arrangements are controlled by the RecyclerView.LayoutManager. A LayoutManager must be provided for RecyclerView to function.” It works well after setup LayoutManager.\nAdapter has also been changed. Below ones are not the full code. It is just for explaining the difference between implementing adapter for ListView and RecyclerView.\n{% highlight java %} {% raw %} public class ItemAdapter extends BaseAdapter {\nList itemList; …\n@Override public int getCount() { return itemList.size(); }\n@Override public Object getItem(int position) { return itemList.get(position); }\n@Override public View getView(int position, View convertView, ViewGroup parent) { convertView = mLayoutInflater.inflate(R.layout.fragment_single_item, parent, false); ImageView itemIcon = (ImageView) convertView.findViewById(R.id.item_icon); TextView itemName = (TextView) convertView.findViewById(R.id.item_name);\nitemIcon.setImageResource(itemList.get(position).icon); itemName.setText(itemList.get(position).name); return convertView; } } {% endraw %} {% endhighlight %}\nAnd, this is for RecyclerView.\n{% highlight java %} {% raw %} public class ItemAdapter extends RecyclerView.Adapter {\nList itemList; …\n@Override public ItemHolder onCreateViewHolder(ViewGroup parent, int viewType) {\nView view = LayoutInflater.from(parent.getContext()).inflate(R.layout.fragment_single_item, parent, false); return new ItemHolder(view); //call holder to design item }\n@Override public void onBindViewHolder(ItemHolder holder, final int position) { Item item = itemList.get(position); holder.bindItem(itemList.get(position)); holder.itemContainer.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) {\nLog.d(LOG_TAG, \"click recyclerview item = \" + itemList.get(position).getExtension()); } }); holder.itemContainer.setOnLongClickListener(new View.OnLongClickListener() { @Override public boolean onLongClick(View v) { Log.d(LOG_TAG, \"click recyclerview item long = \" + position); return false; } }); }\n@Override public long getItemId(int position) { return position; }\n@Override public int getItemCount() { return itemList.size(); } }\npublic class ItemHolder extends RecyclerView.ViewHolder {\npublic View itemContainer; private ImageView mItemIcon; private TextView mItemName; private ItemModel mItemModel private ItemObj mItemObj; public ItemHolder(View itemView) { super(itemView); itemContainer = itemView; //define resources mItemIcon = (ImageView) itemView.findViewById(R.id.item_icon); mItemName = (TextView) itemView.findViewById(R.id.item_name); } //setup value for each items. public void bindItem(ItemModel itemModel) { mItemModel = itemModel; mItemIcon.setImageResource(mItemObj.icon); mItemName.setText(mItemObj.name); } } {% endraw %} {% endhighlight %}\nNow, there are some changes on override methods, and new thing name “ViewHolder”. This describes a view form and data of each items on RecycleView. RecyclerView.Adapter implementation should subclass this and add fields for its parameters. If you compare two code above about Adapter, you can find getView implementation on BaseAdapter has been divided to RecyclerView.onCreateViewHolder and RecyclerView.ViewHolder constructor. After that, call method to bind data to view component in onBindViewHolder(…) on RecyclerView.Adapter.\nThe View variable(ItemHolder.itemContainer) is to handle click event on Adapter class, to give an event when item on list is selected. There are lots of way to implement select event, so this parameter is not necessary.\nOne more thing. After you see the list, you could find no effects are working when you touch the list. Add below on your item resource. NOT list, but item.\n{% highlight xml %} {% raw %} \u003cLinearLayout xmlns:android=“http://schemas.android.com/apk/res/android\" … android:background=\"@drawable/item_list_background” android:clickable=“true” android:focusable=“true”\u003e {% endraw %} {% endhighlight %}\nNow converting ListView to RecyclerView is finished. If your list has few items or takes low resource, it will have no difference with previous one. It will show benefit when your list needs to handle massive data/resource in list.\n","description":"How to change listview to recyclerview","tags":["android","recyclerview"],"title":"ListView to RecyclerView","uri":"/posts/2016-03-06-android-recycler/"}]
